{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.utils\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import gobanana as gb"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "class NaiveGenerator(gb.nn.Generator):\n",
    "    def __init__(self, board_shape, num_metrics, hidden_dim):\n",
    "        super().__init__(board_shape, num_metrics)\n",
    "        board_size = board_shape[0] * board_shape[1]\n",
    "        input_size = board_size + num_metrics\n",
    "        self.fc1 = nn.Linear(input_size, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, board_size * 3)\n",
    "\n",
    "    def forward(self, noise: torch.Tensor, metrics: torch.Tensor):\n",
    "        x = torch.cat([noise.flatten(start_dim=1), metrics.flatten(start_dim=1)], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = x.reshape(-1, *self.board_shape, 3)\n",
    "        x = F.softmax(x, dim=-1)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "class NaiveEvaluator(gb.nn.Evaluator):\n",
    "    def __init__(self, board_shape, num_metrics, hidden_dim):\n",
    "        super().__init__(board_shape, num_metrics)\n",
    "        board_size = board_shape[0] * board_shape[1]\n",
    "        self.embed = nn.Embedding(3, 1)\n",
    "        self.fc1 = nn.Linear(board_size, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, num_metrics)\n",
    "        \n",
    "    def forward(self, one_hot_boards):\n",
    "        x = one_hot_boards\n",
    "        x = self.embed(x).flatten(start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "def batch_generate_boards(generator, board_shape, metrics, batch_size=32):\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        noise = torch.rand(batch_size, *board_shape)\n",
    "        metrics = torch.tensor(metrics).float()\n",
    "        assert batch_size == metrics.shape[0]\n",
    "        generator_out = generator(noise, metrics)\n",
    "        board_matrices = generator_out.argmax(-1).numpy()\n",
    "    return [gb.game.Board(mat) for mat in board_matrices]\n",
    "    \n",
    "def count_bananas(board: gb.game.Board):\n",
    "    return np.sum(board.mat == board.BANANA).item()\n",
    "\n",
    "def build_evaluator_training_samples(boards):\n",
    "    training_samples = []\n",
    "    for board in boards:\n",
    "        actual_num_bananas = count_bananas(board)\n",
    "        board_one_hot_tensor = torch.tensor(board.mat).long()\n",
    "        metrics_tensor = torch.tensor(actual_num_bananas).float()\n",
    "        sample = (board_one_hot_tensor, metrics_tensor)\n",
    "        training_samples.append(sample)\n",
    "    return training_samples"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "def train_evaluator(evaluator, train_loader, epochs=1):\n",
    "    print(\"Training Evaluator ...\")\n",
    "    optimizer = optim.Adam(evaluator.parameters())\n",
    "    loss_function = nn.MSELoss()\n",
    "    evaluator.train()\n",
    "    losses = []\n",
    "    for _ in range(epochs):\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            predicted_metrics = evaluator(x)\n",
    "            loss = loss_function(y, predicted_metrics)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "    print('loss:', np.mean(losses))\n",
    "        \n",
    "            \n",
    "def train_generator(generator, evaluator, train_loader, epochs=1):\n",
    "    print(\"Training Generator ...\")\n",
    "    # should I include both parameters here and freeze the other one?\n",
    "    optimizer = optim.Adam(generator.parameters())\n",
    "    loss_function = nn.MSELoss()\n",
    "    generator.train()\n",
    "    losses = []\n",
    "    for _ in range(epochs):\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            boards = generator(x, y)\n",
    "            predicted_metrics = evaluator(boards.argmax(-1))\n",
    "            loss = loss_function(y, predicted_metrics)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "    print('loss:', np.mean(losses))\n",
    "            \n",
    "def get_random_metrics():\n",
    "    desired_num_bananas = random.randint(0, 9)\n",
    "    return [desired_num_bananas]\n",
    "\n",
    "def get_random_metrics_batch(batch_size):\n",
    "    return [get_random_metrics() for _ in range(batch_size)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "100%|██████████| 128/128 [01:40<00:00,  1.28it/s]\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "Training Evaluator ...\n",
      "loss: 24.443652534484862\n",
      "Training Generator ...\n",
      "loss: 23.22728729248047\n",
      "Training Evaluator ...\n",
      "loss: 22.045560836791992\n",
      "Training Generator ...\n",
      "loss: 23.636838912963867\n",
      "Training Evaluator ...\n",
      "loss: 19.0175573348999\n",
      "Training Generator ...\n",
      "loss: 19.70987319946289\n",
      "Training Evaluator ...\n",
      "loss: 15.33828763961792\n",
      "Training Generator ...\n",
      "loss: 14.698568344116211\n",
      "Training Evaluator ...\n",
      "loss: 10.660590648651123\n",
      "Training Generator ...\n",
      "loss: 15.611869812011719\n",
      "Training Evaluator ...\n",
      "loss: 7.495778942108155\n",
      "Training Generator ...\n",
      "loss: 11.276052474975586\n",
      "Training Evaluator ...\n",
      "loss: 4.045218467712402\n",
      "Training Generator ...\n",
      "loss: 10.237462043762207\n",
      "Training Evaluator ...\n",
      "loss: 1.5218568563461303\n",
      "Training Generator ...\n",
      "loss: 10.755376815795898\n",
      "Training Evaluator ...\n",
      "loss: 1.273093056678772\n",
      "Training Generator ...\n",
      "loss: 9.89112377166748\n",
      "Training Evaluator ...\n",
      "loss: 1.2643842577934266\n",
      "Training Generator ...\n",
      "loss: 8.506629943847656\n",
      "Training Evaluator ...\n",
      "loss: 0.9184126257896423\n",
      "Training Generator ...\n",
      "loss: 9.388216972351074\n",
      "Training Evaluator ...\n",
      "loss: 0.9758804976940155\n",
      "Training Generator ...\n",
      "loss: 8.086414337158203\n",
      "Training Evaluator ...\n",
      "loss: 0.9462249636650085\n",
      "Training Generator ...\n",
      "loss: 8.249999046325684\n",
      "Training Evaluator ...\n",
      "loss: 0.8244322061538696\n",
      "Training Generator ...\n",
      "loss: 9.32841682434082\n",
      "Training Evaluator ...\n",
      "loss: 0.8990567088127136\n",
      "Training Generator ...\n",
      "loss: 8.342262268066406\n",
      "Training Evaluator ...\n",
      "loss: 0.8480610609054565\n",
      "Training Generator ...\n",
      "loss: 8.98576545715332\n",
      "Training Evaluator ...\n",
      "loss: 1.0181916236877442\n",
      "Training Generator ...\n",
      "loss: 8.143585205078125\n",
      "Training Evaluator ...\n",
      "loss: 0.9405230820178986\n",
      "Training Generator ...\n",
      "loss: 8.610830307006836\n",
      "Training Evaluator ...\n",
      "loss: 0.7920620083808899\n",
      "Training Generator ...\n",
      "loss: 8.011984825134277\n",
      "Training Evaluator ...\n",
      "loss: 0.8700700044631958\n",
      "Training Generator ...\n",
      "loss: 8.229179382324219\n",
      "Training Evaluator ...\n",
      "loss: 0.8993562042713166\n",
      "Training Generator ...\n",
      "loss: 8.445733070373535\n",
      "Training Evaluator ...\n",
      "loss: 0.8312331020832062\n",
      "Training Generator ...\n",
      "loss: 8.679316520690918\n",
      "Training Evaluator ...\n",
      "loss: 0.8381654977798462\n",
      "Training Generator ...\n",
      "loss: 8.388886451721191\n",
      "Training Evaluator ...\n",
      "loss: 0.7648418366909027\n",
      "Training Generator ...\n",
      "loss: 8.368877410888672\n",
      "Training Evaluator ...\n",
      "loss: 0.8230195999145508\n",
      "Training Generator ...\n",
      "loss: 8.574024200439453\n",
      "Training Evaluator ...\n",
      "loss: 0.8358176052570343\n",
      "Training Generator ...\n",
      "loss: 9.219661712646484\n",
      "Training Evaluator ...\n",
      "loss: 0.7894076704978943\n",
      "Training Generator ...\n",
      "loss: 9.918928146362305\n",
      "Training Evaluator ...\n",
      "loss: 0.9786761820316314\n",
      "Training Generator ...\n",
      "loss: 8.424637794494629\n",
      "Training Evaluator ...\n",
      "loss: 0.8647472023963928\n",
      "Training Generator ...\n",
      "loss: 8.343385696411133\n",
      "Training Evaluator ...\n",
      "loss: 0.8229293167591095\n",
      "Training Generator ...\n",
      "loss: 8.803622245788574\n",
      "Training Evaluator ...\n",
      "loss: 1.039362370967865\n",
      "Training Generator ...\n",
      "loss: 8.84737777709961\n",
      "Training Evaluator ...\n",
      "loss: 1.1038647413253784\n",
      "Training Generator ...\n",
      "loss: 8.97400951385498\n",
      "Training Evaluator ...\n",
      "loss: 0.8810605108737946\n",
      "Training Generator ...\n",
      "loss: 9.31503677368164\n",
      "Training Evaluator ...\n",
      "loss: 0.8300379753112793\n",
      "Training Generator ...\n",
      "loss: 9.013496398925781\n",
      "Training Evaluator ...\n",
      "loss: 0.8758077085018158\n",
      "Training Generator ...\n",
      "loss: 8.018389701843262\n",
      "Training Evaluator ...\n",
      "loss: 0.9410275161266327\n",
      "Training Generator ...\n",
      "loss: 8.657442092895508\n",
      "Training Evaluator ...\n",
      "loss: 1.02854642868042\n",
      "Training Generator ...\n",
      "loss: 9.575371742248535\n",
      "Training Evaluator ...\n",
      "loss: 0.9883334636688232\n",
      "Training Generator ...\n",
      "loss: 8.666694641113281\n",
      "Training Evaluator ...\n",
      "loss: 0.845982027053833\n",
      "Training Generator ...\n",
      "loss: 8.0703125\n",
      "Training Evaluator ...\n",
      "loss: 0.9534895360469818\n",
      "Training Generator ...\n",
      "loss: 8.799476623535156\n",
      "Training Evaluator ...\n",
      "loss: 0.8424011290073394\n",
      "Training Generator ...\n",
      "loss: 8.2177095413208\n",
      "Training Evaluator ...\n",
      "loss: 0.8979813814163208\n",
      "Training Generator ...\n",
      "loss: 9.464314460754395\n",
      "Training Evaluator ...\n",
      "loss: 0.7840502321720123\n",
      "Training Generator ...\n",
      "loss: 9.105239868164062\n",
      "Training Evaluator ...\n",
      "loss: 0.9443736851215363\n",
      "Training Generator ...\n",
      "loss: 8.361926078796387\n",
      "Training Evaluator ...\n",
      "loss: 0.8233576655387879\n",
      "Training Generator ...\n",
      "loss: 9.490974426269531\n",
      "Training Evaluator ...\n",
      "loss: 1.0016213417053224\n",
      "Training Generator ...\n",
      "loss: 7.765691757202148\n",
      "Training Evaluator ...\n",
      "loss: 0.8985810458660126\n",
      "Training Generator ...\n",
      "loss: 9.225263595581055\n",
      "Training Evaluator ...\n",
      "loss: 0.8765987277030944\n",
      "Training Generator ...\n",
      "loss: 9.623608589172363\n",
      "Training Evaluator ...\n",
      "loss: 0.8222114384174347\n",
      "Training Generator ...\n",
      "loss: 8.468549728393555\n",
      "Training Evaluator ...\n",
      "loss: 0.97246453166008\n",
      "Training Generator ...\n",
      "loss: 8.706608772277832\n",
      "Training Evaluator ...\n",
      "loss: 0.9537965595722199\n",
      "Training Generator ...\n",
      "loss: 8.741025924682617\n",
      "Training Evaluator ...\n",
      "loss: 0.7621433019638062\n",
      "Training Generator ...\n",
      "loss: 8.668268203735352\n",
      "Training Evaluator ...\n",
      "loss: 0.9641885757446289\n",
      "Training Generator ...\n",
      "loss: 8.07928466796875\n",
      "Training Evaluator ...\n",
      "loss: 0.8631649613380432\n",
      "Training Generator ...\n",
      "loss: 8.773492813110352\n",
      "Training Evaluator ...\n",
      "loss: 0.8684228956699371\n",
      "Training Generator ...\n",
      "loss: 9.037415504455566\n",
      "Training Evaluator ...\n",
      "loss: 0.8230623006820679\n",
      "Training Generator ...\n",
      "loss: 8.588008880615234\n",
      "Training Evaluator ...\n",
      "loss: 0.8436656534671784\n",
      "Training Generator ...\n",
      "loss: 9.95725154876709\n",
      "Training Evaluator ...\n",
      "loss: 0.9510843455791473\n",
      "Training Generator ...\n",
      "loss: 8.914789199829102\n",
      "Training Evaluator ...\n",
      "loss: 0.8664048194885254\n",
      "Training Generator ...\n",
      "loss: 7.462037086486816\n",
      "Training Evaluator ...\n",
      "loss: 0.9027317106723786\n",
      "Training Generator ...\n",
      "loss: 8.562603950500488\n",
      "Training Evaluator ...\n",
      "loss: 0.9722885668277741\n",
      "Training Generator ...\n",
      "loss: 9.592962265014648\n",
      "Training Evaluator ...\n",
      "loss: 0.9123747706413269\n",
      "Training Generator ...\n",
      "loss: 7.741288661956787\n",
      "Training Evaluator ...\n",
      "loss: 0.8050522089004517\n",
      "Training Generator ...\n",
      "loss: 9.295482635498047\n",
      "Training Evaluator ...\n",
      "loss: 0.8165816009044647\n",
      "Training Generator ...\n",
      "loss: 8.368099212646484\n",
      "Training Evaluator ...\n",
      "loss: 0.952018016576767\n",
      "Training Generator ...\n",
      "loss: 8.9967622756958\n",
      "Training Evaluator ...\n",
      "loss: 0.806926167011261\n",
      "Training Generator ...\n",
      "loss: 7.588204383850098\n",
      "Training Evaluator ...\n",
      "loss: 0.8882247269153595\n",
      "Training Generator ...\n",
      "loss: 9.454407691955566\n",
      "Training Evaluator ...\n",
      "loss: 0.9470476567745209\n",
      "Training Generator ...\n",
      "loss: 7.92275857925415\n",
      "Training Evaluator ...\n",
      "loss: 0.8195999324321747\n",
      "Training Generator ...\n",
      "loss: 8.035882949829102\n",
      "Training Evaluator ...\n",
      "loss: 0.9994927227497101\n",
      "Training Generator ...\n",
      "loss: 8.871910095214844\n",
      "Training Evaluator ...\n",
      "loss: 0.8336905419826508\n",
      "Training Generator ...\n",
      "loss: 8.67010498046875\n",
      "Training Evaluator ...\n",
      "loss: 0.8942917585372925\n",
      "Training Generator ...\n",
      "loss: 8.713565826416016\n",
      "Training Evaluator ...\n",
      "loss: 0.8414241135120392\n",
      "Training Generator ...\n",
      "loss: 8.565038681030273\n",
      "Training Evaluator ...\n",
      "loss: 0.7941276490688324\n",
      "Training Generator ...\n",
      "loss: 8.220046043395996\n",
      "Training Evaluator ...\n",
      "loss: 0.8070484876632691\n",
      "Training Generator ...\n",
      "loss: 9.50541877746582\n",
      "Training Evaluator ...\n",
      "loss: 0.9357179224491119\n",
      "Training Generator ...\n",
      "loss: 7.356858730316162\n",
      "Training Evaluator ...\n",
      "loss: 0.9675011217594147\n",
      "Training Generator ...\n",
      "loss: 8.496734619140625\n",
      "Training Evaluator ...\n",
      "loss: 0.8742947161197663\n",
      "Training Generator ...\n",
      "loss: 9.159459114074707\n",
      "Training Evaluator ...\n",
      "loss: 0.9424684345722198\n",
      "Training Generator ...\n",
      "loss: 8.371847152709961\n",
      "Training Evaluator ...\n",
      "loss: 1.0103536128997803\n",
      "Training Generator ...\n",
      "loss: 8.45349407196045\n",
      "Training Evaluator ...\n",
      "loss: 0.8599085509777069\n",
      "Training Generator ...\n",
      "loss: 9.579750061035156\n",
      "Training Evaluator ...\n",
      "loss: 0.8974469482898713\n",
      "Training Generator ...\n",
      "loss: 8.613505363464355\n",
      "Training Evaluator ...\n",
      "loss: 0.9694977879524231\n",
      "Training Generator ...\n",
      "loss: 8.85666275024414\n",
      "Training Evaluator ...\n",
      "loss: 1.0078014612197876\n",
      "Training Generator ...\n",
      "loss: 8.719287872314453\n",
      "Training Evaluator ...\n",
      "loss: 1.0822962880134583\n",
      "Training Generator ...\n",
      "loss: 9.275933265686035\n",
      "Training Evaluator ...\n",
      "loss: 0.8478106677532196\n",
      "Training Generator ...\n",
      "loss: 7.935115337371826\n",
      "Training Evaluator ...\n",
      "loss: 0.9799590528011322\n",
      "Training Generator ...\n",
      "loss: 8.874030113220215\n",
      "Training Evaluator ...\n",
      "loss: 0.8268353879451752\n",
      "Training Generator ...\n",
      "loss: 9.232673645019531\n",
      "Training Evaluator ...\n",
      "loss: 0.7784361243247986\n",
      "Training Generator ...\n",
      "loss: 9.345541000366211\n",
      "Training Evaluator ...\n",
      "loss: 0.9049289286136627\n",
      "Training Generator ...\n",
      "loss: 7.465003490447998\n",
      "Training Evaluator ...\n",
      "loss: 0.9185297667980195\n",
      "Training Generator ...\n",
      "loss: 9.507913589477539\n",
      "Training Evaluator ...\n",
      "loss: 0.9358346819877624\n",
      "Training Generator ...\n",
      "loss: 9.37019157409668\n",
      "Training Evaluator ...\n",
      "loss: 0.8878820836544037\n",
      "Training Generator ...\n",
      "loss: 8.187122344970703\n",
      "Training Evaluator ...\n",
      "loss: 0.9101522266864777\n",
      "Training Generator ...\n",
      "loss: 7.901272296905518\n",
      "Training Evaluator ...\n",
      "loss: 0.8753188252449036\n",
      "Training Generator ...\n",
      "loss: 9.01113224029541\n",
      "Training Evaluator ...\n",
      "loss: 0.9773645639419556\n",
      "Training Generator ...\n",
      "loss: 9.275134086608887\n",
      "Training Evaluator ...\n",
      "loss: 0.949426805973053\n",
      "Training Generator ...\n",
      "loss: 9.243682861328125\n",
      "Training Evaluator ...\n",
      "loss: 0.8660801470279693\n",
      "Training Generator ...\n",
      "loss: 7.94732666015625\n",
      "Training Evaluator ...\n",
      "loss: 0.7684884071350098\n",
      "Training Generator ...\n",
      "loss: 9.05774211883545\n",
      "Training Evaluator ...\n",
      "loss: 0.9131996393203735\n",
      "Training Generator ...\n",
      "loss: 9.010859489440918\n",
      "Training Evaluator ...\n",
      "loss: 0.8388460874557495\n",
      "Training Generator ...\n",
      "loss: 8.439932823181152\n",
      "Training Evaluator ...\n",
      "loss: 0.8351989924907685\n",
      "Training Generator ...\n",
      "loss: 7.636221885681152\n",
      "Training Evaluator ...\n",
      "loss: 0.8694864392280579\n",
      "Training Generator ...\n",
      "loss: 9.517720222473145\n",
      "Training Evaluator ...\n",
      "loss: 0.8373651623725891\n",
      "Training Generator ...\n",
      "loss: 8.876138687133789\n",
      "Training Evaluator ...\n",
      "loss: 0.6994956910610199\n",
      "Training Generator ...\n",
      "loss: 8.166823387145996\n",
      "Training Evaluator ...\n",
      "loss: 0.9215052485466003\n",
      "Training Generator ...\n",
      "loss: 9.217412948608398\n",
      "Training Evaluator ...\n",
      "loss: 0.7856195449829102\n",
      "Training Generator ...\n",
      "loss: 9.747495651245117\n",
      "Training Evaluator ...\n",
      "loss: 1.1030167818069458\n",
      "Training Generator ...\n",
      "loss: 9.359346389770508\n",
      "Training Evaluator ...\n",
      "loss: 0.7876034140586853\n",
      "Training Generator ...\n",
      "loss: 8.087057113647461\n",
      "Training Evaluator ...\n",
      "loss: 0.8380342423915863\n",
      "Training Generator ...\n",
      "loss: 7.780817031860352\n",
      "Training Evaluator ...\n",
      "loss: 0.7962969481945038\n",
      "Training Generator ...\n",
      "loss: 8.431254386901855\n",
      "Training Evaluator ...\n",
      "loss: 0.7337243616580963\n",
      "Training Generator ...\n",
      "loss: 8.917293548583984\n",
      "Training Evaluator ...\n",
      "loss: 0.9395237267017365\n",
      "Training Generator ...\n",
      "loss: 8.14836311340332\n",
      "Training Evaluator ...\n",
      "loss: 0.7913070917129517\n",
      "Training Generator ...\n",
      "loss: 8.535662651062012\n",
      "Training Evaluator ...\n",
      "loss: 0.8964980006217956\n",
      "Training Generator ...\n",
      "loss: 7.824136257171631\n",
      "Training Evaluator ...\n",
      "loss: 0.7834267139434814\n",
      "Training Generator ...\n",
      "loss: 8.329458236694336\n",
      "Training Evaluator ...\n",
      "loss: 0.8302168428897858\n",
      "Training Generator ...\n",
      "loss: 10.008544921875\n",
      "Training Evaluator ...\n",
      "loss: 0.8820338249206543\n",
      "Training Generator ...\n",
      "loss: 8.741522789001465\n",
      "Training Evaluator ...\n",
      "loss: 0.8290151536464692\n",
      "Training Generator ...\n",
      "loss: 8.391921997070312\n",
      "Training Evaluator ...\n",
      "loss: 1.0130635738372802\n",
      "Training Generator ...\n",
      "loss: 9.064984321594238\n",
      "Training Evaluator ...\n",
      "loss: 0.9692481935024262\n",
      "Training Generator ...\n",
      "loss: 9.022151947021484\n",
      "Training Evaluator ...\n",
      "loss: 0.8192972540855408\n",
      "Training Generator ...\n",
      "loss: 8.83997631072998\n",
      "Training Evaluator ...\n",
      "loss: 0.9170718014240264\n",
      "Training Generator ...\n",
      "loss: 8.298622131347656\n",
      "Training Evaluator ...\n",
      "loss: 0.6369098484516144\n",
      "Training Generator ...\n",
      "loss: 8.380757331848145\n",
      "Training Evaluator ...\n",
      "loss: 1.0266047954559325\n",
      "Training Generator ...\n",
      "loss: 8.598316192626953\n",
      "Training Evaluator ...\n",
      "loss: 0.820153558254242\n",
      "Training Generator ...\n",
      "loss: 8.871691703796387\n",
      "Training Evaluator ...\n",
      "loss: 0.8469160437583924\n",
      "Training Generator ...\n",
      "loss: 9.3326997756958\n",
      "Training Evaluator ...\n",
      "loss: 0.8448591291904449\n",
      "Training Generator ...\n",
      "loss: 9.277097702026367\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "board_shape = (3, 3)\n",
    "generator = NaiveGenerator(board_shape, 1, 32)\n",
    "evaluator = NaiveEvaluator(board_shape, 1, 32)\n",
    "batch_size = 256\n",
    "iterations = 128\n",
    "\n",
    "for _ in tqdm.tqdm(list(range(iterations))):\n",
    "    batch_metrics = get_random_metrics_batch(batch_size)\n",
    "    generated_boards = batch_generate_boards(generator, board_shape, batch_metrics, batch_size)\n",
    "        \n",
    "    evaluator_training_samples = build_evaluator_training_samples(generated_boards)\n",
    "    evaluator_train_loader = torch.utils.data.DataLoader(\n",
    "            evaluator_training_samples,\n",
    "            batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    train_evaluator(evaluator, evaluator_train_loader, epochs=10)\n",
    "\n",
    "    generator_training_samples = []\n",
    "    \n",
    "    for _ in range(batch_size):\n",
    "        noise = torch.rand(*board_shape)\n",
    "        metrics = torch.tensor(get_random_metrics()).float()\n",
    "        generator_training_samples.append((noise, metrics))\n",
    "        \n",
    "    generator_train_loader = torch.utils.data.DataLoader(\n",
    "        generator_training_samples,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    train_generator(generator, evaluator, generator_train_loader, epochs=50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Desired metrics:  [8]\n",
      "tensor([[[2, 0, 1],\n",
      "         [2, 2, 2],\n",
      "         [1, 2, 2]]])\n",
      "\n",
      "Desired metrics:  [4]\n",
      "tensor([[[0, 0, 1],\n",
      "         [2, 2, 2],\n",
      "         [1, 2, 2]]])\n",
      "\n",
      "Desired metrics:  [8]\n",
      "tensor([[[2, 0, 1],\n",
      "         [0, 2, 0],\n",
      "         [1, 2, 2]]])\n",
      "\n",
      "Desired metrics:  [0]\n",
      "tensor([[[1, 0, 2],\n",
      "         [2, 1, 2],\n",
      "         [1, 2, 2]]])\n",
      "\n",
      "Desired metrics:  [2]\n",
      "tensor([[[0, 0, 2],\n",
      "         [2, 1, 2],\n",
      "         [1, 2, 2]]])\n",
      "\n",
      "Desired metrics:  [5]\n",
      "tensor([[[0, 0, 2],\n",
      "         [2, 2, 2],\n",
      "         [1, 2, 2]]])\n",
      "\n",
      "Desired metrics:  [0]\n",
      "tensor([[[0, 0, 1],\n",
      "         [2, 1, 2],\n",
      "         [1, 2, 1]]])\n",
      "\n",
      "Desired metrics:  [3]\n",
      "tensor([[[0, 0, 1],\n",
      "         [2, 2, 2],\n",
      "         [1, 2, 2]]])\n",
      "\n",
      "Desired metrics:  [8]\n",
      "tensor([[[2, 0, 2],\n",
      "         [0, 2, 0],\n",
      "         [1, 2, 2]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "tensor([[[0, 0, 2],\n",
      "         [2, 1, 2],\n",
      "         [1, 2, 2]]])\n",
      "\n",
      "Desired metrics:  [5]\n",
      "tensor([[[0, 0, 1],\n",
      "         [2, 2, 2],\n",
      "         [1, 2, 2]]])\n",
      "\n",
      "Desired metrics:  [9]\n",
      "tensor([[[2, 0, 2],\n",
      "         [0, 2, 0],\n",
      "         [1, 2, 2]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "tensor([[[0, 0, 2],\n",
      "         [2, 2, 2],\n",
      "         [1, 2, 1]]])\n",
      "\n",
      "Desired metrics:  [2]\n",
      "tensor([[[0, 0, 2],\n",
      "         [2, 2, 2],\n",
      "         [1, 2, 2]]])\n",
      "\n",
      "Desired metrics:  [4]\n",
      "tensor([[[0, 0, 1],\n",
      "         [2, 2, 2],\n",
      "         [1, 2, 2]]])\n",
      "\n",
      "Desired metrics:  [4]\n",
      "tensor([[[0, 0, 2],\n",
      "         [2, 2, 2],\n",
      "         [1, 2, 2]]])\n",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "for _ in range(16):\n",
    "    metrics = get_random_metrics()\n",
    "    print(\"Desired metrics: \", metrics)\n",
    "    metrics = torch.tensor(metrics).float().reshape(1, -1)\n",
    "    noise = torch.rand(1, *board_shape)\n",
    "    board = generator(noise, metrics)\n",
    "    print(board.argmax(-1))\n",
    "    print()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "gobanana",
   "language": "python",
   "display_name": "gobanana"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
