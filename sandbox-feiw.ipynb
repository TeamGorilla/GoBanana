{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T21:58:03.676217Z",
     "start_time": "2020-03-07T21:58:03.392825Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.utils\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import gobanana as gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T21:58:03.688959Z",
     "start_time": "2020-03-07T21:58:03.678314Z"
    }
   },
   "outputs": [],
   "source": [
    "class NaiveGenerator(gb.nn.Generator):\n",
    "    def __init__(self, board_shape, num_metrics, hidden_dim):\n",
    "        super().__init__(board_shape, num_metrics)\n",
    "        board_size = board_shape[0] * board_shape[1]\n",
    "        input_size = board_size + num_metrics\n",
    "        self.fc1 = nn.Linear(input_size, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dp1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dp2 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dp3 = nn.Dropout(0.5)\n",
    "        self.fc4 = nn.Linear(hidden_dim, board_size * 3)\n",
    "\n",
    "    def forward(self, noise: torch.Tensor, metrics: torch.Tensor):\n",
    "        x = torch.cat([noise.flatten(start_dim=1), metrics.flatten(start_dim=1)], dim=1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.dp1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.dp2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.dp3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.reshape(-1, *self.board_shape, 3)\n",
    "        x = F.softmax(x, dim=-1)\n",
    "        return x\n",
    "\n",
    "class NaiveEvaluator(gb.nn.Evaluator):\n",
    "    def __init__(self, board_shape, num_metrics, hidden_dim):\n",
    "        super().__init__(board_shape, num_metrics)\n",
    "        #board_size = board_shape[0] * board_shape[1]\n",
    "        #self.embed = nn.Embedding(3, 1)\n",
    "        self.fc1 = nn.Linear(board_shape[0] * board_shape[1] * 3, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc5 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc6 = nn.Linear(hidden_dim, num_metrics)\n",
    "        \n",
    "    def forward(self, one_hot_boards):\n",
    "        x = one_hot_boards\n",
    "        x = F.leaky_relu(self.fc1(x.flatten(start_dim=1)))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        x = F.leaky_relu(self.fc4(x))\n",
    "        x = F.leaky_relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T22:00:18.743077Z",
     "start_time": "2020-03-07T22:00:18.726584Z"
    }
   },
   "outputs": [],
   "source": [
    "def batch_generate_boards(generator, board_shape, metrics, batch_size=32):\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        noise = torch.rand(batch_size, *board_shape)\n",
    "        metrics = torch.tensor(metrics).float()\n",
    "        assert batch_size == metrics.shape[0]\n",
    "        generator_out = generator(noise, metrics)\n",
    "        board_matrices = generator_out.argmax(-1).numpy()\n",
    "    return [gb.game.Board(mat) for mat in board_matrices]\n",
    "\n",
    "def count_bananas(board: gb.game.Board):\n",
    "    return np.sum(board.mat == board.BANANA).item()\n",
    "\n",
    "def build_evaluator_training_samples(boards):\n",
    "    training_samples = []\n",
    "    for board in boards:\n",
    "        actual_num_bananas = count_bananas(board)\n",
    "        board_one_hot_tensor = torch.tensor(board.mat).long()\n",
    "        metrics_tensor = torch.tensor([actual_num_bananas]).float()\n",
    "        sample = (board_one_hot_tensor, metrics_tensor)\n",
    "        training_samples.append(sample)\n",
    "    return training_samples\n",
    "\n",
    "def train_evaluator(evaluator, train_loader, epochs=1):\n",
    "    print(\"Training Evaluator ...\")\n",
    "    optimizer = optim.Adam(evaluator.parameters())\n",
    "    loss_function = nn.MSELoss()\n",
    "    evaluator.train()\n",
    "    losses = []\n",
    "    for _ in range(epochs):\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            predicted_metrics = evaluator(x)\n",
    "            loss = loss_function(y, predicted_metrics)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "    print('loss:', np.mean(losses))\n",
    "        \n",
    "            \n",
    "def train_generator(generator, evaluator, train_loader, epochs=1):\n",
    "    print(\"Training Generator  ...\")\n",
    "    # should I include both parameters here and freeze the other one?\n",
    "    optimizer = optim.Adam(generator.parameters(), lr=0.0001)\n",
    "    #optimizer = optim.SGD(generator.parameters(), lr=0.0001)\n",
    "    loss_function = nn.MSELoss()\n",
    "    \n",
    "    generator.train()\n",
    "    losses = []\n",
    "    for _ in range(epochs):\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            boards = generator(x, y)\n",
    "            # argmax maybe the culprit of the generator not training.\n",
    "            predicted_metrics = evaluator(boards)#boards.argmax(-1))\n",
    "            loss = loss_function(y, predicted_metrics)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "            print('loss:', np.mean(losses))\n",
    "    \n",
    "\n",
    "def get_one_metrics():\n",
    "    return [1]\n",
    "\n",
    "def get_random_metrics():\n",
    "    desired_num_bananas = random.randint(0, 9)\n",
    "    return [desired_num_bananas]\n",
    "\n",
    "def get_random_metrics_batch(batch_size):\n",
    "    return [get_random_metrics() for _ in range(batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(generator, evaluator, train_loader, optimizer_g, optimizer_e, epochs=1, train_e=True):\n",
    "    print(\"Training Model  ...\")\n",
    "    # should I include both parameters here and freeze the other one?\n",
    "    mse_criterion = nn.MSELoss()\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        generator_losses = []\n",
    "        evaluator_losses = []\n",
    "        \n",
    "        evaluator.train()\n",
    "        generator.train()\n",
    "        for x, y in train_loader:\n",
    "            if train_e:\n",
    "                ###############################################\n",
    "                # Update E\n",
    "                ##############################################\n",
    "                # Forward pass generator , create a set of boards\n",
    "                generator.eval()\n",
    "                evaluator.train()\n",
    "                boards = generator(x, y)\n",
    "                \n",
    "                # Forward pass evaluator\n",
    "                predicted_metrics = evaluator(boards)\n",
    "                \n",
    "                # Compute True Metrics\n",
    "                binarized_boards = torch.argmax(boards, dim=3, keepdim=False).data.numpy()\n",
    "                true_metrics = torch.from_numpy(np.sum(binarized_boards == 2, axis = (1,2)).reshape(boards.size()[0],1)).float()\n",
    "\n",
    "                # Calculate E's loss\n",
    "                optimizer_e.zero_grad()\n",
    "                err_e = mse_criterion(predicted_metrics, true_metrics)\n",
    "                evaluator_losses.append(err_e.item())\n",
    "                \n",
    "                # Calculate the gradients for E\n",
    "                err_e.backward()\n",
    " \n",
    "                # Update E\n",
    "                optimizer_e.step()\n",
    "                print('E loss: ', np.mean(evaluator_losses))\n",
    "            \n",
    "            ###############################################\n",
    "            # Update G\n",
    "            ##############################################\n",
    "                    \n",
    "            # train genertor, and set evalutor to evalate only\n",
    "            generator.train()\n",
    "            evaluator.eval()\n",
    "            \n",
    "            # pass output directly from G to E\n",
    "            boards = generator(x, y)\n",
    "            predicted_metrics = evaluator(boards)\n",
    "            \n",
    "            # Calculate G's MSE loss\n",
    "            optimizer_g.zero_grad() \n",
    "            \n",
    "            # we may need to build categorical distrubtion per each tile\n",
    "            # for now, let us use all cells in the tensor to build the categorical distrubition\n",
    "            # I don't think this should cause a problem\n",
    "            # flatten = boards.flatten(start_dim = 1)\n",
    "            # entropy = categorical.Categorical(flatten).entropy()\n",
    "\n",
    "            err_g = mse_criterion(y, predicted_metrics)\n",
    "            \n",
    "            \n",
    "            err_g.backward()\n",
    "            optimizer_g.step()\n",
    "            generator_losses.append(err_g.item())\n",
    "            \n",
    "            #print(generator.fc4.weight.grad)\n",
    "            #print(generator.fc3.weight.grad)\n",
    "            #print(generator.fc2.weight.grad)\n",
    "            #print(generator.fc1.weight.grad)\n",
    "                  \n",
    "            print('G loss:', np.mean(generator_losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train A model that can output a level with ONE Banana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T22:00:41.671002Z",
     "start_time": "2020-03-07T22:00:18.971478Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0239b7beff8148e9869570d272563a11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model  ...\n",
      "E loss:  17.462127685546875\n",
      "G loss: 1.0908936262130737\n",
      "E loss:  14.101381301879883\n",
      "G loss: 1.047071099281311\n",
      "E loss:  9.459564208984375\n",
      "G loss: 0.9912164211273193\n",
      "E loss:  9.147956848144531\n",
      "G loss: 0.926108717918396\n",
      "E loss:  9.501867294311523\n",
      "G loss: 0.8533459901809692\n",
      "Training Model  ...\n",
      "E loss:  9.214071273803711\n",
      "G loss: 0.8376435041427612\n",
      "E loss:  9.186407089233398\n",
      "G loss: 0.8084427118301392\n",
      "E loss:  9.009054183959961\n",
      "G loss: 0.7678957581520081\n",
      "E loss:  8.63427448272705\n",
      "G loss: 0.7182407379150391\n",
      "E loss:  8.491325378417969\n",
      "G loss: 0.6615898609161377\n",
      "Training Model  ...\n",
      "E loss:  7.685627460479736\n",
      "G loss: 0.6488833427429199\n",
      "E loss:  7.507389545440674\n",
      "G loss: 0.6254515051841736\n",
      "E loss:  7.5054731369018555\n",
      "G loss: 0.5926573276519775\n",
      "E loss:  7.416515827178955\n",
      "G loss: 0.5524371862411499\n",
      "E loss:  7.345907211303711\n",
      "G loss: 0.506517767906189\n",
      "Training Model  ...\n",
      "E loss:  7.441073417663574\n",
      "G loss: 0.4956045150756836\n",
      "E loss:  7.345254898071289\n",
      "G loss: 0.4752200245857239\n",
      "E loss:  7.439686298370361\n",
      "G loss: 0.44679778814315796\n",
      "E loss:  7.550806522369385\n",
      "G loss: 0.4119527041912079\n",
      "E loss:  7.566043853759766\n",
      "G loss: 0.3718554973602295\n",
      "Training Model  ...\n",
      "E loss:  6.787111282348633\n",
      "G loss: 0.36267799139022827\n",
      "E loss:  6.848409175872803\n",
      "G loss: 0.34554988145828247\n",
      "E loss:  6.843298435211182\n",
      "G loss: 0.3216981291770935\n",
      "E loss:  6.968278884887695\n",
      "G loss: 0.2924174666404724\n",
      "E loss:  6.869229316711426\n",
      "G loss: 0.2590950131416321\n",
      "Training Model  ...\n",
      "E loss:  7.303211688995361\n",
      "G loss: 0.2509474754333496\n",
      "E loss:  7.4114251136779785\n",
      "G loss: 0.23562218248844147\n",
      "E loss:  7.396392822265625\n",
      "G loss: 0.21447767317295074\n",
      "E loss:  7.386631011962891\n",
      "G loss: 0.18884433805942535\n",
      "E loss:  7.316277980804443\n",
      "G loss: 0.16028566658496857\n",
      "Training Model  ...\n",
      "E loss:  6.859148979187012\n",
      "G loss: 0.15384285151958466\n",
      "E loss:  6.918522834777832\n",
      "G loss: 0.142004132270813\n",
      "E loss:  6.877866744995117\n",
      "G loss: 0.12564025819301605\n",
      "E loss:  6.7422404289245605\n",
      "G loss: 0.10629754513502121\n",
      "E loss:  6.648723602294922\n",
      "G loss: 0.08516194671392441\n",
      "Training Model  ...\n",
      "E loss:  6.882227420806885\n",
      "G loss: 0.08029885590076447\n",
      "E loss:  6.923913478851318\n",
      "G loss: 0.07142738252878189\n",
      "E loss:  6.927277088165283\n",
      "G loss: 0.05950252711772919\n",
      "E loss:  6.8569722175598145\n",
      "G loss: 0.045855484902858734\n",
      "E loss:  6.794106960296631\n",
      "G loss: 0.03181915730237961\n",
      "Training Model  ...\n",
      "E loss:  6.427701473236084\n",
      "G loss: 0.02884964644908905\n",
      "E loss:  6.5089311599731445\n",
      "G loss: 0.023539848625659943\n",
      "E loss:  6.480719566345215\n",
      "G loss: 0.016824722290039062\n",
      "E loss:  6.378724098205566\n",
      "G loss: 0.009861314669251442\n",
      "E loss:  6.285465717315674\n",
      "G loss: 0.003954292740672827\n",
      "Training Model  ...\n",
      "E loss:  6.1309309005737305\n",
      "G loss: 0.0028956274036318064\n",
      "E loss:  6.1036787033081055\n",
      "G loss: 0.0013416243018582463\n",
      "E loss:  6.070620059967041\n",
      "G loss: 0.00014362626825459301\n",
      "E loss:  5.992203712463379\n",
      "G loss: 0.0003880987351294607\n",
      "E loss:  5.8550262451171875\n",
      "G loss: 0.0033541489392518997\n",
      "Training Model  ...\n",
      "E loss:  5.4448723793029785\n",
      "G loss: 0.004506599623709917\n",
      "E loss:  5.327242374420166\n",
      "G loss: 0.007130391895771027\n",
      "E loss:  5.270820140838623\n",
      "G loss: 0.011939121410250664\n",
      "E loss:  5.241241931915283\n",
      "G loss: 0.01995021104812622\n",
      "E loss:  5.126250743865967\n",
      "G loss: 0.03230574354529381\n",
      "Training Model  ...\n",
      "E loss:  5.171645164489746\n",
      "G loss: 0.03594265878200531\n",
      "E loss:  5.172493934631348\n",
      "G loss: 0.043350208550691605\n",
      "E loss:  5.127745628356934\n",
      "G loss: 0.055168524384498596\n",
      "E loss:  4.920873641967773\n",
      "G loss: 0.07262536883354187\n",
      "E loss:  4.764538764953613\n",
      "G loss: 0.09656496345996857\n",
      "Training Model  ...\n",
      "E loss:  4.461709976196289\n",
      "G loss: 0.10270780324935913\n",
      "E loss:  4.5201544761657715\n",
      "G loss: 0.11488618701696396\n",
      "E loss:  4.473045349121094\n",
      "G loss: 0.1337074339389801\n",
      "E loss:  4.400787830352783\n",
      "G loss: 0.15994462370872498\n",
      "E loss:  4.304533958435059\n",
      "G loss: 0.1946188509464264\n",
      "Training Model  ...\n",
      "E loss:  4.074670791625977\n",
      "G loss: 0.20349468290805817\n",
      "E loss:  4.1159539222717285\n",
      "G loss: 0.22068068385124207\n",
      "E loss:  4.096278667449951\n",
      "G loss: 0.24674664437770844\n",
      "E loss:  4.010847091674805\n",
      "G loss: 0.28214162588119507\n",
      "E loss:  3.930494546890259\n",
      "G loss: 0.32845133543014526\n",
      "Training Model  ...\n",
      "E loss:  3.997096538543701\n",
      "G loss: 0.34017908573150635\n",
      "E loss:  3.948547124862671\n",
      "G loss: 0.3634112775325775\n",
      "E loss:  3.8431649208068848\n",
      "G loss: 0.3975241184234619\n",
      "E loss:  3.8063788414001465\n",
      "G loss: 0.44398021697998047\n",
      "E loss:  3.7311630249023438\n",
      "G loss: 0.502927303314209\n",
      "Training Model  ...\n",
      "E loss:  3.567810535430908\n",
      "G loss: 0.5180786848068237\n",
      "E loss:  3.4998035430908203\n",
      "G loss: 0.5469557046890259\n",
      "E loss:  3.4969139099121094\n",
      "G loss: 0.5907222032546997\n",
      "E loss:  3.3954625129699707\n",
      "G loss: 0.6484336853027344\n",
      "E loss:  3.2512669563293457\n",
      "G loss: 0.7214767932891846\n",
      "Training Model  ...\n",
      "E loss:  3.160078287124634\n",
      "G loss: 0.7391079664230347\n",
      "E loss:  3.155235767364502\n",
      "G loss: 0.7744718790054321\n",
      "E loss:  3.1103038787841797\n",
      "G loss: 0.8253965377807617\n",
      "E loss:  3.0000500679016113\n",
      "G loss: 0.8922417163848877\n",
      "E loss:  2.94185471534729\n",
      "G loss: 0.9765230417251587\n",
      "Training Model  ...\n",
      "E loss:  2.4631614685058594\n",
      "G loss: 0.9960238933563232\n",
      "E loss:  2.4724931716918945\n",
      "G loss: 1.0336259603500366\n",
      "E loss:  2.46171498298645\n",
      "G loss: 1.08806574344635\n",
      "E loss:  2.4449405670166016\n",
      "G loss: 1.1603171825408936\n",
      "E loss:  2.378230094909668\n",
      "G loss: 1.2513326406478882\n",
      "Training Model  ...\n",
      "E loss:  2.4547927379608154\n",
      "G loss: 1.2734246253967285\n",
      "E loss:  2.4222471714019775\n",
      "G loss: 1.3153083324432373\n",
      "E loss:  2.4296886920928955\n",
      "G loss: 1.3782224655151367\n",
      "E loss:  2.3860034942626953\n",
      "G loss: 1.4601999521255493\n",
      "E loss:  2.3432374000549316\n",
      "G loss: 1.561667561531067\n",
      "Training Model  ...\n",
      "E loss:  2.1604599952697754\n",
      "G loss: 1.5856871604919434\n",
      "E loss:  2.1600754261016846\n",
      "G loss: 1.6295255422592163\n",
      "E loss:  2.11287522315979\n",
      "G loss: 1.695379614830017\n",
      "E loss:  2.104604482650757\n",
      "G loss: 1.7798786163330078\n",
      "E loss:  2.0586771965026855\n",
      "G loss: 1.8854244947433472\n",
      "Training Model  ...\n",
      "E loss:  2.0213074684143066\n",
      "G loss: 1.909488558769226\n",
      "E loss:  2.0462121963500977\n",
      "G loss: 1.9575116634368896\n",
      "E loss:  2.0315771102905273\n",
      "G loss: 2.026613235473633\n",
      "E loss:  1.9900436401367188\n",
      "G loss: 2.1165223121643066\n",
      "E loss:  1.9623596668243408\n",
      "G loss: 2.2281792163848877\n",
      "Training Model  ...\n",
      "E loss:  2.2063605785369873\n",
      "G loss: 2.2535572052001953\n",
      "E loss:  2.155810832977295\n",
      "G loss: 2.303696870803833\n",
      "E loss:  2.1311838626861572\n",
      "G loss: 2.379978656768799\n",
      "E loss:  2.1400938034057617\n",
      "G loss: 2.4762871265411377\n",
      "E loss:  2.1018571853637695\n",
      "G loss: 2.590211868286133\n",
      "Training Model  ...\n",
      "E loss:  1.843540906906128\n",
      "G loss: 2.613379716873169\n",
      "E loss:  1.8272775411605835\n",
      "G loss: 2.660088062286377\n",
      "E loss:  1.841426134109497\n",
      "G loss: 2.721552848815918\n",
      "E loss:  1.795353651046753\n",
      "G loss: 2.804246664047241\n",
      "E loss:  1.7662477493286133\n",
      "G loss: 2.9014434814453125\n",
      "Training Model  ...\n",
      "E loss:  1.8420584201812744\n",
      "G loss: 2.927854537963867\n",
      "E loss:  1.8194233179092407\n",
      "G loss: 2.977506160736084\n",
      "E loss:  1.8136367797851562\n",
      "G loss: 3.0465924739837646\n",
      "E loss:  1.766729474067688\n",
      "G loss: 3.1308648586273193\n",
      "E loss:  1.750413179397583\n",
      "G loss: 3.23903751373291\n",
      "Training Model  ...\n",
      "E loss:  1.7960296869277954\n",
      "G loss: 3.2496509552001953\n",
      "E loss:  1.8170497417449951\n",
      "G loss: 3.2782015800476074\n",
      "E loss:  1.8015013933181763\n",
      "G loss: 3.319424867630005\n",
      "E loss:  1.8448569774627686\n",
      "G loss: 3.369121551513672\n",
      "E loss:  1.8089781999588013\n",
      "G loss: 3.4329638481140137\n",
      "Training Model  ...\n",
      "E loss:  1.744389533996582\n",
      "G loss: 3.4444944858551025\n",
      "E loss:  1.7489229440689087\n",
      "G loss: 3.4638519287109375\n",
      "E loss:  1.7557802200317383\n",
      "G loss: 3.4913809299468994\n",
      "E loss:  1.7436341047286987\n",
      "G loss: 3.5311131477355957\n",
      "E loss:  1.761427879333496\n",
      "G loss: 3.5742697715759277\n",
      "Training Model  ...\n",
      "E loss:  1.640000820159912\n",
      "G loss: 3.5912108421325684\n",
      "E loss:  1.6541671752929688\n",
      "G loss: 3.613795757293701\n",
      "E loss:  1.6264865398406982\n",
      "G loss: 3.651348829269409\n",
      "E loss:  1.6519774198532104\n",
      "G loss: 3.69260573387146\n",
      "E loss:  1.6426414251327515\n",
      "G loss: 3.750558376312256\n",
      "Training Model  ...\n",
      "E loss:  1.7243647575378418\n",
      "G loss: 3.7596025466918945\n",
      "E loss:  1.704349160194397\n",
      "G loss: 3.7833847999572754\n",
      "E loss:  1.7188729047775269\n",
      "G loss: 3.81977915763855\n",
      "E loss:  1.7537693977355957\n",
      "G loss: 3.86281156539917\n",
      "E loss:  1.7719440460205078\n",
      "G loss: 3.9152233600616455\n",
      "Training Model  ...\n",
      "E loss:  1.6466290950775146\n",
      "G loss: 3.920762300491333\n",
      "E loss:  1.6464601755142212\n",
      "G loss: 3.932936906814575\n",
      "E loss:  1.6186408996582031\n",
      "G loss: 3.948035955429077\n",
      "E loss:  1.5824613571166992\n",
      "G loss: 3.9657673835754395\n",
      "E loss:  1.583810567855835\n",
      "G loss: 3.987933397293091\n",
      "Training Model  ...\n",
      "E loss:  1.707619071006775\n",
      "G loss: 3.9920527935028076\n",
      "E loss:  1.697378396987915\n",
      "G loss: 4.010876178741455\n",
      "E loss:  1.6652641296386719\n",
      "G loss: 4.027462482452393\n",
      "E loss:  1.6816456317901611\n",
      "G loss: 4.060577392578125\n",
      "E loss:  1.6818870306015015\n",
      "G loss: 4.087928295135498\n",
      "Training Model  ...\n",
      "E loss:  1.47857666015625\n",
      "G loss: 4.090427875518799\n",
      "E loss:  1.4358041286468506\n",
      "G loss: 4.098018646240234\n",
      "E loss:  1.4153462648391724\n",
      "G loss: 4.104926109313965\n",
      "E loss:  1.408648133277893\n",
      "G loss: 4.119645595550537\n",
      "E loss:  1.3932642936706543\n",
      "G loss: 4.132878303527832\n",
      "Training Model  ...\n",
      "E loss:  1.5872222185134888\n",
      "G loss: 4.131418228149414\n",
      "E loss:  1.598750114440918\n",
      "G loss: 4.134952068328857\n",
      "E loss:  1.5850409269332886\n",
      "G loss: 4.139935493469238\n",
      "E loss:  1.5886313915252686\n",
      "G loss: 4.148808479309082\n",
      "E loss:  1.6190260648727417\n",
      "G loss: 4.155001163482666\n",
      "Training Model  ...\n",
      "E loss:  1.5701881647109985\n",
      "G loss: 4.168197154998779\n",
      "E loss:  1.6071816682815552\n",
      "G loss: 4.1928582191467285\n",
      "E loss:  1.6278187036514282\n",
      "G loss: 4.21873140335083\n",
      "E loss:  1.6428226232528687\n",
      "G loss: 4.257303237915039\n",
      "E loss:  1.6311200857162476\n",
      "G loss: 4.291926383972168\n",
      "Training Model  ...\n",
      "E loss:  1.5005295276641846\n",
      "G loss: 4.292468070983887\n",
      "E loss:  1.4652172327041626\n",
      "G loss: 4.291493892669678\n",
      "E loss:  1.4877004623413086\n",
      "G loss: 4.285336494445801\n",
      "E loss:  1.474278211593628\n",
      "G loss: 4.281891822814941\n",
      "E loss:  1.4976322650909424\n",
      "G loss: 4.276712417602539\n",
      "Training Model  ...\n",
      "E loss:  1.7115118503570557\n",
      "G loss: 4.2747802734375\n",
      "E loss:  1.712119460105896\n",
      "G loss: 4.281703472137451\n",
      "E loss:  1.7168779373168945\n",
      "G loss: 4.292257785797119\n",
      "E loss:  1.7299984693527222\n",
      "G loss: 4.299989223480225\n",
      "E loss:  1.7031861543655396\n",
      "G loss: 4.310853958129883\n",
      "Training Model  ...\n",
      "E loss:  1.5042847394943237\n",
      "G loss: 4.311228275299072\n",
      "E loss:  1.4889634847640991\n",
      "G loss: 4.311386585235596\n",
      "E loss:  1.5406724214553833\n",
      "G loss: 4.317843437194824\n",
      "E loss:  1.5288971662521362\n",
      "G loss: 4.31428337097168\n",
      "E loss:  1.5553455352783203\n",
      "G loss: 4.316323757171631\n",
      "Training Model  ...\n",
      "E loss:  1.5619655847549438\n",
      "G loss: 4.317811012268066\n",
      "E loss:  1.572219729423523\n",
      "G loss: 4.323885440826416\n",
      "E loss:  1.5665687322616577\n",
      "G loss: 4.331904411315918\n",
      "E loss:  1.5673491954803467\n",
      "G loss: 4.347458839416504\n",
      "E loss:  1.571018099784851\n",
      "G loss: 4.358896255493164\n",
      "Training Model  ...\n",
      "E loss:  1.7435212135314941\n",
      "G loss: 4.3497419357299805\n",
      "E loss:  1.752071499824524\n",
      "G loss: 4.334569931030273\n",
      "E loss:  1.7572805881500244\n",
      "G loss: 4.319448471069336\n",
      "E loss:  1.7754418849945068\n",
      "G loss: 4.301424026489258\n",
      "E loss:  1.7743422985076904\n",
      "G loss: 4.275850296020508\n",
      "Training Model  ...\n",
      "E loss:  1.607431411743164\n",
      "G loss: 4.282814979553223\n",
      "E loss:  1.6034936904907227\n",
      "G loss: 4.294360160827637\n",
      "E loss:  1.584817886352539\n",
      "G loss: 4.302427291870117\n",
      "E loss:  1.6012582778930664\n",
      "G loss: 4.31491231918335\n",
      "E loss:  1.6196298599243164\n",
      "G loss: 4.327796459197998\n",
      "Training Model  ...\n",
      "E loss:  1.7882249355316162\n",
      "G loss: 4.324373722076416\n",
      "E loss:  1.7681574821472168\n",
      "G loss: 4.326719284057617\n",
      "E loss:  1.7689837217330933\n",
      "G loss: 4.322829246520996\n",
      "E loss:  1.817358136177063\n",
      "G loss: 4.315255641937256\n",
      "E loss:  1.7862106561660767\n",
      "G loss: 4.303206443786621\n",
      "Training Model  ...\n",
      "E loss:  1.6321096420288086\n",
      "G loss: 4.309745788574219\n",
      "E loss:  1.6369531154632568\n",
      "G loss: 4.307160377502441\n",
      "E loss:  1.6479986906051636\n",
      "G loss: 4.301889419555664\n",
      "E loss:  1.66554856300354\n",
      "G loss: 4.2966413497924805\n",
      "E loss:  1.6755189895629883\n",
      "G loss: 4.290455341339111\n",
      "Training Model  ...\n",
      "E loss:  1.6555368900299072\n",
      "G loss: 4.298442363739014\n",
      "E loss:  1.624588966369629\n",
      "G loss: 4.312549591064453\n",
      "E loss:  1.5969966650009155\n",
      "G loss: 4.3314208984375\n",
      "E loss:  1.5948913097381592\n",
      "G loss: 4.347062587738037\n",
      "E loss:  1.6077308654785156\n",
      "G loss: 4.374235153198242\n",
      "Training Model  ...\n",
      "E loss:  1.3765738010406494\n",
      "G loss: 4.369693279266357\n",
      "E loss:  1.378676176071167\n",
      "G loss: 4.373102188110352\n",
      "E loss:  1.4075515270233154\n",
      "G loss: 4.361482620239258\n",
      "E loss:  1.4209380149841309\n",
      "G loss: 4.357244968414307\n",
      "E loss:  1.468524694442749\n",
      "G loss: 4.3452229499816895\n",
      "Training Model  ...\n",
      "E loss:  1.4737591743469238\n",
      "G loss: 4.3506646156311035\n",
      "E loss:  1.478655219078064\n",
      "G loss: 4.357132911682129\n",
      "E loss:  1.485646367073059\n",
      "G loss: 4.370599746704102\n",
      "E loss:  1.509464979171753\n",
      "G loss: 4.381603240966797\n",
      "E loss:  1.5029895305633545\n",
      "G loss: 4.3963470458984375\n",
      "Training Model  ...\n",
      "E loss:  1.603592038154602\n",
      "G loss: 4.392467498779297\n",
      "E loss:  1.5998730659484863\n",
      "G loss: 4.380746841430664\n",
      "E loss:  1.6034497022628784\n",
      "G loss: 4.365435600280762\n",
      "E loss:  1.6359033584594727\n",
      "G loss: 4.349055290222168\n",
      "E loss:  1.6155788898468018\n",
      "G loss: 4.331786155700684\n",
      "Training Model  ...\n",
      "E loss:  1.627210259437561\n",
      "G loss: 4.331799030303955\n",
      "E loss:  1.651424527168274\n",
      "G loss: 4.332170486450195\n",
      "E loss:  1.6429287195205688\n",
      "G loss: 4.337937355041504\n",
      "E loss:  1.651323914527893\n",
      "G loss: 4.3418779373168945\n",
      "E loss:  1.6649928092956543\n",
      "G loss: 4.3436055183410645\n",
      "Training Model  ...\n",
      "E loss:  1.6066267490386963\n",
      "G loss: 4.347998142242432\n",
      "E loss:  1.6067103147506714\n",
      "G loss: 4.356461048126221\n",
      "E loss:  1.6124247312545776\n",
      "G loss: 4.375792980194092\n",
      "E loss:  1.6182281970977783\n",
      "G loss: 4.38668966293335\n",
      "E loss:  1.6537847518920898\n",
      "G loss: 4.413844108581543\n",
      "Training Model  ...\n",
      "E loss:  1.7375645637512207\n",
      "G loss: 4.415534019470215\n",
      "E loss:  1.6854355335235596\n",
      "G loss: 4.423145294189453\n",
      "E loss:  1.6675500869750977\n",
      "G loss: 4.41955041885376\n",
      "E loss:  1.6552802324295044\n",
      "G loss: 4.421583652496338\n",
      "E loss:  1.6398367881774902\n",
      "G loss: 4.4153361320495605\n",
      "Training Model  ...\n",
      "E loss:  1.5307719707489014\n",
      "G loss: 4.422257423400879\n",
      "E loss:  1.5374683141708374\n",
      "G loss: 4.428364276885986\n",
      "E loss:  1.5076990127563477\n",
      "G loss: 4.435644149780273\n",
      "E loss:  1.475294828414917\n",
      "G loss: 4.453371524810791\n",
      "E loss:  1.4687180519104004\n",
      "G loss: 4.456404685974121\n",
      "Training Model  ...\n",
      "E loss:  1.6896262168884277\n",
      "G loss: 4.45849609375\n",
      "E loss:  1.690645456314087\n",
      "G loss: 4.459059715270996\n",
      "E loss:  1.7080199718475342\n",
      "G loss: 4.461363792419434\n",
      "E loss:  1.6987277269363403\n",
      "G loss: 4.451972007751465\n",
      "E loss:  1.6985225677490234\n",
      "G loss: 4.445745468139648\n",
      "Training Model  ...\n",
      "E loss:  1.7127567529678345\n",
      "G loss: 4.446633815765381\n",
      "E loss:  1.718317985534668\n",
      "G loss: 4.443079471588135\n",
      "E loss:  1.7229325771331787\n",
      "G loss: 4.434582710266113\n",
      "E loss:  1.6526929140090942\n",
      "G loss: 4.424442768096924\n",
      "E loss:  1.6502151489257812\n",
      "G loss: 4.418493270874023\n",
      "Training Model  ...\n",
      "E loss:  1.3833699226379395\n",
      "G loss: 4.404610633850098\n",
      "E loss:  1.4117708206176758\n",
      "G loss: 4.390393257141113\n",
      "E loss:  1.4179892539978027\n",
      "G loss: 4.36322021484375\n",
      "E loss:  1.396643042564392\n",
      "G loss: 4.334806442260742\n",
      "E loss:  1.39750337600708\n",
      "G loss: 4.30184268951416\n",
      "Training Model  ...\n",
      "E loss:  1.706076741218567\n",
      "G loss: 4.297966003417969\n",
      "E loss:  1.699169397354126\n",
      "G loss: 4.296077251434326\n",
      "E loss:  1.6972856521606445\n",
      "G loss: 4.2838897705078125\n",
      "E loss:  1.6884500980377197\n",
      "G loss: 4.277315139770508\n",
      "E loss:  1.6962459087371826\n",
      "G loss: 4.266934394836426\n",
      "Training Model  ...\n",
      "E loss:  1.6022429466247559\n",
      "G loss: 4.277824401855469\n",
      "E loss:  1.5832786560058594\n",
      "G loss: 4.2962565422058105\n",
      "E loss:  1.5484217405319214\n",
      "G loss: 4.317185878753662\n",
      "E loss:  1.5427210330963135\n",
      "G loss: 4.344639778137207\n",
      "E loss:  1.5669749975204468\n",
      "G loss: 4.378884315490723\n",
      "Training Model  ...\n",
      "E loss:  1.5295406579971313\n",
      "G loss: 4.381356239318848\n",
      "E loss:  1.5249664783477783\n",
      "G loss: 4.368254661560059\n",
      "E loss:  1.5311925411224365\n",
      "G loss: 4.369655132293701\n",
      "E loss:  1.5475307703018188\n",
      "G loss: 4.363517761230469\n",
      "E loss:  1.5552763938903809\n",
      "G loss: 4.354455471038818\n",
      "Training Model  ...\n",
      "E loss:  1.6013485193252563\n",
      "G loss: 4.355663776397705\n",
      "E loss:  1.5826255083084106\n",
      "G loss: 4.349736213684082\n",
      "E loss:  1.5897204875946045\n",
      "G loss: 4.343634605407715\n",
      "E loss:  1.5804500579833984\n",
      "G loss: 4.341308116912842\n",
      "E loss:  1.5686297416687012\n",
      "G loss: 4.339574337005615\n",
      "Training Model  ...\n",
      "E loss:  1.5694782733917236\n",
      "G loss: 4.334029674530029\n",
      "E loss:  1.582573413848877\n",
      "G loss: 4.325326919555664\n",
      "E loss:  1.5781490802764893\n",
      "G loss: 4.308255672454834\n",
      "E loss:  1.5710951089859009\n",
      "G loss: 4.293716907501221\n",
      "E loss:  1.5890698432922363\n",
      "G loss: 4.283151149749756\n",
      "Training Model  ...\n",
      "E loss:  1.6438548564910889\n",
      "G loss: 4.287892818450928\n",
      "E loss:  1.6518276929855347\n",
      "G loss: 4.2982258796691895\n",
      "E loss:  1.6542329788208008\n",
      "G loss: 4.311408996582031\n",
      "E loss:  1.640122413635254\n",
      "G loss: 4.331057071685791\n",
      "E loss:  1.6382596492767334\n",
      "G loss: 4.352703094482422\n",
      "Training Model  ...\n",
      "E loss:  1.657569169998169\n",
      "G loss: 4.356351852416992\n",
      "E loss:  1.6426244974136353\n",
      "G loss: 4.357110977172852\n",
      "E loss:  1.6408722400665283\n",
      "G loss: 4.355982303619385\n",
      "E loss:  1.593078851699829\n",
      "G loss: 4.358052730560303\n",
      "E loss:  1.5869327783584595\n",
      "G loss: 4.354657173156738\n",
      "Training Model  ...\n",
      "E loss:  1.4667075872421265\n",
      "G loss: 4.359762668609619\n",
      "E loss:  1.4628933668136597\n",
      "G loss: 4.36798095703125\n",
      "E loss:  1.4667974710464478\n",
      "G loss: 4.374119281768799\n",
      "E loss:  1.4572913646697998\n",
      "G loss: 4.387299537658691\n",
      "E loss:  1.4671350717544556\n",
      "G loss: 4.402705192565918\n",
      "Training Model  ...\n",
      "E loss:  1.6830708980560303\n",
      "G loss: 4.411815643310547\n",
      "E loss:  1.6955469846725464\n",
      "G loss: 4.4161481857299805\n",
      "E loss:  1.689380407333374\n",
      "G loss: 4.426149845123291\n",
      "E loss:  1.6773971319198608\n",
      "G loss: 4.439002513885498\n",
      "E loss:  1.6828231811523438\n",
      "G loss: 4.447842597961426\n",
      "Training Model  ...\n",
      "E loss:  1.7180094718933105\n",
      "G loss: 4.43948221206665\n",
      "E loss:  1.7375283241271973\n",
      "G loss: 4.419406890869141\n",
      "E loss:  1.7044211626052856\n",
      "G loss: 4.3974103927612305\n",
      "E loss:  1.7158030271530151\n",
      "G loss: 4.362350940704346\n",
      "E loss:  1.7129266262054443\n",
      "G loss: 4.325498580932617\n",
      "Training Model  ...\n",
      "E loss:  1.488649845123291\n",
      "G loss: 4.326205730438232\n",
      "E loss:  1.4759283065795898\n",
      "G loss: 4.325198173522949\n",
      "E loss:  1.44411301612854\n",
      "G loss: 4.325735092163086\n",
      "E loss:  1.4456356763839722\n",
      "G loss: 4.323904991149902\n",
      "E loss:  1.4703634977340698\n",
      "G loss: 4.321896553039551\n",
      "Training Model  ...\n",
      "E loss:  1.6663686037063599\n",
      "G loss: 4.322505474090576\n",
      "E loss:  1.6770431995391846\n",
      "G loss: 4.317131042480469\n",
      "E loss:  1.695073127746582\n",
      "G loss: 4.319592475891113\n",
      "E loss:  1.7116425037384033\n",
      "G loss: 4.32228946685791\n",
      "E loss:  1.701620101928711\n",
      "G loss: 4.324706554412842\n",
      "Training Model  ...\n",
      "E loss:  1.5084660053253174\n",
      "G loss: 4.323963165283203\n",
      "E loss:  1.475803256034851\n",
      "G loss: 4.334609031677246\n",
      "E loss:  1.4758106470108032\n",
      "G loss: 4.337040424346924\n",
      "E loss:  1.4833699464797974\n",
      "G loss: 4.347192287445068\n",
      "E loss:  1.4759488105773926\n",
      "G loss: 4.353668689727783\n",
      "Training Model  ...\n",
      "E loss:  1.4579256772994995\n",
      "G loss: 4.3444647789001465\n",
      "E loss:  1.4485828876495361\n",
      "G loss: 4.324651718139648\n",
      "E loss:  1.4447237253189087\n",
      "G loss: 4.300771236419678\n",
      "E loss:  1.474596619606018\n",
      "G loss: 4.268369197845459\n",
      "E loss:  1.4707989692687988\n",
      "G loss: 4.236146926879883\n",
      "Training Model  ...\n",
      "E loss:  1.6488616466522217\n",
      "G loss: 4.2446794509887695\n",
      "E loss:  1.5964571237564087\n",
      "G loss: 4.258484840393066\n",
      "E loss:  1.5999008417129517\n",
      "G loss: 4.280432224273682\n",
      "E loss:  1.6026266813278198\n",
      "G loss: 4.300407886505127\n",
      "E loss:  1.598310947418213\n",
      "G loss: 4.333131790161133\n",
      "Training Model  ...\n",
      "E loss:  1.5684863328933716\n",
      "G loss: 4.326905727386475\n",
      "E loss:  1.5773953199386597\n",
      "G loss: 4.324547290802002\n",
      "E loss:  1.5825161933898926\n",
      "G loss: 4.31787633895874\n",
      "E loss:  1.560657262802124\n",
      "G loss: 4.310734748840332\n",
      "E loss:  1.5431824922561646\n",
      "G loss: 4.30308723449707\n",
      "Training Model  ...\n",
      "E loss:  1.4730191230773926\n",
      "G loss: 4.306236743927002\n",
      "E loss:  1.48367440700531\n",
      "G loss: 4.312873840332031\n",
      "E loss:  1.4642366170883179\n",
      "G loss: 4.316727638244629\n",
      "E loss:  1.4860976934432983\n",
      "G loss: 4.327414035797119\n",
      "E loss:  1.49837064743042\n",
      "G loss: 4.342018127441406\n",
      "Training Model  ...\n",
      "E loss:  1.6221349239349365\n",
      "G loss: 4.339087009429932\n",
      "E loss:  1.6324284076690674\n",
      "G loss: 4.335638046264648\n",
      "E loss:  1.6298452615737915\n",
      "G loss: 4.336612224578857\n",
      "E loss:  1.6613165140151978\n",
      "G loss: 4.3338212966918945\n",
      "E loss:  1.6543241739273071\n",
      "G loss: 4.328702926635742\n",
      "Training Model  ...\n",
      "E loss:  1.6766462326049805\n",
      "G loss: 4.3412628173828125\n",
      "E loss:  1.6756579875946045\n",
      "G loss: 4.361474990844727\n",
      "E loss:  1.7061790227890015\n",
      "G loss: 4.392917633056641\n",
      "E loss:  1.6949059963226318\n",
      "G loss: 4.426726818084717\n",
      "E loss:  1.702142596244812\n",
      "G loss: 4.464574337005615\n",
      "Training Model  ...\n",
      "E loss:  1.6559457778930664\n",
      "G loss: 4.457664489746094\n",
      "E loss:  1.6400526762008667\n",
      "G loss: 4.451529502868652\n",
      "E loss:  1.6404281854629517\n",
      "G loss: 4.436409950256348\n",
      "E loss:  1.6214661598205566\n",
      "G loss: 4.414981842041016\n",
      "E loss:  1.650001883506775\n",
      "G loss: 4.401847839355469\n",
      "Training Model  ...\n",
      "E loss:  1.5502979755401611\n",
      "G loss: 4.399291038513184\n",
      "E loss:  1.5237458944320679\n",
      "G loss: 4.397795677185059\n",
      "E loss:  1.530751347541809\n",
      "G loss: 4.396505355834961\n",
      "E loss:  1.528349757194519\n",
      "G loss: 4.390612602233887\n",
      "E loss:  1.5259227752685547\n",
      "G loss: 4.391907691955566\n",
      "Training Model  ...\n",
      "E loss:  1.4731285572052002\n",
      "G loss: 4.387423038482666\n",
      "E loss:  1.4465333223342896\n",
      "G loss: 4.396312236785889\n",
      "E loss:  1.4501986503601074\n",
      "G loss: 4.406996726989746\n",
      "E loss:  1.4735136032104492\n",
      "G loss: 4.419463634490967\n",
      "E loss:  1.4667497873306274\n",
      "G loss: 4.430244445800781\n",
      "Training Model  ...\n",
      "E loss:  1.6245943307876587\n",
      "G loss: 4.421262741088867\n",
      "E loss:  1.65023934841156\n",
      "G loss: 4.403602123260498\n",
      "E loss:  1.653361439704895\n",
      "G loss: 4.378422737121582\n",
      "E loss:  1.6265380382537842\n",
      "G loss: 4.353995323181152\n",
      "E loss:  1.5917094945907593\n",
      "G loss: 4.31538200378418\n",
      "Training Model  ...\n",
      "E loss:  1.5358328819274902\n",
      "G loss: 4.315532684326172\n",
      "E loss:  1.5275582075119019\n",
      "G loss: 4.307319641113281\n",
      "E loss:  1.5035345554351807\n",
      "G loss: 4.300512313842773\n",
      "E loss:  1.4940398931503296\n",
      "G loss: 4.291522979736328\n",
      "E loss:  1.4911766052246094\n",
      "G loss: 4.2730393409729\n",
      "Training Model  ...\n",
      "E loss:  1.5757747888565063\n",
      "G loss: 4.284247398376465\n",
      "E loss:  1.5383453369140625\n",
      "G loss: 4.283148765563965\n",
      "E loss:  1.5342297554016113\n",
      "G loss: 4.294971942901611\n",
      "E loss:  1.5501713752746582\n",
      "G loss: 4.302663803100586\n",
      "E loss:  1.5240528583526611\n",
      "G loss: 4.317014217376709\n",
      "Training Model  ...\n",
      "E loss:  1.7689625024795532\n",
      "G loss: 4.318545818328857\n",
      "E loss:  1.7432429790496826\n",
      "G loss: 4.319814682006836\n",
      "E loss:  1.730472445487976\n",
      "G loss: 4.32316780090332\n",
      "E loss:  1.6946794986724854\n",
      "G loss: 4.323126792907715\n",
      "E loss:  1.7012485265731812\n",
      "G loss: 4.317680358886719\n",
      "Training Model  ...\n",
      "E loss:  1.5827717781066895\n",
      "G loss: 4.309050559997559\n",
      "E loss:  1.5737539529800415\n",
      "G loss: 4.287647724151611\n",
      "E loss:  1.6092133522033691\n",
      "G loss: 4.259714126586914\n",
      "E loss:  1.6192047595977783\n",
      "G loss: 4.227846622467041\n",
      "E loss:  1.6029307842254639\n",
      "G loss: 4.186803817749023\n",
      "Training Model  ...\n",
      "E loss:  1.637309193611145\n",
      "G loss: 4.187431335449219\n",
      "E loss:  1.6390974521636963\n",
      "G loss: 4.19681453704834\n",
      "E loss:  1.6597398519515991\n",
      "G loss: 4.201111793518066\n",
      "E loss:  1.633764386177063\n",
      "G loss: 4.207672595977783\n",
      "E loss:  1.6556289196014404\n",
      "G loss: 4.207585334777832\n",
      "Training Model  ...\n",
      "E loss:  1.5216577053070068\n",
      "G loss: 4.205630302429199\n",
      "E loss:  1.5232354402542114\n",
      "G loss: 4.206993103027344\n",
      "E loss:  1.5202717781066895\n",
      "G loss: 4.210211753845215\n",
      "E loss:  1.5249639749526978\n",
      "G loss: 4.2119975090026855\n",
      "E loss:  1.5407562255859375\n",
      "G loss: 4.209545135498047\n",
      "Training Model  ...\n",
      "E loss:  1.6266037225723267\n",
      "G loss: 4.214837074279785\n",
      "E loss:  1.6324517726898193\n",
      "G loss: 4.222022533416748\n",
      "E loss:  1.587954044342041\n",
      "G loss: 4.2366623878479\n",
      "E loss:  1.6222095489501953\n",
      "G loss: 4.255447864532471\n",
      "E loss:  1.6237127780914307\n",
      "G loss: 4.27689266204834\n",
      "Training Model  ...\n",
      "E loss:  1.348876714706421\n",
      "G loss: 4.269889831542969\n",
      "E loss:  1.3472517728805542\n",
      "G loss: 4.261860370635986\n",
      "E loss:  1.3595938682556152\n",
      "G loss: 4.260140895843506\n",
      "E loss:  1.3415567874908447\n",
      "G loss: 4.253195762634277\n",
      "E loss:  1.3550697565078735\n",
      "G loss: 4.2475385665893555\n",
      "Training Model  ...\n",
      "E loss:  1.4108893871307373\n",
      "G loss: 4.247289180755615\n",
      "E loss:  1.419377088546753\n",
      "G loss: 4.249897480010986\n",
      "E loss:  1.42294442653656\n",
      "G loss: 4.253108024597168\n",
      "E loss:  1.4335192441940308\n",
      "G loss: 4.251664638519287\n",
      "E loss:  1.4456603527069092\n",
      "G loss: 4.260619640350342\n",
      "Training Model  ...\n",
      "E loss:  1.5256341695785522\n",
      "G loss: 4.261837482452393\n",
      "E loss:  1.5090035200119019\n",
      "G loss: 4.2700018882751465\n",
      "E loss:  1.5038660764694214\n",
      "G loss: 4.2779364585876465\n",
      "E loss:  1.52375328540802\n",
      "G loss: 4.286508560180664\n",
      "E loss:  1.5079693794250488\n",
      "G loss: 4.300494194030762\n",
      "Training Model  ...\n",
      "E loss:  1.6036622524261475\n",
      "G loss: 4.306926250457764\n",
      "E loss:  1.591681957244873\n",
      "G loss: 4.322600364685059\n",
      "E loss:  1.6095012426376343\n",
      "G loss: 4.342074394226074\n",
      "E loss:  1.6287322044372559\n",
      "G loss: 4.372943878173828\n",
      "E loss:  1.6190208196640015\n",
      "G loss: 4.3962883949279785\n",
      "Training Model  ...\n",
      "E loss:  1.5736491680145264\n",
      "G loss: 4.388309478759766\n",
      "E loss:  1.577274203300476\n",
      "G loss: 4.371252059936523\n",
      "E loss:  1.5715833902359009\n",
      "G loss: 4.359229564666748\n",
      "E loss:  1.5968260765075684\n",
      "G loss: 4.33722448348999\n",
      "E loss:  1.550855278968811\n",
      "G loss: 4.314886093139648\n",
      "Training Model  ...\n",
      "E loss:  1.6267108917236328\n",
      "G loss: 4.317266464233398\n",
      "E loss:  1.6211827993392944\n",
      "G loss: 4.322380542755127\n",
      "E loss:  1.6619266271591187\n",
      "G loss: 4.328612804412842\n",
      "E loss:  1.6723145246505737\n",
      "G loss: 4.339055061340332\n",
      "E loss:  1.6778535842895508\n",
      "G loss: 4.362079620361328\n",
      "Training Model  ...\n",
      "E loss:  1.5238354206085205\n",
      "G loss: 4.353137493133545\n",
      "E loss:  1.5334782600402832\n",
      "G loss: 4.33685302734375\n",
      "E loss:  1.5211966037750244\n",
      "G loss: 4.3130717277526855\n",
      "E loss:  1.5211519002914429\n",
      "G loss: 4.290705680847168\n",
      "E loss:  1.5120283365249634\n",
      "G loss: 4.2524237632751465\n",
      "Training Model  ...\n",
      "E loss:  1.5328867435455322\n",
      "G loss: 4.24867057800293\n",
      "E loss:  1.5277588367462158\n",
      "G loss: 4.246819972991943\n",
      "E loss:  1.5470144748687744\n",
      "G loss: 4.236173629760742\n",
      "E loss:  1.5388820171356201\n",
      "G loss: 4.227747917175293\n",
      "E loss:  1.5371677875518799\n",
      "G loss: 4.218522071838379\n",
      "Training Model  ...\n",
      "E loss:  1.5040241479873657\n",
      "G loss: 4.215364933013916\n",
      "E loss:  1.5171175003051758\n",
      "G loss: 4.212400436401367\n",
      "E loss:  1.5416719913482666\n",
      "G loss: 4.2035627365112305\n",
      "E loss:  1.5466320514678955\n",
      "G loss: 4.199182510375977\n",
      "E loss:  1.564329743385315\n",
      "G loss: 4.195501327514648\n",
      "Training Model  ...\n",
      "E loss:  1.790786623954773\n",
      "G loss: 4.2057881355285645\n",
      "E loss:  1.7506074905395508\n",
      "G loss: 4.2357354164123535\n",
      "E loss:  1.7147001028060913\n",
      "G loss: 4.266003131866455\n",
      "E loss:  1.720916986465454\n",
      "G loss: 4.308008193969727\n",
      "E loss:  1.6982347965240479\n",
      "G loss: 4.353976249694824\n",
      "Training Model  ...\n",
      "E loss:  1.696810007095337\n",
      "G loss: 4.362125873565674\n",
      "E loss:  1.7003763914108276\n",
      "G loss: 4.3755998611450195\n",
      "E loss:  1.7021663188934326\n",
      "G loss: 4.391304016113281\n",
      "E loss:  1.7248138189315796\n",
      "G loss: 4.417266845703125\n",
      "E loss:  1.7329448461532593\n",
      "G loss: 4.4426679611206055\n",
      "Training Model  ...\n",
      "E loss:  1.5622882843017578\n",
      "G loss: 4.440952777862549\n",
      "E loss:  1.5574108362197876\n",
      "G loss: 4.433593273162842\n",
      "E loss:  1.6165724992752075\n",
      "G loss: 4.4348530769348145\n",
      "E loss:  1.6347744464874268\n",
      "G loss: 4.420762062072754\n",
      "E loss:  1.62864089012146\n",
      "G loss: 4.414533615112305\n",
      "Training Model  ...\n",
      "E loss:  1.5565273761749268\n",
      "G loss: 4.416145324707031\n",
      "E loss:  1.573702335357666\n",
      "G loss: 4.4096832275390625\n",
      "E loss:  1.6007097959518433\n",
      "G loss: 4.39984655380249\n",
      "E loss:  1.5874747037887573\n",
      "G loss: 4.395529270172119\n",
      "E loss:  1.6106679439544678\n",
      "G loss: 4.383784294128418\n",
      "Training Model  ...\n",
      "E loss:  1.5857791900634766\n",
      "G loss: 4.384952545166016\n",
      "E loss:  1.564527153968811\n",
      "G loss: 4.379761219024658\n",
      "E loss:  1.5567855834960938\n",
      "G loss: 4.380071640014648\n",
      "E loss:  1.5560222864151\n",
      "G loss: 4.382327079772949\n",
      "E loss:  1.5831031799316406\n",
      "G loss: 4.376870632171631\n",
      "Training Model  ...\n",
      "E loss:  1.740655541419983\n",
      "G loss: 4.378211498260498\n",
      "E loss:  1.73830246925354\n",
      "G loss: 4.372651100158691\n",
      "E loss:  1.774709701538086\n",
      "G loss: 4.360085487365723\n",
      "E loss:  1.766904354095459\n",
      "G loss: 4.352263450622559\n",
      "E loss:  1.799446940422058\n",
      "G loss: 4.34079122543335\n",
      "Training Model  ...\n",
      "E loss:  1.7479207515716553\n",
      "G loss: 4.329026699066162\n",
      "E loss:  1.706167459487915\n",
      "G loss: 4.312683582305908\n",
      "E loss:  1.7130589485168457\n",
      "G loss: 4.283839225769043\n",
      "E loss:  1.6826250553131104\n",
      "G loss: 4.257141590118408\n",
      "E loss:  1.6793841123580933\n",
      "G loss: 4.219639778137207\n",
      "Training Model  ...\n",
      "E loss:  1.6022049188613892\n",
      "G loss: 4.222862720489502\n",
      "E loss:  1.6058528423309326\n",
      "G loss: 4.223516464233398\n",
      "E loss:  1.5678040981292725\n",
      "G loss: 4.224715709686279\n",
      "E loss:  1.5459586381912231\n",
      "G loss: 4.234139442443848\n",
      "E loss:  1.5571576356887817\n",
      "G loss: 4.244462490081787\n",
      "Training Model  ...\n",
      "E loss:  1.5698133707046509\n",
      "G loss: 4.248363494873047\n",
      "E loss:  1.5755926370620728\n",
      "G loss: 4.252749443054199\n",
      "E loss:  1.6112256050109863\n",
      "G loss: 4.2635393142700195\n",
      "E loss:  1.6144462823867798\n",
      "G loss: 4.280006408691406\n",
      "E loss:  1.6095014810562134\n",
      "G loss: 4.287540435791016\n",
      "Training Model  ...\n",
      "E loss:  1.5333447456359863\n",
      "G loss: 4.294119834899902\n",
      "E loss:  1.5197426080703735\n",
      "G loss: 4.312962532043457\n",
      "E loss:  1.5250678062438965\n",
      "G loss: 4.328296661376953\n",
      "E loss:  1.5339231491088867\n",
      "G loss: 4.3541178703308105\n",
      "E loss:  1.521904706954956\n",
      "G loss: 4.373204231262207\n",
      "Training Model  ...\n",
      "E loss:  1.576768398284912\n",
      "G loss: 4.363596439361572\n",
      "E loss:  1.5860008001327515\n",
      "G loss: 4.364588737487793\n",
      "E loss:  1.6104816198349\n",
      "G loss: 4.344404697418213\n",
      "E loss:  1.6142059564590454\n",
      "G loss: 4.324689865112305\n",
      "E loss:  1.615179419517517\n",
      "G loss: 4.31162691116333\n",
      "Training Model  ...\n",
      "E loss:  1.6369173526763916\n",
      "G loss: 4.308422088623047\n",
      "E loss:  1.6312918663024902\n",
      "G loss: 4.298858642578125\n",
      "E loss:  1.6083203554153442\n",
      "G loss: 4.289883613586426\n",
      "E loss:  1.638132095336914\n",
      "G loss: 4.27642297744751\n",
      "E loss:  1.667356014251709\n",
      "G loss: 4.2640204429626465\n",
      "Training Model  ...\n",
      "E loss:  1.8225536346435547\n",
      "G loss: 4.27620267868042\n",
      "E loss:  1.8098909854888916\n",
      "G loss: 4.2897515296936035\n",
      "E loss:  1.8121733665466309\n",
      "G loss: 4.312504768371582\n",
      "E loss:  1.8205347061157227\n",
      "G loss: 4.344256401062012\n",
      "E loss:  1.8243218660354614\n",
      "G loss: 4.375535011291504\n",
      "Training Model  ...\n",
      "E loss:  1.5161669254302979\n",
      "G loss: 4.374484062194824\n",
      "E loss:  1.5341198444366455\n",
      "G loss: 4.373103141784668\n",
      "E loss:  1.5191656351089478\n",
      "G loss: 4.363795280456543\n",
      "E loss:  1.523722767829895\n",
      "G loss: 4.35463809967041\n",
      "E loss:  1.5300688743591309\n",
      "G loss: 4.352275848388672\n",
      "Training Model  ...\n",
      "E loss:  1.6525061130523682\n",
      "G loss: 4.347618579864502\n",
      "E loss:  1.6326826810836792\n",
      "G loss: 4.330751895904541\n",
      "E loss:  1.6354444026947021\n",
      "G loss: 4.307027339935303\n",
      "E loss:  1.6199898719787598\n",
      "G loss: 4.282411575317383\n",
      "E loss:  1.6135408878326416\n",
      "G loss: 4.255029201507568\n",
      "Training Model  ...\n",
      "E loss:  1.6586949825286865\n",
      "G loss: 4.256473064422607\n",
      "E loss:  1.6671026945114136\n",
      "G loss: 4.250453472137451\n",
      "E loss:  1.6718028783798218\n",
      "G loss: 4.245227336883545\n",
      "E loss:  1.6584223508834839\n",
      "G loss: 4.243805885314941\n",
      "E loss:  1.6783186197280884\n",
      "G loss: 4.235537052154541\n",
      "Training Model  ...\n",
      "E loss:  1.5709998607635498\n",
      "G loss: 4.225379943847656\n",
      "E loss:  1.556370496749878\n",
      "G loss: 4.208395004272461\n",
      "E loss:  1.5363848209381104\n",
      "G loss: 4.181420803070068\n",
      "E loss:  1.5845292806625366\n",
      "G loss: 4.153454780578613\n",
      "E loss:  1.6018882989883423\n",
      "G loss: 4.127505302429199\n",
      "Training Model  ...\n",
      "E loss:  1.4925174713134766\n",
      "G loss: 4.130810737609863\n",
      "E loss:  1.4839330911636353\n",
      "G loss: 4.140890598297119\n",
      "E loss:  1.510970115661621\n",
      "G loss: 4.153491973876953\n",
      "E loss:  1.5468403100967407\n",
      "G loss: 4.173326015472412\n",
      "E loss:  1.5210309028625488\n",
      "G loss: 4.195949554443359\n",
      "Training Model  ...\n",
      "E loss:  1.5118714570999146\n",
      "G loss: 4.18667459487915\n",
      "E loss:  1.4947593212127686\n",
      "G loss: 4.179766654968262\n",
      "E loss:  1.4686858654022217\n",
      "G loss: 4.173252105712891\n",
      "E loss:  1.4722434282302856\n",
      "G loss: 4.152594089508057\n",
      "E loss:  1.436871886253357\n",
      "G loss: 4.135224342346191\n",
      "Training Model  ...\n",
      "E loss:  1.684957504272461\n",
      "G loss: 4.141701698303223\n",
      "E loss:  1.6541895866394043\n",
      "G loss: 4.153360366821289\n",
      "E loss:  1.656029224395752\n",
      "G loss: 4.174452304840088\n",
      "E loss:  1.653029441833496\n",
      "G loss: 4.1884331703186035\n",
      "E loss:  1.6428204774856567\n",
      "G loss: 4.209765434265137\n",
      "Training Model  ...\n",
      "E loss:  1.5405795574188232\n",
      "G loss: 4.206190586090088\n",
      "E loss:  1.547057867050171\n",
      "G loss: 4.200806140899658\n",
      "E loss:  1.545509696006775\n",
      "G loss: 4.194172382354736\n",
      "E loss:  1.5569860935211182\n",
      "G loss: 4.184219837188721\n",
      "E loss:  1.5629427433013916\n",
      "G loss: 4.1776580810546875\n",
      "Training Model  ...\n",
      "E loss:  1.7580134868621826\n",
      "G loss: 4.181651592254639\n",
      "E loss:  1.744706392288208\n",
      "G loss: 4.191447734832764\n",
      "E loss:  1.7418757677078247\n",
      "G loss: 4.210429668426514\n",
      "E loss:  1.7465099096298218\n",
      "G loss: 4.22506046295166\n",
      "E loss:  1.7292481660842896\n",
      "G loss: 4.241346836090088\n",
      "Training Model  ...\n",
      "E loss:  1.7482000589370728\n",
      "G loss: 4.246026039123535\n",
      "E loss:  1.739159107208252\n",
      "G loss: 4.254835605621338\n",
      "E loss:  1.6751997470855713\n",
      "G loss: 4.262518882751465\n",
      "E loss:  1.6922714710235596\n",
      "G loss: 4.274812698364258\n",
      "E loss:  1.687118411064148\n",
      "G loss: 4.284132957458496\n",
      "Training Model  ...\n",
      "E loss:  1.5203360319137573\n",
      "G loss: 4.286048412322998\n",
      "E loss:  1.5125600099563599\n",
      "G loss: 4.291008472442627\n",
      "E loss:  1.5240800380706787\n",
      "G loss: 4.293558597564697\n",
      "E loss:  1.5725821256637573\n",
      "G loss: 4.307873249053955\n",
      "E loss:  1.5180962085723877\n",
      "G loss: 4.316157817840576\n",
      "Training Model  ...\n",
      "E loss:  1.6776819229125977\n",
      "G loss: 4.313051223754883\n",
      "E loss:  1.6411138772964478\n",
      "G loss: 4.301512718200684\n",
      "E loss:  1.623999834060669\n",
      "G loss: 4.286370277404785\n",
      "E loss:  1.6319881677627563\n",
      "G loss: 4.265405654907227\n",
      "E loss:  1.6034821271896362\n",
      "G loss: 4.238268852233887\n",
      "Training Model  ...\n",
      "E loss:  1.656975269317627\n",
      "G loss: 4.238785743713379\n",
      "E loss:  1.6469001770019531\n",
      "G loss: 4.232553482055664\n",
      "E loss:  1.66402006149292\n",
      "G loss: 4.223911762237549\n",
      "E loss:  1.647402048110962\n",
      "G loss: 4.212010383605957\n",
      "E loss:  1.63632071018219\n",
      "G loss: 4.19640588760376\n",
      "Training Model  ...\n",
      "E loss:  1.7236924171447754\n",
      "G loss: 4.190624237060547\n",
      "E loss:  1.7499409914016724\n",
      "G loss: 4.18219518661499\n",
      "E loss:  1.7420895099639893\n",
      "G loss: 4.176472187042236\n",
      "E loss:  1.7448010444641113\n",
      "G loss: 4.169075965881348\n",
      "E loss:  1.6985690593719482\n",
      "G loss: 4.152547836303711\n",
      "Training Model  ...\n",
      "E loss:  1.6715118885040283\n",
      "G loss: 4.1580915451049805\n",
      "E loss:  1.692009449005127\n",
      "G loss: 4.157798767089844\n",
      "E loss:  1.6945990324020386\n",
      "G loss: 4.163301944732666\n",
      "E loss:  1.69827401638031\n",
      "G loss: 4.173099040985107\n",
      "E loss:  1.6892752647399902\n",
      "G loss: 4.17989444732666\n",
      "Training Model  ...\n",
      "E loss:  1.5778428316116333\n",
      "G loss: 4.1796746253967285\n",
      "E loss:  1.5354185104370117\n",
      "G loss: 4.17739725112915\n",
      "E loss:  1.5081838369369507\n",
      "G loss: 4.178056716918945\n",
      "E loss:  1.4989323616027832\n",
      "G loss: 4.187175273895264\n",
      "E loss:  1.540393590927124\n",
      "G loss: 4.190975189208984\n",
      "Training Model  ...\n",
      "E loss:  1.363566279411316\n",
      "G loss: 4.185114860534668\n",
      "E loss:  1.3956148624420166\n",
      "G loss: 4.170422077178955\n",
      "E loss:  1.3855628967285156\n",
      "G loss: 4.153185844421387\n",
      "E loss:  1.4103937149047852\n",
      "G loss: 4.128724098205566\n",
      "E loss:  1.4170782566070557\n",
      "G loss: 4.102153301239014\n",
      "Training Model  ...\n",
      "E loss:  1.5762956142425537\n",
      "G loss: 4.101867198944092\n",
      "E loss:  1.5530047416687012\n",
      "G loss: 4.098960876464844\n",
      "E loss:  1.5450258255004883\n",
      "G loss: 4.103756904602051\n",
      "E loss:  1.57966148853302\n",
      "G loss: 4.109200477600098\n",
      "E loss:  1.5558549165725708\n",
      "G loss: 4.112740516662598\n",
      "Training Model  ...\n",
      "E loss:  1.6684091091156006\n",
      "G loss: 4.117124080657959\n",
      "E loss:  1.665429711341858\n",
      "G loss: 4.124312877655029\n",
      "E loss:  1.6774190664291382\n",
      "G loss: 4.143754959106445\n",
      "E loss:  1.6900891065597534\n",
      "G loss: 4.159878253936768\n",
      "E loss:  1.6740576028823853\n",
      "G loss: 4.180408000946045\n",
      "Training Model  ...\n",
      "E loss:  1.6456162929534912\n",
      "G loss: 4.169612407684326\n",
      "E loss:  1.6451634168624878\n",
      "G loss: 4.158003807067871\n",
      "E loss:  1.612635612487793\n",
      "G loss: 4.140902042388916\n",
      "E loss:  1.6446139812469482\n",
      "G loss: 4.127142429351807\n",
      "E loss:  1.6262301206588745\n",
      "G loss: 4.106228828430176\n",
      "Training Model  ...\n",
      "E loss:  1.6259701251983643\n",
      "G loss: 4.10828971862793\n",
      "E loss:  1.6295948028564453\n",
      "G loss: 4.121521472930908\n",
      "E loss:  1.629198670387268\n",
      "G loss: 4.140177249908447\n",
      "E loss:  1.6369107961654663\n",
      "G loss: 4.163442134857178\n",
      "E loss:  1.660538911819458\n",
      "G loss: 4.185066223144531\n",
      "Training Model  ...\n",
      "E loss:  1.6044583320617676\n",
      "G loss: 4.1850409507751465\n",
      "E loss:  1.5908355712890625\n",
      "G loss: 4.202999591827393\n",
      "E loss:  1.5799235105514526\n",
      "G loss: 4.222379684448242\n",
      "E loss:  1.5307544469833374\n",
      "G loss: 4.239274024963379\n",
      "E loss:  1.4942151308059692\n",
      "G loss: 4.263998031616211\n",
      "Training Model  ...\n",
      "E loss:  1.7186928987503052\n",
      "G loss: 4.265287399291992\n",
      "E loss:  1.68373703956604\n",
      "G loss: 4.271146774291992\n",
      "E loss:  1.686056137084961\n",
      "G loss: 4.276554584503174\n",
      "E loss:  1.7367091178894043\n",
      "G loss: 4.286606788635254\n",
      "E loss:  1.7282726764678955\n",
      "G loss: 4.296904563903809\n",
      "Training Model  ...\n",
      "E loss:  1.6222279071807861\n",
      "G loss: 4.2910237312316895\n",
      "E loss:  1.5821722745895386\n",
      "G loss: 4.287984371185303\n",
      "E loss:  1.5852363109588623\n",
      "G loss: 4.281170845031738\n",
      "E loss:  1.5528427362442017\n",
      "G loss: 4.270298004150391\n",
      "E loss:  1.5628259181976318\n",
      "G loss: 4.2654242515563965\n",
      "Training Model  ...\n",
      "E loss:  1.5294694900512695\n",
      "G loss: 4.254554271697998\n",
      "E loss:  1.5278122425079346\n",
      "G loss: 4.235716819763184\n",
      "E loss:  1.5672094821929932\n",
      "G loss: 4.213163375854492\n",
      "E loss:  1.5899354219436646\n",
      "G loss: 4.18111515045166\n",
      "E loss:  1.572451114654541\n",
      "G loss: 4.14613151550293\n",
      "Training Model  ...\n",
      "E loss:  1.5897114276885986\n",
      "G loss: 4.148999214172363\n",
      "E loss:  1.5744502544403076\n",
      "G loss: 4.168868064880371\n",
      "E loss:  1.5873324871063232\n",
      "G loss: 4.1899495124816895\n",
      "E loss:  1.6193965673446655\n",
      "G loss: 4.211384296417236\n",
      "E loss:  1.662433385848999\n",
      "G loss: 4.240884304046631\n",
      "Training Model  ...\n",
      "E loss:  1.6762104034423828\n",
      "G loss: 4.251314640045166\n",
      "E loss:  1.6828926801681519\n",
      "G loss: 4.267969608306885\n",
      "E loss:  1.6660603284835815\n",
      "G loss: 4.282496452331543\n",
      "E loss:  1.6610862016677856\n",
      "G loss: 4.307860851287842\n",
      "E loss:  1.6472028493881226\n",
      "G loss: 4.332062721252441\n",
      "Training Model  ...\n",
      "E loss:  1.516205906867981\n",
      "G loss: 4.320937156677246\n",
      "E loss:  1.5162125825881958\n",
      "G loss: 4.313814640045166\n",
      "E loss:  1.5075966119766235\n",
      "G loss: 4.293919086456299\n",
      "E loss:  1.522713541984558\n",
      "G loss: 4.272176265716553\n",
      "E loss:  1.508966088294983\n",
      "G loss: 4.245027542114258\n",
      "Training Model  ...\n",
      "E loss:  1.659360408782959\n",
      "G loss: 4.243820667266846\n",
      "E loss:  1.6532840728759766\n",
      "G loss: 4.242721080780029\n",
      "E loss:  1.6883593797683716\n",
      "G loss: 4.236076831817627\n",
      "E loss:  1.6883317232131958\n",
      "G loss: 4.225278377532959\n",
      "E loss:  1.6729588508605957\n",
      "G loss: 4.217083930969238\n",
      "Training Model  ...\n",
      "E loss:  1.4595253467559814\n",
      "G loss: 4.215673446655273\n",
      "E loss:  1.4617695808410645\n",
      "G loss: 4.216724872589111\n",
      "E loss:  1.45731520652771\n",
      "G loss: 4.219329833984375\n",
      "E loss:  1.4259428977966309\n",
      "G loss: 4.226414680480957\n",
      "E loss:  1.4226065874099731\n",
      "G loss: 4.235910415649414\n",
      "Training Model  ...\n",
      "E loss:  1.6820966005325317\n",
      "G loss: 4.230600357055664\n",
      "E loss:  1.6816633939743042\n",
      "G loss: 4.232260704040527\n",
      "E loss:  1.6904618740081787\n",
      "G loss: 4.229036808013916\n",
      "E loss:  1.699067234992981\n",
      "G loss: 4.2297186851501465\n",
      "E loss:  1.6828099489212036\n",
      "G loss: 4.226269721984863\n",
      "Training Model  ...\n",
      "E loss:  1.53139328956604\n",
      "G loss: 4.238819122314453\n",
      "E loss:  1.5551602840423584\n",
      "G loss: 4.253152370452881\n",
      "E loss:  1.5561503171920776\n",
      "G loss: 4.269958972930908\n",
      "E loss:  1.5389920473098755\n",
      "G loss: 4.298428535461426\n",
      "E loss:  1.5769145488739014\n",
      "G loss: 4.319800853729248\n",
      "Training Model  ...\n",
      "E loss:  1.607014775276184\n",
      "G loss: 4.321871280670166\n",
      "E loss:  1.5994874238967896\n",
      "G loss: 4.318014144897461\n",
      "E loss:  1.6412205696105957\n",
      "G loss: 4.320992946624756\n",
      "E loss:  1.6756078004837036\n",
      "G loss: 4.32246732711792\n",
      "E loss:  1.6790691614151\n",
      "G loss: 4.332569122314453\n",
      "Training Model  ...\n",
      "E loss:  1.713813066482544\n",
      "G loss: 4.329061508178711\n",
      "E loss:  1.727534532546997\n",
      "G loss: 4.324263095855713\n",
      "E loss:  1.7049429416656494\n",
      "G loss: 4.318629741668701\n",
      "E loss:  1.6929041147232056\n",
      "G loss: 4.305722236633301\n",
      "E loss:  1.6713143587112427\n",
      "G loss: 4.294426918029785\n",
      "Training Model  ...\n",
      "E loss:  1.7109123468399048\n",
      "G loss: 4.292482852935791\n",
      "E loss:  1.7056056261062622\n",
      "G loss: 4.284424304962158\n",
      "E loss:  1.6759570837020874\n",
      "G loss: 4.266687870025635\n",
      "E loss:  1.6691691875457764\n",
      "G loss: 4.262716770172119\n",
      "E loss:  1.6717476844787598\n",
      "G loss: 4.24751615524292\n",
      "Training Model  ...\n",
      "E loss:  1.6133664846420288\n",
      "G loss: 4.242783546447754\n",
      "E loss:  1.6122188568115234\n",
      "G loss: 4.25304651260376\n",
      "E loss:  1.5657868385314941\n",
      "G loss: 4.253597259521484\n",
      "E loss:  1.575968861579895\n",
      "G loss: 4.252244472503662\n",
      "E loss:  1.5776374340057373\n",
      "G loss: 4.24628210067749\n",
      "Training Model  ...\n",
      "E loss:  1.776587724685669\n",
      "G loss: 4.243238925933838\n",
      "E loss:  1.7325804233551025\n",
      "G loss: 4.244032382965088\n",
      "E loss:  1.6994162797927856\n",
      "G loss: 4.239979267120361\n",
      "E loss:  1.748526930809021\n",
      "G loss: 4.2412896156311035\n",
      "E loss:  1.746110200881958\n",
      "G loss: 4.233338356018066\n",
      "Training Model  ...\n",
      "E loss:  1.610465407371521\n",
      "G loss: 4.2268524169921875\n",
      "E loss:  1.6562005281448364\n",
      "G loss: 4.2034101486206055\n",
      "E loss:  1.6088608503341675\n",
      "G loss: 4.176819801330566\n",
      "E loss:  1.6707359552383423\n",
      "G loss: 4.141594886779785\n",
      "E loss:  1.6556177139282227\n",
      "G loss: 4.108946800231934\n",
      "Training Model  ...\n",
      "E loss:  1.6529029607772827\n",
      "G loss: 4.109286308288574\n",
      "E loss:  1.653780460357666\n",
      "G loss: 4.104039192199707\n",
      "E loss:  1.6173200607299805\n",
      "G loss: 4.111617088317871\n",
      "E loss:  1.5851869583129883\n",
      "G loss: 4.108462333679199\n",
      "E loss:  1.5768593549728394\n",
      "G loss: 4.104549407958984\n",
      "Training Model  ...\n",
      "E loss:  1.5425567626953125\n",
      "G loss: 4.116508960723877\n",
      "E loss:  1.5659704208374023\n",
      "G loss: 4.126996994018555\n",
      "E loss:  1.5614863634109497\n",
      "G loss: 4.136170864105225\n",
      "E loss:  1.5495879650115967\n",
      "G loss: 4.154616355895996\n",
      "E loss:  1.5442452430725098\n",
      "G loss: 4.178004741668701\n",
      "Training Model  ...\n",
      "E loss:  1.462639570236206\n",
      "G loss: 4.174333095550537\n",
      "E loss:  1.4517254829406738\n",
      "G loss: 4.162771224975586\n",
      "E loss:  1.4468965530395508\n",
      "G loss: 4.146851539611816\n",
      "E loss:  1.4483683109283447\n",
      "G loss: 4.131213188171387\n",
      "E loss:  1.4493505954742432\n",
      "G loss: 4.109333515167236\n",
      "Training Model  ...\n",
      "E loss:  1.6304271221160889\n",
      "G loss: 4.106883525848389\n",
      "E loss:  1.6544188261032104\n",
      "G loss: 4.1059393882751465\n",
      "E loss:  1.6596601009368896\n",
      "G loss: 4.1069159507751465\n",
      "E loss:  1.6770044565200806\n",
      "G loss: 4.096650123596191\n",
      "E loss:  1.730168342590332\n",
      "G loss: 4.094455718994141\n",
      "Training Model  ...\n",
      "E loss:  1.67153000831604\n",
      "G loss: 4.097980976104736\n",
      "E loss:  1.648808479309082\n",
      "G loss: 4.105234622955322\n",
      "E loss:  1.6611372232437134\n",
      "G loss: 4.116678237915039\n",
      "E loss:  1.6436516046524048\n",
      "G loss: 4.124094486236572\n",
      "E loss:  1.6322171688079834\n",
      "G loss: 4.138480186462402\n",
      "Training Model  ...\n",
      "E loss:  1.479933500289917\n",
      "G loss: 4.131183624267578\n",
      "E loss:  1.5031179189682007\n",
      "G loss: 4.119660377502441\n",
      "E loss:  1.5099912881851196\n",
      "G loss: 4.1046881675720215\n",
      "E loss:  1.4844334125518799\n",
      "G loss: 4.087174415588379\n",
      "E loss:  1.483965516090393\n",
      "G loss: 4.0640459060668945\n",
      "Training Model  ...\n",
      "E loss:  1.651252031326294\n",
      "G loss: 4.055446147918701\n",
      "E loss:  1.623571753501892\n",
      "G loss: 4.036714553833008\n",
      "E loss:  1.6111260652542114\n",
      "G loss: 4.014575958251953\n",
      "E loss:  1.6077854633331299\n",
      "G loss: 3.9840002059936523\n",
      "E loss:  1.5947885513305664\n",
      "G loss: 3.9488632678985596\n",
      "Training Model  ...\n",
      "E loss:  1.6936283111572266\n",
      "G loss: 3.9500999450683594\n",
      "E loss:  1.6468861103057861\n",
      "G loss: 3.954103946685791\n",
      "E loss:  1.6543875932693481\n",
      "G loss: 3.9596798419952393\n",
      "E loss:  1.648756980895996\n",
      "G loss: 3.960554599761963\n",
      "E loss:  1.6626046895980835\n",
      "G loss: 3.9649410247802734\n",
      "Training Model  ...\n",
      "E loss:  1.5571779012680054\n",
      "G loss: 3.976219654083252\n",
      "E loss:  1.5451691150665283\n",
      "G loss: 3.9903736114501953\n",
      "E loss:  1.5448378324508667\n",
      "G loss: 4.007602691650391\n",
      "E loss:  1.5207884311676025\n",
      "G loss: 4.028278827667236\n",
      "E loss:  1.529820203781128\n",
      "G loss: 4.049350261688232\n",
      "Training Model  ...\n",
      "E loss:  1.6488984823226929\n",
      "G loss: 4.048125743865967\n",
      "E loss:  1.6334246397018433\n",
      "G loss: 4.0542168617248535\n",
      "E loss:  1.6819977760314941\n",
      "G loss: 4.05422830581665\n",
      "E loss:  1.6564381122589111\n",
      "G loss: 4.056765556335449\n",
      "E loss:  1.640974998474121\n",
      "G loss: 4.063539981842041\n",
      "Training Model  ...\n",
      "E loss:  1.6293421983718872\n",
      "G loss: 4.057141304016113\n",
      "E loss:  1.5981032848358154\n",
      "G loss: 4.054423809051514\n",
      "E loss:  1.5963760614395142\n",
      "G loss: 4.045562744140625\n",
      "E loss:  1.5688371658325195\n",
      "G loss: 4.034084796905518\n",
      "E loss:  1.5415925979614258\n",
      "G loss: 4.023150444030762\n",
      "Training Model  ...\n",
      "E loss:  1.449185848236084\n",
      "G loss: 4.028963565826416\n",
      "E loss:  1.4879056215286255\n",
      "G loss: 4.039831161499023\n",
      "E loss:  1.5129166841506958\n",
      "G loss: 4.046558380126953\n",
      "E loss:  1.4970967769622803\n",
      "G loss: 4.070032596588135\n",
      "E loss:  1.5253934860229492\n",
      "G loss: 4.0876970291137695\n",
      "Training Model  ...\n",
      "E loss:  1.7636667490005493\n",
      "G loss: 4.099007606506348\n",
      "E loss:  1.7513614892959595\n",
      "G loss: 4.1165337562561035\n",
      "E loss:  1.7348328828811646\n",
      "G loss: 4.150029182434082\n",
      "E loss:  1.7273452281951904\n",
      "G loss: 4.185634136199951\n",
      "E loss:  1.70915949344635\n",
      "G loss: 4.2273173332214355\n",
      "Training Model  ...\n",
      "E loss:  1.7800745964050293\n",
      "G loss: 4.229203701019287\n",
      "E loss:  1.7540074586868286\n",
      "G loss: 4.233138561248779\n",
      "E loss:  1.7093641757965088\n",
      "G loss: 4.241384983062744\n",
      "E loss:  1.7288258075714111\n",
      "G loss: 4.253694534301758\n",
      "E loss:  1.7286051511764526\n",
      "G loss: 4.259906768798828\n",
      "Training Model  ...\n",
      "E loss:  1.7326675653457642\n",
      "G loss: 4.262396335601807\n",
      "E loss:  1.6856820583343506\n",
      "G loss: 4.26639986038208\n",
      "E loss:  1.6717361211776733\n",
      "G loss: 4.273982524871826\n",
      "E loss:  1.7060065269470215\n",
      "G loss: 4.273743152618408\n",
      "E loss:  1.6972683668136597\n",
      "G loss: 4.282654285430908\n",
      "Training Model  ...\n",
      "E loss:  1.611378788948059\n",
      "G loss: 4.267039775848389\n",
      "E loss:  1.6077349185943604\n",
      "G loss: 4.234711647033691\n",
      "E loss:  1.6038486957550049\n",
      "G loss: 4.196052551269531\n",
      "E loss:  1.604138731956482\n",
      "G loss: 4.143476963043213\n",
      "E loss:  1.608565330505371\n",
      "G loss: 4.087774753570557\n",
      "Training Model  ...\n",
      "E loss:  1.7306965589523315\n",
      "G loss: 4.0946760177612305\n",
      "E loss:  1.7233737707138062\n",
      "G loss: 4.099462032318115\n",
      "E loss:  1.7312660217285156\n",
      "G loss: 4.108384609222412\n",
      "E loss:  1.7159862518310547\n",
      "G loss: 4.115025043487549\n",
      "E loss:  1.7112784385681152\n",
      "G loss: 4.126610279083252\n",
      "Training Model  ...\n",
      "E loss:  1.467634677886963\n",
      "G loss: 4.1259236335754395\n",
      "E loss:  1.4772019386291504\n",
      "G loss: 4.12648868560791\n",
      "E loss:  1.4873791933059692\n",
      "G loss: 4.127828121185303\n",
      "E loss:  1.459692120552063\n",
      "G loss: 4.124818325042725\n",
      "E loss:  1.447174310684204\n",
      "G loss: 4.124149322509766\n",
      "Training Model  ...\n",
      "E loss:  1.6569923162460327\n",
      "G loss: 4.1227593421936035\n",
      "E loss:  1.6778241395950317\n",
      "G loss: 4.11135721206665\n",
      "E loss:  1.6896703243255615\n",
      "G loss: 4.094538688659668\n",
      "E loss:  1.663208246231079\n",
      "G loss: 4.082749366760254\n",
      "E loss:  1.688357949256897\n",
      "G loss: 4.063628196716309\n",
      "Training Model  ...\n",
      "E loss:  1.7324782609939575\n",
      "G loss: 4.07385778427124\n",
      "E loss:  1.7379992008209229\n",
      "G loss: 4.077516555786133\n",
      "E loss:  1.7372455596923828\n",
      "G loss: 4.093776702880859\n",
      "E loss:  1.7314598560333252\n",
      "G loss: 4.1127119064331055\n",
      "E loss:  1.736464500427246\n",
      "G loss: 4.134613513946533\n",
      "Training Model  ...\n",
      "E loss:  1.5140588283538818\n",
      "G loss: 4.128316879272461\n",
      "E loss:  1.5178196430206299\n",
      "G loss: 4.120920658111572\n",
      "E loss:  1.5226434469223022\n",
      "G loss: 4.113240718841553\n",
      "E loss:  1.539231538772583\n",
      "G loss: 4.106671333312988\n",
      "E loss:  1.5467591285705566\n",
      "G loss: 4.096098899841309\n",
      "Training Model  ...\n",
      "E loss:  1.491760015487671\n",
      "G loss: 4.088906288146973\n",
      "E loss:  1.5426188707351685\n",
      "G loss: 4.07768440246582\n",
      "E loss:  1.5373085737228394\n",
      "G loss: 4.063628673553467\n",
      "E loss:  1.5486377477645874\n",
      "G loss: 4.0491766929626465\n",
      "E loss:  1.5289180278778076\n",
      "G loss: 4.023918151855469\n",
      "Training Model  ...\n",
      "E loss:  1.4519741535186768\n",
      "G loss: 4.040653228759766\n",
      "E loss:  1.4337949752807617\n",
      "G loss: 4.0587310791015625\n",
      "E loss:  1.4538071155548096\n",
      "G loss: 4.080097198486328\n",
      "E loss:  1.4413262605667114\n",
      "G loss: 4.124699592590332\n",
      "E loss:  1.4422980546951294\n",
      "G loss: 4.158702373504639\n",
      "Training Model  ...\n",
      "E loss:  1.5784595012664795\n",
      "G loss: 4.1624650955200195\n",
      "E loss:  1.5803474187850952\n",
      "G loss: 4.168019771575928\n",
      "E loss:  1.5843082666397095\n",
      "G loss: 4.179949760437012\n",
      "E loss:  1.5846275091171265\n",
      "G loss: 4.1889262199401855\n",
      "E loss:  1.5538060665130615\n",
      "G loss: 4.201822757720947\n",
      "Training Model  ...\n",
      "E loss:  1.6554702520370483\n",
      "G loss: 4.19784688949585\n",
      "E loss:  1.6407133340835571\n",
      "G loss: 4.1768951416015625\n",
      "E loss:  1.6384210586547852\n",
      "G loss: 4.158076286315918\n",
      "E loss:  1.6446528434753418\n",
      "G loss: 4.1330366134643555\n",
      "E loss:  1.6263065338134766\n",
      "G loss: 4.103443145751953\n",
      "Training Model  ...\n",
      "E loss:  1.572022557258606\n",
      "G loss: 4.1048502922058105\n",
      "E loss:  1.5526258945465088\n",
      "G loss: 4.116241455078125\n",
      "E loss:  1.5565828084945679\n",
      "G loss: 4.128986835479736\n",
      "E loss:  1.567628264427185\n",
      "G loss: 4.143849849700928\n",
      "E loss:  1.5630238056182861\n",
      "G loss: 4.152045249938965\n",
      "Training Model  ...\n",
      "E loss:  1.7588341236114502\n",
      "G loss: 4.144150733947754\n",
      "E loss:  1.7507656812667847\n",
      "G loss: 4.120790004730225\n",
      "E loss:  1.6993739604949951\n",
      "G loss: 4.106559753417969\n",
      "E loss:  1.7469851970672607\n",
      "G loss: 4.079680919647217\n",
      "E loss:  1.7753665447235107\n",
      "G loss: 4.06246280670166\n",
      "Training Model  ...\n",
      "E loss:  1.7704181671142578\n",
      "G loss: 4.0683817863464355\n",
      "E loss:  1.7663812637329102\n",
      "G loss: 4.08281135559082\n",
      "E loss:  1.7728805541992188\n",
      "G loss: 4.106899261474609\n",
      "E loss:  1.7780181169509888\n",
      "G loss: 4.135341167449951\n",
      "E loss:  1.785179615020752\n",
      "G loss: 4.167672634124756\n",
      "Training Model  ...\n",
      "E loss:  1.6597099304199219\n",
      "G loss: 4.155682563781738\n",
      "E loss:  1.696868658065796\n",
      "G loss: 4.142822742462158\n",
      "E loss:  1.675668716430664\n",
      "G loss: 4.110257625579834\n",
      "E loss:  1.6704715490341187\n",
      "G loss: 4.089937210083008\n",
      "E loss:  1.6382551193237305\n",
      "G loss: 4.057108402252197\n",
      "Training Model  ...\n",
      "E loss:  1.519672155380249\n",
      "G loss: 4.052608966827393\n",
      "E loss:  1.5522056818008423\n",
      "G loss: 4.049927711486816\n",
      "E loss:  1.5558747053146362\n",
      "G loss: 4.0528564453125\n",
      "E loss:  1.5639339685440063\n",
      "G loss: 4.039706707000732\n",
      "E loss:  1.5771138668060303\n",
      "G loss: 4.034875869750977\n",
      "Training Model  ...\n",
      "E loss:  1.5657994747161865\n",
      "G loss: 4.043424129486084\n",
      "E loss:  1.536318063735962\n",
      "G loss: 4.05748987197876\n",
      "E loss:  1.573307752609253\n",
      "G loss: 4.073443412780762\n",
      "E loss:  1.5376489162445068\n",
      "G loss: 4.099508285522461\n",
      "E loss:  1.5596158504486084\n",
      "G loss: 4.120977401733398\n",
      "Training Model  ...\n",
      "E loss:  1.5323095321655273\n",
      "G loss: 4.1271514892578125\n",
      "E loss:  1.5209802389144897\n",
      "G loss: 4.137093544006348\n",
      "E loss:  1.548391342163086\n",
      "G loss: 4.145226001739502\n",
      "E loss:  1.5357391834259033\n",
      "G loss: 4.166508197784424\n",
      "E loss:  1.5616393089294434\n",
      "G loss: 4.190118789672852\n",
      "Training Model  ...\n",
      "E loss:  1.6304872035980225\n",
      "G loss: 4.19313907623291\n",
      "E loss:  1.6532347202301025\n",
      "G loss: 4.193281173706055\n",
      "E loss:  1.6341952085494995\n",
      "G loss: 4.205760955810547\n",
      "E loss:  1.629380464553833\n",
      "G loss: 4.206310749053955\n",
      "E loss:  1.6030675172805786\n",
      "G loss: 4.207350730895996\n",
      "Training Model  ...\n",
      "E loss:  1.6514739990234375\n",
      "G loss: 4.216019630432129\n",
      "E loss:  1.6495630741119385\n",
      "G loss: 4.214540481567383\n",
      "E loss:  1.6624629497528076\n",
      "G loss: 4.220171928405762\n",
      "E loss:  1.6609220504760742\n",
      "G loss: 4.2323431968688965\n",
      "E loss:  1.6604647636413574\n",
      "G loss: 4.239340782165527\n",
      "Training Model  ...\n",
      "E loss:  1.5151417255401611\n",
      "G loss: 4.242166519165039\n",
      "E loss:  1.486510992050171\n",
      "G loss: 4.233969211578369\n",
      "E loss:  1.467440128326416\n",
      "G loss: 4.22230339050293\n",
      "E loss:  1.4617995023727417\n",
      "G loss: 4.207520484924316\n",
      "E loss:  1.4953019618988037\n",
      "G loss: 4.193079948425293\n",
      "Training Model  ...\n",
      "E loss:  1.7127859592437744\n",
      "G loss: 4.1862688064575195\n",
      "E loss:  1.7094480991363525\n",
      "G loss: 4.171997547149658\n",
      "E loss:  1.7044243812561035\n",
      "G loss: 4.14983606338501\n",
      "E loss:  1.714692234992981\n",
      "G loss: 4.126513957977295\n",
      "E loss:  1.7041434049606323\n",
      "G loss: 4.10919189453125\n",
      "Training Model  ...\n",
      "E loss:  1.6993364095687866\n",
      "G loss: 4.104092597961426\n",
      "E loss:  1.6967099905014038\n",
      "G loss: 4.096883296966553\n",
      "E loss:  1.7031134366989136\n",
      "G loss: 4.087713241577148\n",
      "E loss:  1.7337101697921753\n",
      "G loss: 4.075369834899902\n",
      "E loss:  1.737345814704895\n",
      "G loss: 4.061863422393799\n",
      "Training Model  ...\n",
      "E loss:  1.7688225507736206\n",
      "G loss: 4.057824611663818\n",
      "E loss:  1.743049144744873\n",
      "G loss: 4.038856506347656\n",
      "E loss:  1.7519640922546387\n",
      "G loss: 4.019413471221924\n",
      "E loss:  1.760617971420288\n",
      "G loss: 3.9798192977905273\n",
      "E loss:  1.7688125371932983\n",
      "G loss: 3.9531149864196777\n",
      "Training Model  ...\n",
      "E loss:  1.621991515159607\n",
      "G loss: 3.954270601272583\n",
      "E loss:  1.623795986175537\n",
      "G loss: 3.9589595794677734\n",
      "E loss:  1.6764724254608154\n",
      "G loss: 3.975140333175659\n",
      "E loss:  1.6956911087036133\n",
      "G loss: 3.987031936645508\n",
      "E loss:  1.6699875593185425\n",
      "G loss: 3.9953598976135254\n",
      "Training Model  ...\n",
      "E loss:  1.4587581157684326\n",
      "G loss: 4.0006914138793945\n",
      "E loss:  1.4528100490570068\n",
      "G loss: 4.016688346862793\n",
      "E loss:  1.454703450202942\n",
      "G loss: 4.026848316192627\n",
      "E loss:  1.5053367614746094\n",
      "G loss: 4.0435285568237305\n",
      "E loss:  1.4794269800186157\n",
      "G loss: 4.058008193969727\n",
      "Training Model  ...\n",
      "E loss:  1.5645750761032104\n",
      "G loss: 4.068312644958496\n",
      "E loss:  1.6030319929122925\n",
      "G loss: 4.084563255310059\n",
      "E loss:  1.589186429977417\n",
      "G loss: 4.103387355804443\n",
      "E loss:  1.597924828529358\n",
      "G loss: 4.128222465515137\n",
      "E loss:  1.5996594429016113\n",
      "G loss: 4.167548656463623\n",
      "Training Model  ...\n",
      "E loss:  1.4802651405334473\n",
      "G loss: 4.1522932052612305\n",
      "E loss:  1.4872134923934937\n",
      "G loss: 4.137225151062012\n",
      "E loss:  1.4947924613952637\n",
      "G loss: 4.1121954917907715\n",
      "E loss:  1.5209062099456787\n",
      "G loss: 4.082467079162598\n",
      "E loss:  1.5346832275390625\n",
      "G loss: 4.052638530731201\n",
      "Training Model  ...\n",
      "E loss:  1.7136592864990234\n",
      "G loss: 4.054316520690918\n",
      "E loss:  1.704049825668335\n",
      "G loss: 4.0588765144348145\n",
      "E loss:  1.7176240682601929\n",
      "G loss: 4.057973384857178\n",
      "E loss:  1.7095036506652832\n",
      "G loss: 4.062865257263184\n",
      "E loss:  1.7035869359970093\n",
      "G loss: 4.072450637817383\n",
      "Training Model  ...\n",
      "E loss:  1.6282387971878052\n",
      "G loss: 4.068859100341797\n",
      "E loss:  1.6597366333007812\n",
      "G loss: 4.063068389892578\n",
      "E loss:  1.7022056579589844\n",
      "G loss: 4.055403232574463\n",
      "E loss:  1.6773064136505127\n",
      "G loss: 4.035425662994385\n",
      "E loss:  1.701979160308838\n",
      "G loss: 4.024312973022461\n",
      "Training Model  ...\n",
      "E loss:  1.5582005977630615\n",
      "G loss: 4.023508548736572\n",
      "E loss:  1.5505205392837524\n",
      "G loss: 4.030204772949219\n",
      "E loss:  1.527153491973877\n",
      "G loss: 4.032313346862793\n",
      "E loss:  1.5095012187957764\n",
      "G loss: 4.037261009216309\n",
      "E loss:  1.5252357721328735\n",
      "G loss: 4.046141147613525\n",
      "Training Model  ...\n",
      "E loss:  1.5481778383255005\n",
      "G loss: 4.040353775024414\n",
      "E loss:  1.5169602632522583\n",
      "G loss: 4.025918483734131\n",
      "E loss:  1.5304615497589111\n",
      "G loss: 4.02447509765625\n",
      "E loss:  1.537826657295227\n",
      "G loss: 4.010316371917725\n",
      "E loss:  1.5511999130249023\n",
      "G loss: 3.992997407913208\n",
      "Training Model  ...\n",
      "E loss:  1.645235300064087\n",
      "G loss: 3.9983670711517334\n",
      "E loss:  1.6237905025482178\n",
      "G loss: 3.990922451019287\n",
      "E loss:  1.6255481243133545\n",
      "G loss: 3.99715518951416\n",
      "E loss:  1.6370854377746582\n",
      "G loss: 3.991326332092285\n",
      "E loss:  1.6546566486358643\n",
      "G loss: 3.9860787391662598\n",
      "Training Model  ...\n",
      "E loss:  1.6071113348007202\n",
      "G loss: 3.991450071334839\n",
      "E loss:  1.6127305030822754\n",
      "G loss: 3.9902472496032715\n",
      "E loss:  1.5503736734390259\n",
      "G loss: 3.987988233566284\n",
      "E loss:  1.5503021478652954\n",
      "G loss: 3.987546443939209\n",
      "E loss:  1.5564806461334229\n",
      "G loss: 3.9888129234313965\n",
      "Training Model  ...\n",
      "E loss:  1.6255983114242554\n",
      "G loss: 3.9833407402038574\n",
      "E loss:  1.621442437171936\n",
      "G loss: 3.9891109466552734\n",
      "E loss:  1.5998722314834595\n",
      "G loss: 3.9949567317962646\n",
      "E loss:  1.627152442932129\n",
      "G loss: 3.9866106510162354\n",
      "E loss:  1.6738760471343994\n",
      "G loss: 3.996124744415283\n",
      "Training Model  ...\n",
      "E loss:  1.5935158729553223\n",
      "G loss: 3.9889347553253174\n",
      "E loss:  1.630033254623413\n",
      "G loss: 3.981433629989624\n",
      "E loss:  1.6391443014144897\n",
      "G loss: 3.961470365524292\n",
      "E loss:  1.6255264282226562\n",
      "G loss: 3.944014549255371\n",
      "E loss:  1.5981556177139282\n",
      "G loss: 3.919170379638672\n",
      "Training Model  ...\n",
      "E loss:  1.6048834323883057\n",
      "G loss: 3.9271740913391113\n",
      "E loss:  1.548202395439148\n",
      "G loss: 3.941526412963867\n",
      "E loss:  1.5201419591903687\n",
      "G loss: 3.9523918628692627\n",
      "E loss:  1.5240793228149414\n",
      "G loss: 3.9741249084472656\n",
      "E loss:  1.5275177955627441\n",
      "G loss: 3.9880664348602295\n",
      "Training Model  ...\n",
      "E loss:  1.5457842350006104\n",
      "G loss: 3.9851455688476562\n",
      "E loss:  1.540010929107666\n",
      "G loss: 3.9931352138519287\n",
      "E loss:  1.5186610221862793\n",
      "G loss: 3.998016834259033\n",
      "E loss:  1.507030963897705\n",
      "G loss: 4.008233547210693\n",
      "E loss:  1.506986141204834\n",
      "G loss: 4.021912574768066\n",
      "Training Model  ...\n",
      "E loss:  1.5575408935546875\n",
      "G loss: 4.0180253982543945\n",
      "E loss:  1.5380983352661133\n",
      "G loss: 4.025177955627441\n",
      "E loss:  1.5184357166290283\n",
      "G loss: 4.029369831085205\n",
      "E loss:  1.5169246196746826\n",
      "G loss: 4.0279035568237305\n",
      "E loss:  1.5210825204849243\n",
      "G loss: 4.031681060791016\n",
      "Training Model  ...\n",
      "E loss:  1.4792073965072632\n",
      "G loss: 4.030228137969971\n",
      "E loss:  1.4425768852233887\n",
      "G loss: 4.037402153015137\n",
      "E loss:  1.4305756092071533\n",
      "G loss: 4.044269561767578\n",
      "E loss:  1.4516807794570923\n",
      "G loss: 4.047785758972168\n",
      "E loss:  1.4459209442138672\n",
      "G loss: 4.046370506286621\n",
      "Training Model  ...\n",
      "E loss:  1.6725566387176514\n",
      "G loss: 4.068057537078857\n",
      "E loss:  1.6698850393295288\n",
      "G loss: 4.088679790496826\n",
      "E loss:  1.6341267824172974\n",
      "G loss: 4.116759300231934\n",
      "E loss:  1.644986867904663\n",
      "G loss: 4.146978855133057\n",
      "E loss:  1.6685311794281006\n",
      "G loss: 4.184378147125244\n",
      "Training Model  ...\n",
      "E loss:  1.5283734798431396\n",
      "G loss: 4.178999423980713\n",
      "E loss:  1.571664571762085\n",
      "G loss: 4.1650614738464355\n",
      "E loss:  1.5822287797927856\n",
      "G loss: 4.142685890197754\n",
      "E loss:  1.5829952955245972\n",
      "G loss: 4.118056297302246\n",
      "E loss:  1.592503309249878\n",
      "G loss: 4.092991828918457\n",
      "Training Model  ...\n",
      "E loss:  1.6420931816101074\n",
      "G loss: 4.092432498931885\n",
      "E loss:  1.580506682395935\n",
      "G loss: 4.093576431274414\n",
      "E loss:  1.5629721879959106\n",
      "G loss: 4.092147350311279\n",
      "E loss:  1.544897198677063\n",
      "G loss: 4.104700565338135\n",
      "E loss:  1.5462067127227783\n",
      "G loss: 4.104132175445557\n",
      "Training Model  ...\n",
      "E loss:  1.36164391040802\n",
      "G loss: 4.103269577026367\n",
      "E loss:  1.3484671115875244\n",
      "G loss: 4.094422340393066\n",
      "E loss:  1.3309887647628784\n",
      "G loss: 4.081683158874512\n",
      "E loss:  1.346203327178955\n",
      "G loss: 4.068384170532227\n",
      "E loss:  1.3107237815856934\n",
      "G loss: 4.050946235656738\n",
      "Training Model  ...\n",
      "E loss:  1.7761783599853516\n",
      "G loss: 4.050973892211914\n",
      "E loss:  1.7524099349975586\n",
      "G loss: 4.041156768798828\n",
      "E loss:  1.722731351852417\n",
      "G loss: 4.025561809539795\n",
      "E loss:  1.753529667854309\n",
      "G loss: 4.0173234939575195\n",
      "E loss:  1.7649427652359009\n",
      "G loss: 3.99584698677063\n",
      "Training Model  ...\n",
      "E loss:  1.629755973815918\n",
      "G loss: 4.006684303283691\n",
      "E loss:  1.6526950597763062\n",
      "G loss: 4.017712116241455\n",
      "E loss:  1.6619658470153809\n",
      "G loss: 4.0416154861450195\n",
      "E loss:  1.6243382692337036\n",
      "G loss: 4.062129020690918\n",
      "E loss:  1.6409908533096313\n",
      "G loss: 4.0965142250061035\n",
      "Training Model  ...\n",
      "E loss:  1.5265581607818604\n",
      "G loss: 4.096511363983154\n",
      "E loss:  1.4649310111999512\n",
      "G loss: 4.09481143951416\n",
      "E loss:  1.4828951358795166\n",
      "G loss: 4.102156162261963\n",
      "E loss:  1.4778774976730347\n",
      "G loss: 4.113966941833496\n",
      "E loss:  1.4792232513427734\n",
      "G loss: 4.11372184753418\n",
      "Training Model  ...\n",
      "E loss:  1.6125996112823486\n",
      "G loss: 4.121319770812988\n",
      "E loss:  1.6108636856079102\n",
      "G loss: 4.131332874298096\n",
      "E loss:  1.627288579940796\n",
      "G loss: 4.148731231689453\n",
      "E loss:  1.6427682638168335\n",
      "G loss: 4.160885810852051\n",
      "E loss:  1.6447060108184814\n",
      "G loss: 4.182480335235596\n",
      "Training Model  ...\n",
      "E loss:  1.70363187789917\n",
      "G loss: 4.170378684997559\n",
      "E loss:  1.7059528827667236\n",
      "G loss: 4.166438102722168\n",
      "E loss:  1.730291485786438\n",
      "G loss: 4.164126873016357\n",
      "E loss:  1.7414789199829102\n",
      "G loss: 4.147298812866211\n",
      "E loss:  1.7351267337799072\n",
      "G loss: 4.1416802406311035\n",
      "Training Model  ...\n",
      "E loss:  1.677561640739441\n",
      "G loss: 4.144080638885498\n",
      "E loss:  1.6825612783432007\n",
      "G loss: 4.152307987213135\n",
      "E loss:  1.6498713493347168\n",
      "G loss: 4.1582417488098145\n",
      "E loss:  1.636225700378418\n",
      "G loss: 4.167223930358887\n",
      "E loss:  1.6141021251678467\n",
      "G loss: 4.173892974853516\n",
      "Training Model  ...\n",
      "E loss:  1.5753602981567383\n",
      "G loss: 4.17767333984375\n",
      "E loss:  1.619591236114502\n",
      "G loss: 4.181732654571533\n",
      "E loss:  1.6216099262237549\n",
      "G loss: 4.190384387969971\n",
      "E loss:  1.574066162109375\n",
      "G loss: 4.192020893096924\n",
      "E loss:  1.5972076654434204\n",
      "G loss: 4.193030834197998\n",
      "Training Model  ...\n",
      "E loss:  1.9133594036102295\n",
      "G loss: 4.1964335441589355\n",
      "E loss:  1.9475582838058472\n",
      "G loss: 4.207647323608398\n",
      "E loss:  1.95750892162323\n",
      "G loss: 4.2171454429626465\n",
      "E loss:  1.90944504737854\n",
      "G loss: 4.219856262207031\n",
      "E loss:  1.8949933052062988\n",
      "G loss: 4.23568058013916\n",
      "Training Model  ...\n",
      "E loss:  1.8867454528808594\n",
      "G loss: 4.246094703674316\n",
      "E loss:  1.8646774291992188\n",
      "G loss: 4.253517150878906\n",
      "E loss:  1.8865642547607422\n",
      "G loss: 4.270997047424316\n",
      "E loss:  1.8614718914031982\n",
      "G loss: 4.281571388244629\n",
      "E loss:  1.852737545967102\n",
      "G loss: 4.300022602081299\n",
      "Training Model  ...\n",
      "E loss:  1.6046743392944336\n",
      "G loss: 4.2972798347473145\n",
      "E loss:  1.594025731086731\n",
      "G loss: 4.283970355987549\n",
      "E loss:  1.587149739265442\n",
      "G loss: 4.275569438934326\n",
      "E loss:  1.5700509548187256\n",
      "G loss: 4.2514777183532715\n",
      "E loss:  1.568873643875122\n",
      "G loss: 4.236906051635742\n",
      "Training Model  ...\n",
      "E loss:  1.63632071018219\n",
      "G loss: 4.233000755310059\n",
      "E loss:  1.6190170049667358\n",
      "G loss: 4.227130889892578\n",
      "E loss:  1.6120408773422241\n",
      "G loss: 4.215924263000488\n",
      "E loss:  1.5716164112091064\n",
      "G loss: 4.209028720855713\n",
      "E loss:  1.5782787799835205\n",
      "G loss: 4.201003551483154\n",
      "Training Model  ...\n",
      "E loss:  1.5428156852722168\n",
      "G loss: 4.179013252258301\n",
      "E loss:  1.5484299659729004\n",
      "G loss: 4.156259536743164\n",
      "E loss:  1.5367321968078613\n",
      "G loss: 4.118808269500732\n",
      "E loss:  1.516139268875122\n",
      "G loss: 4.078985214233398\n",
      "E loss:  1.528631329536438\n",
      "G loss: 4.024965763092041\n",
      "Training Model  ...\n",
      "E loss:  1.7817870378494263\n",
      "G loss: 4.0065484046936035\n",
      "E loss:  1.7453336715698242\n",
      "G loss: 3.980532646179199\n",
      "E loss:  1.7570172548294067\n",
      "G loss: 3.9421639442443848\n",
      "E loss:  1.7644785642623901\n",
      "G loss: 3.9031152725219727\n",
      "E loss:  1.7739336490631104\n",
      "G loss: 3.8558270931243896\n",
      "Training Model  ...\n",
      "E loss:  1.6031694412231445\n",
      "G loss: 3.870676040649414\n",
      "E loss:  1.5975677967071533\n",
      "G loss: 3.8837921619415283\n",
      "E loss:  1.6292415857315063\n",
      "G loss: 3.9114065170288086\n",
      "E loss:  1.6144505739212036\n",
      "G loss: 3.9402573108673096\n",
      "E loss:  1.5991111993789673\n",
      "G loss: 3.975301742553711\n",
      "Training Model  ...\n",
      "E loss:  1.6399736404418945\n",
      "G loss: 3.9832639694213867\n",
      "E loss:  1.659034013748169\n",
      "G loss: 3.992372751235962\n",
      "E loss:  1.637668251991272\n",
      "G loss: 4.002490520477295\n",
      "E loss:  1.6313655376434326\n",
      "G loss: 4.0072245597839355\n",
      "E loss:  1.6585851907730103\n",
      "G loss: 4.018040180206299\n",
      "Training Model  ...\n",
      "E loss:  1.7063627243041992\n",
      "G loss: 4.011845111846924\n",
      "E loss:  1.723773717880249\n",
      "G loss: 4.004141330718994\n",
      "E loss:  1.7254211902618408\n",
      "G loss: 3.989863395690918\n",
      "E loss:  1.7233412265777588\n",
      "G loss: 3.977297306060791\n",
      "E loss:  1.6997783184051514\n",
      "G loss: 3.971004009246826\n",
      "Training Model  ...\n",
      "E loss:  1.611161231994629\n",
      "G loss: 3.977916955947876\n",
      "E loss:  1.630671501159668\n",
      "G loss: 3.979365825653076\n",
      "E loss:  1.650241732597351\n",
      "G loss: 3.9861130714416504\n",
      "E loss:  1.6504137516021729\n",
      "G loss: 3.9937448501586914\n",
      "E loss:  1.663791298866272\n",
      "G loss: 4.004280090332031\n",
      "Training Model  ...\n",
      "E loss:  1.601228952407837\n",
      "G loss: 4.015397548675537\n",
      "E loss:  1.5867289304733276\n",
      "G loss: 4.035470485687256\n",
      "E loss:  1.5851716995239258\n",
      "G loss: 4.06537389755249\n",
      "E loss:  1.5819238424301147\n",
      "G loss: 4.109211444854736\n",
      "E loss:  1.5728449821472168\n",
      "G loss: 4.145332336425781\n",
      "Training Model  ...\n",
      "E loss:  1.5222610235214233\n",
      "G loss: 4.145668029785156\n",
      "E loss:  1.5036382675170898\n",
      "G loss: 4.1494364738464355\n",
      "E loss:  1.487320065498352\n",
      "G loss: 4.154058456420898\n",
      "E loss:  1.5014183521270752\n",
      "G loss: 4.163302898406982\n",
      "E loss:  1.49269700050354\n",
      "G loss: 4.171988487243652\n",
      "Training Model  ...\n",
      "E loss:  1.5802857875823975\n",
      "G loss: 4.162727355957031\n",
      "E loss:  1.5852162837982178\n",
      "G loss: 4.151169300079346\n",
      "E loss:  1.564603328704834\n",
      "G loss: 4.1245527267456055\n",
      "E loss:  1.5282988548278809\n",
      "G loss: 4.096736431121826\n",
      "E loss:  1.5215870141983032\n",
      "G loss: 4.0588579177856445\n",
      "Training Model  ...\n",
      "E loss:  1.7233307361602783\n",
      "G loss: 4.0572357177734375\n",
      "E loss:  1.7405920028686523\n",
      "G loss: 4.056872367858887\n",
      "E loss:  1.7615084648132324\n",
      "G loss: 4.056588172912598\n",
      "E loss:  1.7809978723526\n",
      "G loss: 4.055500030517578\n",
      "E loss:  1.8020968437194824\n",
      "G loss: 4.051763534545898\n",
      "Training Model  ...\n",
      "E loss:  1.5420105457305908\n",
      "G loss: 4.038309574127197\n",
      "E loss:  1.543083906173706\n",
      "G loss: 4.027056694030762\n",
      "E loss:  1.5185070037841797\n",
      "G loss: 3.995264768600464\n",
      "E loss:  1.5388401746749878\n",
      "G loss: 3.968724012374878\n",
      "E loss:  1.5314608812332153\n",
      "G loss: 3.936371088027954\n",
      "Training Model  ...\n",
      "E loss:  1.822044014930725\n",
      "G loss: 3.9439356327056885\n",
      "E loss:  1.8235712051391602\n",
      "G loss: 3.958925724029541\n",
      "E loss:  1.8092635869979858\n",
      "G loss: 3.984893321990967\n",
      "E loss:  1.7593183517456055\n",
      "G loss: 4.011277675628662\n",
      "E loss:  1.7773507833480835\n",
      "G loss: 4.052281856536865\n",
      "Training Model  ...\n",
      "E loss:  1.5450104475021362\n",
      "G loss: 4.053343772888184\n",
      "E loss:  1.5779744386672974\n",
      "G loss: 4.055710315704346\n",
      "E loss:  1.6011039018630981\n",
      "G loss: 4.050602436065674\n",
      "E loss:  1.6321996450424194\n",
      "G loss: 4.054837226867676\n",
      "E loss:  1.6224573850631714\n",
      "G loss: 4.051766395568848\n",
      "Training Model  ...\n",
      "E loss:  1.5468461513519287\n",
      "G loss: 4.058775424957275\n",
      "E loss:  1.5528920888900757\n",
      "G loss: 4.058579921722412\n",
      "E loss:  1.5603327751159668\n",
      "G loss: 4.071417808532715\n",
      "E loss:  1.5818397998809814\n",
      "G loss: 4.077033996582031\n",
      "E loss:  1.5776137113571167\n",
      "G loss: 4.083699703216553\n",
      "Training Model  ...\n",
      "E loss:  1.3563010692596436\n",
      "G loss: 4.081061363220215\n",
      "E loss:  1.407009243965149\n",
      "G loss: 4.080334663391113\n",
      "E loss:  1.38215172290802\n",
      "G loss: 4.078126907348633\n",
      "E loss:  1.4037244319915771\n",
      "G loss: 4.070217609405518\n",
      "E loss:  1.437027931213379\n",
      "G loss: 4.056153297424316\n",
      "Training Model  ...\n",
      "E loss:  1.6960676908493042\n",
      "G loss: 4.051546573638916\n",
      "E loss:  1.692104458808899\n",
      "G loss: 4.052374839782715\n",
      "E loss:  1.6958340406417847\n",
      "G loss: 4.044035911560059\n",
      "E loss:  1.648690104484558\n",
      "G loss: 4.020962238311768\n",
      "E loss:  1.6439781188964844\n",
      "G loss: 4.00943660736084\n",
      "Training Model  ...\n",
      "E loss:  1.4777730703353882\n",
      "G loss: 4.000413417816162\n",
      "E loss:  1.4519925117492676\n",
      "G loss: 3.9879555702209473\n",
      "E loss:  1.4653714895248413\n",
      "G loss: 3.9706573486328125\n",
      "E loss:  1.478290319442749\n",
      "G loss: 3.9483842849731445\n",
      "E loss:  1.468196153640747\n",
      "G loss: 3.926323890686035\n",
      "Training Model  ...\n",
      "E loss:  1.609666109085083\n",
      "G loss: 3.9169628620147705\n",
      "E loss:  1.619614839553833\n",
      "G loss: 3.9060606956481934\n",
      "E loss:  1.588210105895996\n",
      "G loss: 3.875476121902466\n",
      "E loss:  1.5692315101623535\n",
      "G loss: 3.8548264503479004\n",
      "E loss:  1.5821080207824707\n",
      "G loss: 3.825374126434326\n",
      "Training Model  ...\n",
      "E loss:  1.657426357269287\n",
      "G loss: 3.834782600402832\n",
      "E loss:  1.6787986755371094\n",
      "G loss: 3.840179204940796\n",
      "E loss:  1.6997156143188477\n",
      "G loss: 3.8533895015716553\n",
      "E loss:  1.6595525741577148\n",
      "G loss: 3.86989426612854\n",
      "E loss:  1.6503121852874756\n",
      "G loss: 3.8877665996551514\n",
      "Training Model  ...\n",
      "E loss:  1.559172511100769\n",
      "G loss: 3.891658306121826\n",
      "E loss:  1.5750770568847656\n",
      "G loss: 3.9003491401672363\n",
      "E loss:  1.5695421695709229\n",
      "G loss: 3.9133572578430176\n",
      "E loss:  1.5724891424179077\n",
      "G loss: 3.9198317527770996\n",
      "E loss:  1.5861079692840576\n",
      "G loss: 3.937838077545166\n",
      "Training Model  ...\n",
      "E loss:  1.6320116519927979\n",
      "G loss: 3.942056179046631\n",
      "E loss:  1.6417289972305298\n",
      "G loss: 3.933039903640747\n",
      "E loss:  1.662681221961975\n",
      "G loss: 3.940497875213623\n",
      "E loss:  1.6414361000061035\n",
      "G loss: 3.937915325164795\n",
      "E loss:  1.6472209692001343\n",
      "G loss: 3.938692808151245\n",
      "Training Model  ...\n",
      "E loss:  1.4842889308929443\n",
      "G loss: 3.9373607635498047\n",
      "E loss:  1.4959907531738281\n",
      "G loss: 3.930192708969116\n",
      "E loss:  1.5118675231933594\n",
      "G loss: 3.920109748840332\n",
      "E loss:  1.5256117582321167\n",
      "G loss: 3.9168179035186768\n",
      "E loss:  1.5388485193252563\n",
      "G loss: 3.904675245285034\n",
      "Training Model  ...\n",
      "E loss:  1.649632215499878\n",
      "G loss: 3.9131534099578857\n",
      "E loss:  1.6761878728866577\n",
      "G loss: 3.9256832599639893\n",
      "E loss:  1.6871449947357178\n",
      "G loss: 3.9481050968170166\n",
      "E loss:  1.6644352674484253\n",
      "G loss: 3.976257562637329\n",
      "E loss:  1.7065268754959106\n",
      "G loss: 4.000292778015137\n",
      "Training Model  ...\n",
      "E loss:  1.4620723724365234\n",
      "G loss: 3.9973082542419434\n",
      "E loss:  1.4583873748779297\n",
      "G loss: 3.9863486289978027\n",
      "E loss:  1.4408212900161743\n",
      "G loss: 3.985966205596924\n",
      "E loss:  1.4390323162078857\n",
      "G loss: 3.9818854331970215\n",
      "E loss:  1.4138519763946533\n",
      "G loss: 3.9767324924468994\n",
      "Training Model  ...\n",
      "E loss:  1.6175148487091064\n",
      "G loss: 3.9723360538482666\n",
      "E loss:  1.6232173442840576\n",
      "G loss: 3.962580680847168\n",
      "E loss:  1.6137149333953857\n",
      "G loss: 3.9575936794281006\n",
      "E loss:  1.6093230247497559\n",
      "G loss: 3.944201707839966\n",
      "E loss:  1.6164995431900024\n",
      "G loss: 3.9340667724609375\n",
      "Training Model  ...\n",
      "E loss:  1.772722840309143\n",
      "G loss: 3.9299027919769287\n",
      "E loss:  1.762645959854126\n",
      "G loss: 3.9141361713409424\n",
      "E loss:  1.7323001623153687\n",
      "G loss: 3.8959126472473145\n",
      "E loss:  1.7259238958358765\n",
      "G loss: 3.868446111679077\n",
      "E loss:  1.738429069519043\n",
      "G loss: 3.8374428749084473\n",
      "Training Model  ...\n",
      "E loss:  1.5857505798339844\n",
      "G loss: 3.8383307456970215\n",
      "E loss:  1.5836502313613892\n",
      "G loss: 3.8405189514160156\n",
      "E loss:  1.6014313697814941\n",
      "G loss: 3.8428006172180176\n",
      "E loss:  1.599160075187683\n",
      "G loss: 3.840179920196533\n",
      "E loss:  1.589227557182312\n",
      "G loss: 3.844593048095703\n",
      "Training Model  ...\n",
      "E loss:  1.523372769355774\n",
      "G loss: 3.847641944885254\n",
      "E loss:  1.5333927869796753\n",
      "G loss: 3.8584320545196533\n",
      "E loss:  1.542801022529602\n",
      "G loss: 3.8764872550964355\n",
      "E loss:  1.5319406986236572\n",
      "G loss: 3.8989996910095215\n",
      "E loss:  1.5272934436798096\n",
      "G loss: 3.9180870056152344\n",
      "Training Model  ...\n",
      "E loss:  1.5117350816726685\n",
      "G loss: 3.921696901321411\n",
      "E loss:  1.5130149126052856\n",
      "G loss: 3.928788185119629\n",
      "E loss:  1.5144612789154053\n",
      "G loss: 3.9365110397338867\n",
      "E loss:  1.5047556161880493\n",
      "G loss: 3.9490816593170166\n",
      "E loss:  1.5083744525909424\n",
      "G loss: 3.955556631088257\n",
      "Training Model  ...\n",
      "E loss:  1.5594912767410278\n",
      "G loss: 3.960528612136841\n",
      "E loss:  1.5575417280197144\n",
      "G loss: 3.9668970108032227\n",
      "E loss:  1.545334815979004\n",
      "G loss: 3.9752697944641113\n",
      "E loss:  1.5408687591552734\n",
      "G loss: 3.9866864681243896\n",
      "E loss:  1.5367990732192993\n",
      "G loss: 3.992626428604126\n",
      "Training Model  ...\n",
      "E loss:  1.5498862266540527\n",
      "G loss: 3.9839301109313965\n",
      "E loss:  1.549950361251831\n",
      "G loss: 3.9700403213500977\n",
      "E loss:  1.5732245445251465\n",
      "G loss: 3.960461139678955\n",
      "E loss:  1.5611603260040283\n",
      "G loss: 3.9447994232177734\n",
      "E loss:  1.5592714548110962\n",
      "G loss: 3.934347629547119\n",
      "Training Model  ...\n",
      "E loss:  1.5588737726211548\n",
      "G loss: 3.9374897480010986\n",
      "E loss:  1.5626957416534424\n",
      "G loss: 3.946798801422119\n",
      "E loss:  1.5912612676620483\n",
      "G loss: 3.9542312622070312\n",
      "E loss:  1.5638079643249512\n",
      "G loss: 3.9569284915924072\n",
      "E loss:  1.5855662822723389\n",
      "G loss: 3.975541353225708\n",
      "Training Model  ...\n",
      "E loss:  1.5036933422088623\n",
      "G loss: 3.9669129848480225\n",
      "E loss:  1.5170795917510986\n",
      "G loss: 3.959787130355835\n",
      "E loss:  1.5106756687164307\n",
      "G loss: 3.94816517829895\n",
      "E loss:  1.4756417274475098\n",
      "G loss: 3.9296226501464844\n",
      "E loss:  1.4920800924301147\n",
      "G loss: 3.904883623123169\n",
      "Training Model  ...\n",
      "E loss:  1.5920085906982422\n",
      "G loss: 3.919013500213623\n",
      "E loss:  1.5381560325622559\n",
      "G loss: 3.9156277179718018\n",
      "E loss:  1.5678808689117432\n",
      "G loss: 3.9200916290283203\n",
      "E loss:  1.581749439239502\n",
      "G loss: 3.9195032119750977\n",
      "E loss:  1.5725984573364258\n",
      "G loss: 3.9218764305114746\n",
      "Training Model  ...\n",
      "E loss:  1.646878719329834\n",
      "G loss: 3.93027400970459\n",
      "E loss:  1.609506368637085\n",
      "G loss: 3.9537501335144043\n",
      "E loss:  1.628240704536438\n",
      "G loss: 3.9657015800476074\n",
      "E loss:  1.6149351596832275\n",
      "G loss: 4.001450061798096\n",
      "E loss:  1.6175848245620728\n",
      "G loss: 4.032885551452637\n",
      "Training Model  ...\n",
      "E loss:  1.5882487297058105\n",
      "G loss: 4.027792453765869\n",
      "E loss:  1.5591167211532593\n",
      "G loss: 4.0239434242248535\n",
      "E loss:  1.5605255365371704\n",
      "G loss: 4.002121448516846\n",
      "E loss:  1.5346198081970215\n",
      "G loss: 3.994746208190918\n",
      "E loss:  1.5556919574737549\n",
      "G loss: 3.9666178226470947\n",
      "Training Model  ...\n",
      "E loss:  1.5094718933105469\n",
      "G loss: 3.9581947326660156\n",
      "E loss:  1.5302590131759644\n",
      "G loss: 3.9437243938446045\n",
      "E loss:  1.514021873474121\n",
      "G loss: 3.9201231002807617\n",
      "E loss:  1.5254237651824951\n",
      "G loss: 3.904625654220581\n",
      "E loss:  1.5171843767166138\n",
      "G loss: 3.8748648166656494\n",
      "Training Model  ...\n",
      "E loss:  1.5354973077774048\n",
      "G loss: 3.8738274574279785\n",
      "E loss:  1.5278904438018799\n",
      "G loss: 3.871842861175537\n",
      "E loss:  1.5269056558609009\n",
      "G loss: 3.876708745956421\n",
      "E loss:  1.546260952949524\n",
      "G loss: 3.875638723373413\n",
      "E loss:  1.5408037900924683\n",
      "G loss: 3.8788790702819824\n",
      "Training Model  ...\n",
      "E loss:  1.6272765398025513\n",
      "G loss: 3.866302251815796\n",
      "E loss:  1.645305871963501\n",
      "G loss: 3.8553056716918945\n",
      "E loss:  1.6318137645721436\n",
      "G loss: 3.8506298065185547\n",
      "E loss:  1.6076346635818481\n",
      "G loss: 3.836859703063965\n",
      "E loss:  1.6021480560302734\n",
      "G loss: 3.8200337886810303\n",
      "Training Model  ...\n",
      "E loss:  1.7502877712249756\n",
      "G loss: 3.8258588314056396\n",
      "E loss:  1.6764490604400635\n",
      "G loss: 3.834364891052246\n",
      "E loss:  1.651991605758667\n",
      "G loss: 3.853724956512451\n",
      "E loss:  1.6496992111206055\n",
      "G loss: 3.8684635162353516\n",
      "E loss:  1.6402027606964111\n",
      "G loss: 3.895897626876831\n",
      "Training Model  ...\n",
      "E loss:  1.6224571466445923\n",
      "G loss: 3.890775680541992\n",
      "E loss:  1.6301512718200684\n",
      "G loss: 3.8908658027648926\n",
      "E loss:  1.6150596141815186\n",
      "G loss: 3.8741540908813477\n",
      "E loss:  1.629873514175415\n",
      "G loss: 3.8680269718170166\n",
      "E loss:  1.6501705646514893\n",
      "G loss: 3.855377674102783\n",
      "Training Model  ...\n",
      "E loss:  1.529569387435913\n",
      "G loss: 3.854548931121826\n",
      "E loss:  1.5357788801193237\n",
      "G loss: 3.8492591381073\n",
      "E loss:  1.577441692352295\n",
      "G loss: 3.840390205383301\n",
      "E loss:  1.5780737400054932\n",
      "G loss: 3.833578586578369\n",
      "E loss:  1.5948307514190674\n",
      "G loss: 3.8184468746185303\n",
      "Training Model  ...\n",
      "E loss:  1.8173561096191406\n",
      "G loss: 3.821326494216919\n",
      "E loss:  1.8070130348205566\n",
      "G loss: 3.8219046592712402\n",
      "E loss:  1.7800960540771484\n",
      "G loss: 3.8234992027282715\n",
      "E loss:  1.8072258234024048\n",
      "G loss: 3.826120138168335\n",
      "E loss:  1.8025158643722534\n",
      "G loss: 3.826105833053589\n",
      "Training Model  ...\n",
      "E loss:  1.4412089586257935\n",
      "G loss: 3.820754051208496\n",
      "E loss:  1.4645990133285522\n",
      "G loss: 3.8207929134368896\n",
      "E loss:  1.4620813131332397\n",
      "G loss: 3.8107242584228516\n",
      "E loss:  1.4252887964248657\n",
      "G loss: 3.7959096431732178\n",
      "E loss:  1.4404350519180298\n",
      "G loss: 3.789541721343994\n",
      "Training Model  ...\n",
      "E loss:  1.636726975440979\n",
      "G loss: 3.795279026031494\n",
      "E loss:  1.6084694862365723\n",
      "G loss: 3.808690071105957\n",
      "E loss:  1.5820631980895996\n",
      "G loss: 3.841999053955078\n",
      "E loss:  1.5927454233169556\n",
      "G loss: 3.8693008422851562\n",
      "E loss:  1.6083694696426392\n",
      "G loss: 3.90183687210083\n",
      "Training Model  ...\n",
      "E loss:  1.7995275259017944\n",
      "G loss: 3.9153456687927246\n",
      "E loss:  1.7691481113433838\n",
      "G loss: 3.9293792247772217\n",
      "E loss:  1.7478669881820679\n",
      "G loss: 3.947946786880493\n",
      "E loss:  1.7485111951828003\n",
      "G loss: 3.9749491214752197\n",
      "E loss:  1.7192221879959106\n",
      "G loss: 4.000074863433838\n",
      "Training Model  ...\n",
      "E loss:  1.511480450630188\n",
      "G loss: 3.9856183528900146\n",
      "E loss:  1.4715582132339478\n",
      "G loss: 3.9539685249328613\n",
      "E loss:  1.5078730583190918\n",
      "G loss: 3.906928539276123\n",
      "E loss:  1.499258041381836\n",
      "G loss: 3.8602774143218994\n",
      "E loss:  1.5006041526794434\n",
      "G loss: 3.807950973510742\n",
      "Training Model  ...\n",
      "E loss:  1.7776960134506226\n",
      "G loss: 3.8105432987213135\n",
      "E loss:  1.7186129093170166\n",
      "G loss: 3.807236433029175\n",
      "E loss:  1.7150537967681885\n",
      "G loss: 3.819488763809204\n",
      "E loss:  1.7432587146759033\n",
      "G loss: 3.823185443878174\n",
      "E loss:  1.7564144134521484\n",
      "G loss: 3.829685688018799\n",
      "Training Model  ...\n",
      "E loss:  1.5557738542556763\n",
      "G loss: 3.8361268043518066\n",
      "E loss:  1.5285636186599731\n",
      "G loss: 3.8281843662261963\n",
      "E loss:  1.5153725147247314\n",
      "G loss: 3.8399081230163574\n",
      "E loss:  1.5013036727905273\n",
      "G loss: 3.8524253368377686\n",
      "E loss:  1.5069924592971802\n",
      "G loss: 3.8582911491394043\n",
      "Training Model  ...\n",
      "E loss:  1.4625916481018066\n",
      "G loss: 3.8546478748321533\n",
      "E loss:  1.4726266860961914\n",
      "G loss: 3.8492324352264404\n",
      "E loss:  1.4636846780776978\n",
      "G loss: 3.8362131118774414\n",
      "E loss:  1.4598854780197144\n",
      "G loss: 3.821108818054199\n",
      "E loss:  1.4631391763687134\n",
      "G loss: 3.810420036315918\n",
      "Training Model  ...\n",
      "E loss:  1.850574254989624\n",
      "G loss: 3.8045074939727783\n",
      "E loss:  1.8143110275268555\n",
      "G loss: 3.803375720977783\n",
      "E loss:  1.7719030380249023\n",
      "G loss: 3.798673629760742\n",
      "E loss:  1.754615306854248\n",
      "G loss: 3.7921276092529297\n",
      "E loss:  1.7357747554779053\n",
      "G loss: 3.7779440879821777\n",
      "Training Model  ...\n",
      "E loss:  1.6057720184326172\n",
      "G loss: 3.7791905403137207\n",
      "E loss:  1.6162943840026855\n",
      "G loss: 3.77443265914917\n",
      "E loss:  1.6556994915008545\n",
      "G loss: 3.7645554542541504\n",
      "E loss:  1.6335265636444092\n",
      "G loss: 3.76676344871521\n",
      "E loss:  1.669096827507019\n",
      "G loss: 3.7554945945739746\n",
      "Training Model  ...\n",
      "E loss:  1.530335545539856\n",
      "G loss: 3.7601096630096436\n",
      "E loss:  1.5381343364715576\n",
      "G loss: 3.76547908782959\n",
      "E loss:  1.5290470123291016\n",
      "G loss: 3.7717700004577637\n",
      "E loss:  1.5086326599121094\n",
      "G loss: 3.774902582168579\n",
      "E loss:  1.5218968391418457\n",
      "G loss: 3.7819912433624268\n",
      "Training Model  ...\n",
      "E loss:  1.5373623371124268\n",
      "G loss: 3.789865732192993\n",
      "E loss:  1.5571619272232056\n",
      "G loss: 3.8020291328430176\n",
      "E loss:  1.5785517692565918\n",
      "G loss: 3.814495086669922\n",
      "E loss:  1.5627151727676392\n",
      "G loss: 3.831251859664917\n",
      "E loss:  1.5872617959976196\n",
      "G loss: 3.852773904800415\n",
      "Training Model  ...\n",
      "E loss:  1.6753138303756714\n",
      "G loss: 3.865875005722046\n",
      "E loss:  1.6688697338104248\n",
      "G loss: 3.88080096244812\n",
      "E loss:  1.6677995920181274\n",
      "G loss: 3.900087594985962\n",
      "E loss:  1.6726748943328857\n",
      "G loss: 3.9373412132263184\n",
      "E loss:  1.6878747940063477\n",
      "G loss: 3.970916986465454\n",
      "Training Model  ...\n",
      "E loss:  1.5873714685440063\n",
      "G loss: 3.954416513442993\n",
      "E loss:  1.5751978158950806\n",
      "G loss: 3.934941053390503\n",
      "E loss:  1.5889354944229126\n",
      "G loss: 3.9091548919677734\n",
      "E loss:  1.5708009004592896\n",
      "G loss: 3.8611207008361816\n",
      "E loss:  1.5411673784255981\n",
      "G loss: 3.8185219764709473\n",
      "Training Model  ...\n",
      "E loss:  1.4750328063964844\n",
      "G loss: 3.8202054500579834\n",
      "E loss:  1.4973852634429932\n",
      "G loss: 3.8161277770996094\n",
      "E loss:  1.4640884399414062\n",
      "G loss: 3.8248157501220703\n",
      "E loss:  1.4613673686981201\n",
      "G loss: 3.824777126312256\n",
      "E loss:  1.4912495613098145\n",
      "G loss: 3.8265151977539062\n",
      "Training Model  ...\n",
      "E loss:  1.6419622898101807\n",
      "G loss: 3.8214681148529053\n",
      "E loss:  1.6183068752288818\n",
      "G loss: 3.8095309734344482\n",
      "E loss:  1.6160889863967896\n",
      "G loss: 3.789242744445801\n",
      "E loss:  1.6150672435760498\n",
      "G loss: 3.768375873565674\n",
      "E loss:  1.6424322128295898\n",
      "G loss: 3.743344306945801\n",
      "Training Model  ...\n",
      "E loss:  1.6550248861312866\n",
      "G loss: 3.747591972351074\n",
      "E loss:  1.6794073581695557\n",
      "G loss: 3.7605419158935547\n",
      "E loss:  1.6731629371643066\n",
      "G loss: 3.761237621307373\n",
      "E loss:  1.6654021739959717\n",
      "G loss: 3.772493362426758\n",
      "E loss:  1.658717155456543\n",
      "G loss: 3.7829232215881348\n",
      "Training Model  ...\n",
      "E loss:  1.6158474683761597\n",
      "G loss: 3.782099962234497\n",
      "E loss:  1.5989391803741455\n",
      "G loss: 3.790393590927124\n",
      "E loss:  1.605317234992981\n",
      "G loss: 3.8005502223968506\n",
      "E loss:  1.5957859754562378\n",
      "G loss: 3.807737112045288\n",
      "E loss:  1.6002402305603027\n",
      "G loss: 3.8160176277160645\n",
      "Training Model  ...\n",
      "E loss:  1.5159900188446045\n",
      "G loss: 3.8130383491516113\n",
      "E loss:  1.510374665260315\n",
      "G loss: 3.7929883003234863\n",
      "E loss:  1.5014857053756714\n",
      "G loss: 3.777360200881958\n",
      "E loss:  1.5205891132354736\n",
      "G loss: 3.751838207244873\n",
      "E loss:  1.5593948364257812\n",
      "G loss: 3.7336418628692627\n",
      "Training Model  ...\n",
      "E loss:  1.3184391260147095\n",
      "G loss: 3.7269928455352783\n",
      "E loss:  1.368463397026062\n",
      "G loss: 3.714775323867798\n",
      "E loss:  1.385829210281372\n",
      "G loss: 3.7008519172668457\n",
      "E loss:  1.3667347431182861\n",
      "G loss: 3.6836752891540527\n",
      "E loss:  1.3649030923843384\n",
      "G loss: 3.6615400314331055\n",
      "Training Model  ...\n",
      "E loss:  1.6413453817367554\n",
      "G loss: 3.661597490310669\n",
      "E loss:  1.6821120977401733\n",
      "G loss: 3.6548638343811035\n",
      "E loss:  1.6727100610733032\n",
      "G loss: 3.6412923336029053\n",
      "E loss:  1.6547086238861084\n",
      "G loss: 3.6265616416931152\n",
      "E loss:  1.6558555364608765\n",
      "G loss: 3.6130263805389404\n",
      "Training Model  ...\n",
      "E loss:  1.525523066520691\n",
      "G loss: 3.603391647338867\n",
      "E loss:  1.494268536567688\n",
      "G loss: 3.6012988090515137\n",
      "E loss:  1.4785743951797485\n",
      "G loss: 3.593600034713745\n",
      "E loss:  1.4815917015075684\n",
      "G loss: 3.5815951824188232\n",
      "E loss:  1.4605603218078613\n",
      "G loss: 3.5743210315704346\n",
      "Training Model  ...\n",
      "E loss:  1.598167896270752\n",
      "G loss: 3.585326910018921\n",
      "E loss:  1.6125649213790894\n",
      "G loss: 3.5930466651916504\n",
      "E loss:  1.5976753234863281\n",
      "G loss: 3.616649627685547\n",
      "E loss:  1.6179614067077637\n",
      "G loss: 3.6434566974639893\n",
      "E loss:  1.6190356016159058\n",
      "G loss: 3.6790454387664795\n",
      "Training Model  ...\n",
      "E loss:  1.7157931327819824\n",
      "G loss: 3.689148426055908\n",
      "E loss:  1.698322057723999\n",
      "G loss: 3.6911697387695312\n",
      "E loss:  1.701155185699463\n",
      "G loss: 3.7042582035064697\n",
      "E loss:  1.6814155578613281\n",
      "G loss: 3.719940662384033\n",
      "E loss:  1.6589622497558594\n",
      "G loss: 3.7302489280700684\n",
      "Training Model  ...\n",
      "E loss:  1.5995177030563354\n",
      "G loss: 3.724566698074341\n",
      "E loss:  1.6190584897994995\n",
      "G loss: 3.7223479747772217\n",
      "E loss:  1.6512506008148193\n",
      "G loss: 3.7296864986419678\n",
      "E loss:  1.6601355075836182\n",
      "G loss: 3.7324323654174805\n",
      "E loss:  1.6726232767105103\n",
      "G loss: 3.736473798751831\n",
      "Training Model  ...\n",
      "E loss:  1.630883812904358\n",
      "G loss: 3.726268768310547\n",
      "E loss:  1.634950876235962\n",
      "G loss: 3.715336322784424\n",
      "E loss:  1.6597790718078613\n",
      "G loss: 3.6885573863983154\n",
      "E loss:  1.6592751741409302\n",
      "G loss: 3.6510226726531982\n",
      "E loss:  1.669317364692688\n",
      "G loss: 3.622466564178467\n",
      "Training Model  ...\n",
      "E loss:  1.636374831199646\n",
      "G loss: 3.6134166717529297\n",
      "E loss:  1.6529161930084229\n",
      "G loss: 3.602294445037842\n",
      "E loss:  1.6264526844024658\n",
      "G loss: 3.583714246749878\n",
      "E loss:  1.62832772731781\n",
      "G loss: 3.5624611377716064\n",
      "E loss:  1.600898265838623\n",
      "G loss: 3.5411691665649414\n",
      "Training Model  ...\n",
      "E loss:  1.5449585914611816\n",
      "G loss: 3.5498085021972656\n",
      "E loss:  1.5471172332763672\n",
      "G loss: 3.566066265106201\n",
      "E loss:  1.5557336807250977\n",
      "G loss: 3.5678043365478516\n",
      "E loss:  1.5814409255981445\n",
      "G loss: 3.5844688415527344\n",
      "E loss:  1.5705769062042236\n",
      "G loss: 3.6054272651672363\n",
      "Training Model  ...\n",
      "E loss:  1.445627212524414\n",
      "G loss: 3.5997369289398193\n",
      "E loss:  1.4826146364212036\n",
      "G loss: 3.6004552841186523\n",
      "E loss:  1.493536114692688\n",
      "G loss: 3.5939574241638184\n",
      "E loss:  1.4826099872589111\n",
      "G loss: 3.5825347900390625\n",
      "E loss:  1.461932897567749\n",
      "G loss: 3.5891993045806885\n",
      "Training Model  ...\n",
      "E loss:  1.671558141708374\n",
      "G loss: 3.585075616836548\n",
      "E loss:  1.6767240762710571\n",
      "G loss: 3.609267234802246\n",
      "E loss:  1.6901906728744507\n",
      "G loss: 3.6137304306030273\n",
      "E loss:  1.7240678071975708\n",
      "G loss: 3.636970043182373\n",
      "E loss:  1.6758434772491455\n",
      "G loss: 3.661581516265869\n",
      "Training Model  ...\n",
      "E loss:  1.7001107931137085\n",
      "G loss: 3.6630442142486572\n",
      "E loss:  1.7119776010513306\n",
      "G loss: 3.6639797687530518\n",
      "E loss:  1.6851117610931396\n",
      "G loss: 3.6640684604644775\n",
      "E loss:  1.6770375967025757\n",
      "G loss: 3.6607909202575684\n",
      "E loss:  1.714817762374878\n",
      "G loss: 3.6532883644104004\n",
      "Training Model  ...\n",
      "E loss:  1.5152637958526611\n",
      "G loss: 3.6568658351898193\n",
      "E loss:  1.498861312866211\n",
      "G loss: 3.6609466075897217\n",
      "E loss:  1.4977595806121826\n",
      "G loss: 3.665454864501953\n",
      "E loss:  1.4951050281524658\n",
      "G loss: 3.6641461849212646\n",
      "E loss:  1.4843145608901978\n",
      "G loss: 3.667304515838623\n",
      "Training Model  ...\n",
      "E loss:  1.6266974210739136\n",
      "G loss: 3.664980888366699\n",
      "E loss:  1.6072708368301392\n",
      "G loss: 3.664888858795166\n",
      "E loss:  1.5877387523651123\n",
      "G loss: 3.6516494750976562\n",
      "E loss:  1.5716447830200195\n",
      "G loss: 3.64274263381958\n",
      "E loss:  1.5728737115859985\n",
      "G loss: 3.6352148056030273\n",
      "Training Model  ...\n",
      "E loss:  1.6265618801116943\n",
      "G loss: 3.634777307510376\n",
      "E loss:  1.6153464317321777\n",
      "G loss: 3.644240379333496\n",
      "E loss:  1.6207656860351562\n",
      "G loss: 3.6472694873809814\n",
      "E loss:  1.6247584819793701\n",
      "G loss: 3.6544578075408936\n",
      "E loss:  1.6219520568847656\n",
      "G loss: 3.6560750007629395\n",
      "Training Model  ...\n",
      "E loss:  1.4986950159072876\n",
      "G loss: 3.659553289413452\n",
      "E loss:  1.5024555921554565\n",
      "G loss: 3.6619715690612793\n",
      "E loss:  1.5122953653335571\n",
      "G loss: 3.659900188446045\n",
      "E loss:  1.4980732202529907\n",
      "G loss: 3.672990322113037\n",
      "E loss:  1.4909570217132568\n",
      "G loss: 3.6832327842712402\n",
      "Training Model  ...\n",
      "E loss:  1.4953761100769043\n",
      "G loss: 3.66686749458313\n",
      "E loss:  1.4997166395187378\n",
      "G loss: 3.666316032409668\n",
      "E loss:  1.5071171522140503\n",
      "G loss: 3.659846067428589\n",
      "E loss:  1.5180721282958984\n",
      "G loss: 3.641127824783325\n",
      "E loss:  1.5232253074645996\n",
      "G loss: 3.6341781616210938\n",
      "Training Model  ...\n",
      "E loss:  1.560036540031433\n",
      "G loss: 3.6495649814605713\n",
      "E loss:  1.5740338563919067\n",
      "G loss: 3.6574790477752686\n",
      "E loss:  1.570084571838379\n",
      "G loss: 3.6636950969696045\n",
      "E loss:  1.5912573337554932\n",
      "G loss: 3.6966755390167236\n",
      "E loss:  1.6071265935897827\n",
      "G loss: 3.7178266048431396\n",
      "Training Model  ...\n",
      "E loss:  1.9312504529953003\n",
      "G loss: 3.7224929332733154\n",
      "E loss:  1.9264782667160034\n",
      "G loss: 3.7493231296539307\n",
      "E loss:  1.9419506788253784\n",
      "G loss: 3.77032470703125\n",
      "E loss:  1.9461969137191772\n",
      "G loss: 3.7942779064178467\n",
      "E loss:  1.911550521850586\n",
      "G loss: 3.826281785964966\n",
      "Training Model  ...\n",
      "E loss:  1.5839402675628662\n",
      "G loss: 3.8317770957946777\n",
      "E loss:  1.5611191987991333\n",
      "G loss: 3.8358635902404785\n",
      "E loss:  1.5970144271850586\n",
      "G loss: 3.8474202156066895\n",
      "E loss:  1.6042518615722656\n",
      "G loss: 3.8545470237731934\n",
      "E loss:  1.6294074058532715\n",
      "G loss: 3.867100954055786\n",
      "Training Model  ...\n",
      "E loss:  1.6241133213043213\n",
      "G loss: 3.865994453430176\n",
      "E loss:  1.5873173475265503\n",
      "G loss: 3.874673366546631\n",
      "E loss:  1.5827850103378296\n",
      "G loss: 3.8831534385681152\n",
      "E loss:  1.5519120693206787\n",
      "G loss: 3.8887641429901123\n",
      "E loss:  1.5603747367858887\n",
      "G loss: 3.8882670402526855\n",
      "Training Model  ...\n",
      "E loss:  1.6789978742599487\n",
      "G loss: 3.893413543701172\n",
      "E loss:  1.6567871570587158\n",
      "G loss: 3.895230770111084\n",
      "E loss:  1.6701903343200684\n",
      "G loss: 3.897536039352417\n",
      "E loss:  1.6660926342010498\n",
      "G loss: 3.8988521099090576\n",
      "E loss:  1.6265003681182861\n",
      "G loss: 3.909743309020996\n",
      "Training Model  ...\n",
      "E loss:  1.5237846374511719\n",
      "G loss: 3.9009408950805664\n",
      "E loss:  1.5662481784820557\n",
      "G loss: 3.899233102798462\n",
      "E loss:  1.5692873001098633\n",
      "G loss: 3.8853261470794678\n",
      "E loss:  1.5328094959259033\n",
      "G loss: 3.8692479133605957\n",
      "E loss:  1.5378897190093994\n",
      "G loss: 3.8592584133148193\n",
      "Training Model  ...\n",
      "E loss:  1.626907229423523\n",
      "G loss: 3.852160930633545\n",
      "E loss:  1.599447250366211\n",
      "G loss: 3.849414348602295\n",
      "E loss:  1.620766282081604\n",
      "G loss: 3.8449509143829346\n",
      "E loss:  1.6101304292678833\n",
      "G loss: 3.8299343585968018\n",
      "E loss:  1.609570860862732\n",
      "G loss: 3.8168184757232666\n",
      "Training Model  ...\n",
      "E loss:  1.4978446960449219\n",
      "G loss: 3.817164659500122\n",
      "E loss:  1.5121891498565674\n",
      "G loss: 3.8127684593200684\n",
      "E loss:  1.5123906135559082\n",
      "G loss: 3.815537929534912\n",
      "E loss:  1.475692629814148\n",
      "G loss: 3.805568218231201\n",
      "E loss:  1.4820106029510498\n",
      "G loss: 3.7982211112976074\n",
      "Training Model  ...\n",
      "E loss:  1.699040412902832\n",
      "G loss: 3.79360294342041\n",
      "E loss:  1.6593501567840576\n",
      "G loss: 3.7919411659240723\n",
      "E loss:  1.6801105737686157\n",
      "G loss: 3.7904603481292725\n",
      "E loss:  1.66981041431427\n",
      "G loss: 3.7869343757629395\n",
      "E loss:  1.6630983352661133\n",
      "G loss: 3.785973310470581\n",
      "Training Model  ...\n",
      "E loss:  1.3904831409454346\n",
      "G loss: 3.781363010406494\n",
      "E loss:  1.4016382694244385\n",
      "G loss: 3.769320487976074\n",
      "E loss:  1.3942772150039673\n",
      "G loss: 3.755272388458252\n",
      "E loss:  1.408252477645874\n",
      "G loss: 3.7410502433776855\n",
      "E loss:  1.413851022720337\n",
      "G loss: 3.7217562198638916\n",
      "Training Model  ...\n",
      "E loss:  1.5999958515167236\n",
      "G loss: 3.730142831802368\n",
      "E loss:  1.5742119550704956\n",
      "G loss: 3.7324490547180176\n",
      "E loss:  1.6128623485565186\n",
      "G loss: 3.7346901893615723\n",
      "E loss:  1.6045767068862915\n",
      "G loss: 3.7460367679595947\n",
      "E loss:  1.5960438251495361\n",
      "G loss: 3.754986047744751\n",
      "Training Model  ...\n",
      "E loss:  1.633484125137329\n",
      "G loss: 3.7565810680389404\n",
      "E loss:  1.635533094406128\n",
      "G loss: 3.7658493518829346\n",
      "E loss:  1.651613473892212\n",
      "G loss: 3.7568938732147217\n",
      "E loss:  1.6353660821914673\n",
      "G loss: 3.7597744464874268\n",
      "E loss:  1.6494389772415161\n",
      "G loss: 3.7594034671783447\n",
      "Training Model  ...\n",
      "E loss:  1.5479133129119873\n",
      "G loss: 3.762909173965454\n",
      "E loss:  1.5563491582870483\n",
      "G loss: 3.7613577842712402\n",
      "E loss:  1.5119777917861938\n",
      "G loss: 3.766425609588623\n",
      "E loss:  1.486523151397705\n",
      "G loss: 3.764341354370117\n",
      "E loss:  1.5174634456634521\n",
      "G loss: 3.779010772705078\n",
      "Training Model  ...\n",
      "E loss:  1.5252633094787598\n",
      "G loss: 3.771317481994629\n",
      "E loss:  1.5443601608276367\n",
      "G loss: 3.777358293533325\n",
      "E loss:  1.5398526191711426\n",
      "G loss: 3.772326707839966\n",
      "E loss:  1.5244945287704468\n",
      "G loss: 3.7813422679901123\n",
      "E loss:  1.5366452932357788\n",
      "G loss: 3.785212755203247\n",
      "Training Model  ...\n",
      "E loss:  1.5444653034210205\n",
      "G loss: 3.7773666381835938\n",
      "E loss:  1.538559079170227\n",
      "G loss: 3.7660999298095703\n",
      "E loss:  1.483364462852478\n",
      "G loss: 3.7473087310791016\n",
      "E loss:  1.4633734226226807\n",
      "G loss: 3.7255053520202637\n",
      "E loss:  1.487121820449829\n",
      "G loss: 3.707970142364502\n",
      "Training Model  ...\n",
      "E loss:  1.5543311834335327\n",
      "G loss: 3.7184224128723145\n",
      "E loss:  1.598105549812317\n",
      "G loss: 3.7260797023773193\n",
      "E loss:  1.5930018424987793\n",
      "G loss: 3.754117012023926\n",
      "E loss:  1.5895298719406128\n",
      "G loss: 3.789792537689209\n",
      "E loss:  1.585336685180664\n",
      "G loss: 3.8147709369659424\n",
      "Training Model  ...\n",
      "E loss:  1.6273759603500366\n",
      "G loss: 3.811326026916504\n",
      "E loss:  1.6331814527511597\n",
      "G loss: 3.795130729675293\n",
      "E loss:  1.6595144271850586\n",
      "G loss: 3.7744216918945312\n",
      "E loss:  1.6332480907440186\n",
      "G loss: 3.748488187789917\n",
      "E loss:  1.6300113201141357\n",
      "G loss: 3.711233139038086\n",
      "Training Model  ...\n",
      "E loss:  1.5304803848266602\n",
      "G loss: 3.7100958824157715\n",
      "E loss:  1.5368437767028809\n",
      "G loss: 3.6888699531555176\n",
      "E loss:  1.555356740951538\n",
      "G loss: 3.672908306121826\n",
      "E loss:  1.567718744277954\n",
      "G loss: 3.649428367614746\n",
      "E loss:  1.5786404609680176\n",
      "G loss: 3.63116192817688\n",
      "Training Model  ...\n",
      "E loss:  1.8369066715240479\n",
      "G loss: 3.6363072395324707\n",
      "E loss:  1.8379731178283691\n",
      "G loss: 3.6407413482666016\n",
      "E loss:  1.8192623853683472\n",
      "G loss: 3.649336814880371\n",
      "E loss:  1.7727634906768799\n",
      "G loss: 3.6621856689453125\n",
      "E loss:  1.7487207651138306\n",
      "G loss: 3.6729907989501953\n",
      "Training Model  ...\n",
      "E loss:  1.4988720417022705\n",
      "G loss: 3.6768922805786133\n",
      "E loss:  1.541516900062561\n",
      "G loss: 3.677996873855591\n",
      "E loss:  1.5412132740020752\n",
      "G loss: 3.675480842590332\n",
      "E loss:  1.5674424171447754\n",
      "G loss: 3.6837854385375977\n",
      "E loss:  1.5718897581100464\n",
      "G loss: 3.676830291748047\n",
      "Training Model  ...\n",
      "E loss:  1.606562852859497\n",
      "G loss: 3.680116653442383\n",
      "E loss:  1.6080800294876099\n",
      "G loss: 3.6789984703063965\n",
      "E loss:  1.6165626049041748\n",
      "G loss: 3.6747331619262695\n",
      "E loss:  1.6222844123840332\n",
      "G loss: 3.6878132820129395\n",
      "E loss:  1.595381736755371\n",
      "G loss: 3.6835901737213135\n",
      "Training Model  ...\n",
      "E loss:  1.5086473226547241\n",
      "G loss: 3.677884101867676\n",
      "E loss:  1.497599720954895\n",
      "G loss: 3.6582674980163574\n",
      "E loss:  1.4547309875488281\n",
      "G loss: 3.6280245780944824\n",
      "E loss:  1.471042513847351\n",
      "G loss: 3.5988881587982178\n",
      "E loss:  1.4700429439544678\n",
      "G loss: 3.5634829998016357\n",
      "Training Model  ...\n",
      "E loss:  1.419244647026062\n",
      "G loss: 3.5590107440948486\n",
      "E loss:  1.443885087966919\n",
      "G loss: 3.5411672592163086\n",
      "E loss:  1.4157676696777344\n",
      "G loss: 3.51365327835083\n",
      "E loss:  1.470077395439148\n",
      "G loss: 3.4876699447631836\n",
      "E loss:  1.4689429998397827\n",
      "G loss: 3.451016902923584\n",
      "Training Model  ...\n",
      "E loss:  1.4929083585739136\n",
      "G loss: 3.4469571113586426\n",
      "E loss:  1.505188226699829\n",
      "G loss: 3.4532320499420166\n",
      "E loss:  1.4861329793930054\n",
      "G loss: 3.4409430027008057\n",
      "E loss:  1.5227168798446655\n",
      "G loss: 3.4416184425354004\n",
      "E loss:  1.5293865203857422\n",
      "G loss: 3.4355661869049072\n",
      "Training Model  ...\n",
      "E loss:  1.5973578691482544\n",
      "G loss: 3.432767868041992\n",
      "E loss:  1.6257916688919067\n",
      "G loss: 3.41815447807312\n",
      "E loss:  1.641819953918457\n",
      "G loss: 3.4050722122192383\n",
      "E loss:  1.665666937828064\n",
      "G loss: 3.3860151767730713\n",
      "E loss:  1.6770719289779663\n",
      "G loss: 3.3676059246063232\n",
      "Training Model  ...\n",
      "E loss:  1.636389136314392\n",
      "G loss: 3.3824362754821777\n",
      "E loss:  1.6116126775741577\n",
      "G loss: 3.4155712127685547\n",
      "E loss:  1.6183487176895142\n",
      "G loss: 3.4534757137298584\n",
      "E loss:  1.5837041139602661\n",
      "G loss: 3.489860773086548\n",
      "E loss:  1.5739129781723022\n",
      "G loss: 3.544508695602417\n",
      "Training Model  ...\n",
      "E loss:  1.867865800857544\n",
      "G loss: 3.5522093772888184\n",
      "E loss:  1.8757292032241821\n",
      "G loss: 3.559922695159912\n",
      "E loss:  1.9005389213562012\n",
      "G loss: 3.5712809562683105\n",
      "E loss:  1.8709771633148193\n",
      "G loss: 3.59354305267334\n",
      "E loss:  1.852047085762024\n",
      "G loss: 3.6147608757019043\n",
      "Training Model  ...\n",
      "E loss:  1.4791934490203857\n",
      "G loss: 3.5975301265716553\n",
      "E loss:  1.4930481910705566\n",
      "G loss: 3.5881383419036865\n",
      "E loss:  1.4500236511230469\n",
      "G loss: 3.573038339614868\n",
      "E loss:  1.4610053300857544\n",
      "G loss: 3.55920672416687\n",
      "E loss:  1.4442323446273804\n",
      "G loss: 3.5355982780456543\n",
      "Training Model  ...\n",
      "E loss:  1.4989440441131592\n",
      "G loss: 3.5385119915008545\n",
      "E loss:  1.4922046661376953\n",
      "G loss: 3.562899589538574\n",
      "E loss:  1.4625264406204224\n",
      "G loss: 3.5876975059509277\n",
      "E loss:  1.4747631549835205\n",
      "G loss: 3.6103808879852295\n",
      "E loss:  1.441735863685608\n",
      "G loss: 3.6456642150878906\n",
      "Training Model  ...\n",
      "E loss:  1.4797654151916504\n",
      "G loss: 3.632991313934326\n",
      "E loss:  1.4763332605361938\n",
      "G loss: 3.624819040298462\n",
      "E loss:  1.4610233306884766\n",
      "G loss: 3.5992839336395264\n",
      "E loss:  1.4514235258102417\n",
      "G loss: 3.577982187271118\n",
      "E loss:  1.4368681907653809\n",
      "G loss: 3.5481412410736084\n",
      "Training Model  ...\n",
      "E loss:  1.6678051948547363\n",
      "G loss: 3.5572776794433594\n",
      "E loss:  1.6873259544372559\n",
      "G loss: 3.5541586875915527\n",
      "E loss:  1.6522314548492432\n",
      "G loss: 3.5519258975982666\n",
      "E loss:  1.617850422859192\n",
      "G loss: 3.5455808639526367\n",
      "E loss:  1.6539396047592163\n",
      "G loss: 3.535888910293579\n",
      "Training Model  ...\n",
      "E loss:  1.7044016122817993\n",
      "G loss: 3.550320625305176\n",
      "E loss:  1.7185145616531372\n",
      "G loss: 3.5614969730377197\n",
      "E loss:  1.7170486450195312\n",
      "G loss: 3.56376051902771\n",
      "E loss:  1.721849799156189\n",
      "G loss: 3.5754270553588867\n",
      "E loss:  1.7112362384796143\n",
      "G loss: 3.586766481399536\n",
      "Training Model  ...\n",
      "E loss:  1.453835129737854\n",
      "G loss: 3.604391098022461\n",
      "E loss:  1.4524104595184326\n",
      "G loss: 3.5946316719055176\n",
      "E loss:  1.4534308910369873\n",
      "G loss: 3.584641933441162\n",
      "E loss:  1.4467508792877197\n",
      "G loss: 3.5922904014587402\n",
      "E loss:  1.473364233970642\n",
      "G loss: 3.5842623710632324\n",
      "Training Model  ...\n",
      "E loss:  1.551438331604004\n",
      "G loss: 3.585879325866699\n",
      "E loss:  1.553187370300293\n",
      "G loss: 3.5719799995422363\n",
      "E loss:  1.5111393928527832\n",
      "G loss: 3.5698094367980957\n",
      "E loss:  1.5009303092956543\n",
      "G loss: 3.5619561672210693\n",
      "E loss:  1.5073813199996948\n",
      "G loss: 3.560816764831543\n",
      "Training Model  ...\n",
      "E loss:  1.4893662929534912\n",
      "G loss: 3.549126148223877\n",
      "E loss:  1.492483377456665\n",
      "G loss: 3.537329912185669\n",
      "E loss:  1.4720346927642822\n",
      "G loss: 3.5337791442871094\n",
      "E loss:  1.47489595413208\n",
      "G loss: 3.519751787185669\n",
      "E loss:  1.4688184261322021\n",
      "G loss: 3.507181167602539\n",
      "Training Model  ...\n",
      "E loss:  1.6329350471496582\n",
      "G loss: 3.50187349319458\n",
      "E loss:  1.596792459487915\n",
      "G loss: 3.5083584785461426\n",
      "E loss:  1.5808417797088623\n",
      "G loss: 3.494978189468384\n",
      "E loss:  1.5862832069396973\n",
      "G loss: 3.502460241317749\n",
      "E loss:  1.5669807195663452\n",
      "G loss: 3.501296043395996\n",
      "Training Model  ...\n",
      "E loss:  1.548703670501709\n",
      "G loss: 3.5001895427703857\n",
      "E loss:  1.5465011596679688\n",
      "G loss: 3.4892501831054688\n",
      "E loss:  1.5755763053894043\n",
      "G loss: 3.47603440284729\n",
      "E loss:  1.5731918811798096\n",
      "G loss: 3.47269344329834\n",
      "E loss:  1.5339559316635132\n",
      "G loss: 3.4493162631988525\n",
      "Training Model  ...\n",
      "E loss:  1.6605850458145142\n",
      "G loss: 3.4527032375335693\n",
      "E loss:  1.674651026725769\n",
      "G loss: 3.4549520015716553\n",
      "E loss:  1.6926217079162598\n",
      "G loss: 3.4561355113983154\n",
      "E loss:  1.6875544786453247\n",
      "G loss: 3.4628853797912598\n",
      "E loss:  1.6113064289093018\n",
      "G loss: 3.4696881771087646\n",
      "Training Model  ...\n",
      "E loss:  1.6579923629760742\n",
      "G loss: 3.4656364917755127\n",
      "E loss:  1.6715672016143799\n",
      "G loss: 3.459885835647583\n",
      "E loss:  1.6437345743179321\n",
      "G loss: 3.463672637939453\n",
      "E loss:  1.6478509902954102\n",
      "G loss: 3.4563868045806885\n",
      "E loss:  1.6416592597961426\n",
      "G loss: 3.4541354179382324\n",
      "Training Model  ...\n",
      "E loss:  1.678534984588623\n",
      "G loss: 3.4602198600769043\n",
      "E loss:  1.650660753250122\n",
      "G loss: 3.48163104057312\n",
      "E loss:  1.6702079772949219\n",
      "G loss: 3.5018198490142822\n",
      "E loss:  1.6473751068115234\n",
      "G loss: 3.5329294204711914\n",
      "E loss:  1.6368954181671143\n",
      "G loss: 3.5700840950012207\n",
      "Training Model  ...\n",
      "E loss:  1.4821807146072388\n",
      "G loss: 3.5767269134521484\n",
      "E loss:  1.4641157388687134\n",
      "G loss: 3.568371295928955\n",
      "E loss:  1.4653204679489136\n",
      "G loss: 3.577294111251831\n",
      "E loss:  1.4667317867279053\n",
      "G loss: 3.5771756172180176\n",
      "E loss:  1.4715890884399414\n",
      "G loss: 3.5715420246124268\n",
      "Training Model  ...\n",
      "E loss:  1.7183910608291626\n",
      "G loss: 3.571077823638916\n",
      "E loss:  1.6570253372192383\n",
      "G loss: 3.5610246658325195\n",
      "E loss:  1.6331747770309448\n",
      "G loss: 3.5575063228607178\n",
      "E loss:  1.6582111120224\n",
      "G loss: 3.5482614040374756\n",
      "E loss:  1.6164449453353882\n",
      "G loss: 3.542670965194702\n",
      "Training Model  ...\n",
      "E loss:  1.4641671180725098\n",
      "G loss: 3.5387063026428223\n",
      "E loss:  1.4605324268341064\n",
      "G loss: 3.5190815925598145\n",
      "E loss:  1.4271130561828613\n",
      "G loss: 3.50225830078125\n",
      "E loss:  1.417411208152771\n",
      "G loss: 3.48378849029541\n",
      "E loss:  1.4058457612991333\n",
      "G loss: 3.4506852626800537\n",
      "Training Model  ...\n",
      "E loss:  1.4344696998596191\n",
      "G loss: 3.4577128887176514\n",
      "E loss:  1.4586104154586792\n",
      "G loss: 3.451385498046875\n",
      "E loss:  1.4620925188064575\n",
      "G loss: 3.4566051959991455\n",
      "E loss:  1.4556567668914795\n",
      "G loss: 3.4581854343414307\n",
      "E loss:  1.4641971588134766\n",
      "G loss: 3.459625005722046\n",
      "Training Model  ...\n",
      "E loss:  1.5780613422393799\n",
      "G loss: 3.4665439128875732\n",
      "E loss:  1.5977611541748047\n",
      "G loss: 3.467365026473999\n",
      "E loss:  1.6065032482147217\n",
      "G loss: 3.461137533187866\n",
      "E loss:  1.6244105100631714\n",
      "G loss: 3.468094825744629\n",
      "E loss:  1.6056365966796875\n",
      "G loss: 3.4661061763763428\n",
      "Training Model  ...\n",
      "E loss:  1.6556363105773926\n",
      "G loss: 3.469548463821411\n",
      "E loss:  1.6481229066848755\n",
      "G loss: 3.4678049087524414\n",
      "E loss:  1.633864402770996\n",
      "G loss: 3.481840133666992\n",
      "E loss:  1.6452915668487549\n",
      "G loss: 3.492363214492798\n",
      "E loss:  1.6244902610778809\n",
      "G loss: 3.498918294906616\n",
      "Training Model  ...\n",
      "E loss:  1.6371654272079468\n",
      "G loss: 3.5055007934570312\n",
      "E loss:  1.6536484956741333\n",
      "G loss: 3.5076558589935303\n",
      "E loss:  1.6550599336624146\n",
      "G loss: 3.5154683589935303\n",
      "E loss:  1.6734848022460938\n",
      "G loss: 3.5192878246307373\n",
      "E loss:  1.6692397594451904\n",
      "G loss: 3.524592399597168\n",
      "Training Model  ...\n",
      "E loss:  1.6094489097595215\n",
      "G loss: 3.531383752822876\n",
      "E loss:  1.5790051221847534\n",
      "G loss: 3.531538963317871\n",
      "E loss:  1.5949804782867432\n",
      "G loss: 3.5344035625457764\n",
      "E loss:  1.6217701435089111\n",
      "G loss: 3.553828716278076\n",
      "E loss:  1.6210229396820068\n",
      "G loss: 3.560849905014038\n",
      "Training Model  ...\n",
      "E loss:  1.5720710754394531\n",
      "G loss: 3.5608654022216797\n",
      "E loss:  1.5674206018447876\n",
      "G loss: 3.5617775917053223\n",
      "E loss:  1.5913463830947876\n",
      "G loss: 3.5652883052825928\n",
      "E loss:  1.6119937896728516\n",
      "G loss: 3.570913076400757\n",
      "E loss:  1.5740814208984375\n",
      "G loss: 3.577251672744751\n",
      "Training Model  ...\n",
      "E loss:  1.6357868909835815\n",
      "G loss: 3.58516263961792\n",
      "E loss:  1.6152222156524658\n",
      "G loss: 3.5848989486694336\n",
      "E loss:  1.5880426168441772\n",
      "G loss: 3.6057987213134766\n",
      "E loss:  1.616363286972046\n",
      "G loss: 3.624311923980713\n",
      "E loss:  1.5958460569381714\n",
      "G loss: 3.6404218673706055\n",
      "Training Model  ...\n",
      "E loss:  1.617175817489624\n",
      "G loss: 3.645395278930664\n",
      "E loss:  1.6493244171142578\n",
      "G loss: 3.6561431884765625\n",
      "E loss:  1.65437912940979\n",
      "G loss: 3.6749165058135986\n",
      "E loss:  1.6894276142120361\n",
      "G loss: 3.6911063194274902\n",
      "E loss:  1.6972501277923584\n",
      "G loss: 3.71639347076416\n",
      "Training Model  ...\n",
      "E loss:  1.6339796781539917\n",
      "G loss: 3.710923194885254\n",
      "E loss:  1.5986206531524658\n",
      "G loss: 3.700833559036255\n",
      "E loss:  1.5567752122879028\n",
      "G loss: 3.695277214050293\n",
      "E loss:  1.5861012935638428\n",
      "G loss: 3.6811206340789795\n",
      "E loss:  1.6011425256729126\n",
      "G loss: 3.6671879291534424\n",
      "Training Model  ...\n",
      "E loss:  1.6065654754638672\n",
      "G loss: 3.666444778442383\n",
      "E loss:  1.5943838357925415\n",
      "G loss: 3.6601548194885254\n",
      "E loss:  1.5891518592834473\n",
      "G loss: 3.6310625076293945\n",
      "E loss:  1.590363621711731\n",
      "G loss: 3.61163330078125\n",
      "E loss:  1.609845519065857\n",
      "G loss: 3.581604242324829\n",
      "Training Model  ...\n",
      "E loss:  1.4914944171905518\n",
      "G loss: 3.5765390396118164\n",
      "E loss:  1.496711254119873\n",
      "G loss: 3.5776476860046387\n",
      "E loss:  1.4942843914031982\n",
      "G loss: 3.5817556381225586\n",
      "E loss:  1.5256882905960083\n",
      "G loss: 3.590130090713501\n",
      "E loss:  1.5380189418792725\n",
      "G loss: 3.5940794944763184\n",
      "Training Model  ...\n",
      "E loss:  1.7474910020828247\n",
      "G loss: 3.587296724319458\n",
      "E loss:  1.732925295829773\n",
      "G loss: 3.581028461456299\n",
      "E loss:  1.6978460550308228\n",
      "G loss: 3.577425003051758\n",
      "E loss:  1.7131071090698242\n",
      "G loss: 3.5645525455474854\n",
      "E loss:  1.7179076671600342\n",
      "G loss: 3.5462498664855957\n",
      "Training Model  ...\n",
      "E loss:  1.5810790061950684\n",
      "G loss: 3.5568838119506836\n",
      "E loss:  1.5438458919525146\n",
      "G loss: 3.5413246154785156\n",
      "E loss:  1.5222185850143433\n",
      "G loss: 3.5385851860046387\n",
      "E loss:  1.4904056787490845\n",
      "G loss: 3.5375585556030273\n",
      "E loss:  1.4953863620758057\n",
      "G loss: 3.530088186264038\n",
      "Training Model  ...\n",
      "E loss:  1.648898959159851\n",
      "G loss: 3.5291619300842285\n",
      "E loss:  1.6649768352508545\n",
      "G loss: 3.5268633365631104\n",
      "E loss:  1.647150993347168\n",
      "G loss: 3.5094521045684814\n",
      "E loss:  1.6551871299743652\n",
      "G loss: 3.4935150146484375\n",
      "E loss:  1.6392276287078857\n",
      "G loss: 3.479944944381714\n",
      "Training Model  ...\n",
      "E loss:  1.7156018018722534\n",
      "G loss: 3.4851107597351074\n",
      "E loss:  1.6998744010925293\n",
      "G loss: 3.4884417057037354\n",
      "E loss:  1.681545376777649\n",
      "G loss: 3.5089125633239746\n",
      "E loss:  1.6826632022857666\n",
      "G loss: 3.526045083999634\n",
      "E loss:  1.6603156328201294\n",
      "G loss: 3.5408408641815186\n",
      "Training Model  ...\n",
      "E loss:  1.6237738132476807\n",
      "G loss: 3.5451302528381348\n",
      "E loss:  1.5895808935165405\n",
      "G loss: 3.5560462474823\n",
      "E loss:  1.611542820930481\n",
      "G loss: 3.5903427600860596\n",
      "E loss:  1.6138074398040771\n",
      "G loss: 3.593047618865967\n",
      "E loss:  1.6496506929397583\n",
      "G loss: 3.6117687225341797\n",
      "Training Model  ...\n",
      "E loss:  1.6607308387756348\n",
      "G loss: 3.5888559818267822\n",
      "E loss:  1.6389515399932861\n",
      "G loss: 3.5749993324279785\n",
      "E loss:  1.6166965961456299\n",
      "G loss: 3.5482194423675537\n",
      "E loss:  1.6039739847183228\n",
      "G loss: 3.5133278369903564\n",
      "E loss:  1.5851737260818481\n",
      "G loss: 3.4780497550964355\n",
      "Training Model  ...\n",
      "E loss:  1.6921095848083496\n",
      "G loss: 3.4758105278015137\n",
      "E loss:  1.661899209022522\n",
      "G loss: 3.484001636505127\n",
      "E loss:  1.631028413772583\n",
      "G loss: 3.4916810989379883\n",
      "E loss:  1.626212239265442\n",
      "G loss: 3.5166735649108887\n",
      "E loss:  1.6478254795074463\n",
      "G loss: 3.5265581607818604\n",
      "Training Model  ...\n",
      "E loss:  1.5193421840667725\n",
      "G loss: 3.5173420906066895\n",
      "E loss:  1.5020018815994263\n",
      "G loss: 3.509951591491699\n",
      "E loss:  1.5117005109786987\n",
      "G loss: 3.5061113834381104\n",
      "E loss:  1.51090407371521\n",
      "G loss: 3.4848313331604004\n",
      "E loss:  1.543565034866333\n",
      "G loss: 3.4685380458831787\n",
      "Training Model  ...\n",
      "E loss:  1.5814228057861328\n",
      "G loss: 3.4591565132141113\n",
      "E loss:  1.5908297300338745\n",
      "G loss: 3.4576897621154785\n",
      "E loss:  1.6106908321380615\n",
      "G loss: 3.443211317062378\n",
      "E loss:  1.6333290338516235\n",
      "G loss: 3.441796064376831\n",
      "E loss:  1.6635305881500244\n",
      "G loss: 3.424466371536255\n",
      "Training Model  ...\n",
      "E loss:  1.7537344694137573\n",
      "G loss: 3.424643039703369\n",
      "E loss:  1.7333016395568848\n",
      "G loss: 3.43363618850708\n",
      "E loss:  1.7070322036743164\n",
      "G loss: 3.4398837089538574\n",
      "E loss:  1.7236905097961426\n",
      "G loss: 3.440581798553467\n",
      "E loss:  1.6986782550811768\n",
      "G loss: 3.4586877822875977\n",
      "Training Model  ...\n",
      "E loss:  1.6085259914398193\n",
      "G loss: 3.4355101585388184\n",
      "E loss:  1.6110347509384155\n",
      "G loss: 3.442551612854004\n",
      "E loss:  1.6107488870620728\n",
      "G loss: 3.4409360885620117\n",
      "E loss:  1.6088833808898926\n",
      "G loss: 3.4182186126708984\n",
      "E loss:  1.553511381149292\n",
      "G loss: 3.4061288833618164\n",
      "Training Model  ...\n",
      "E loss:  1.4802405834197998\n",
      "G loss: 3.4078304767608643\n",
      "E loss:  1.5317736864089966\n",
      "G loss: 3.4027090072631836\n",
      "E loss:  1.513742446899414\n",
      "G loss: 3.400817394256592\n",
      "E loss:  1.509689450263977\n",
      "G loss: 3.393361806869507\n",
      "E loss:  1.4770859479904175\n",
      "G loss: 3.3917770385742188\n",
      "Training Model  ...\n",
      "E loss:  1.500244379043579\n",
      "G loss: 3.3895926475524902\n",
      "E loss:  1.4617735147476196\n",
      "G loss: 3.3861541748046875\n",
      "E loss:  1.4838120937347412\n",
      "G loss: 3.383833646774292\n",
      "E loss:  1.459783911705017\n",
      "G loss: 3.3737926483154297\n",
      "E loss:  1.45595383644104\n",
      "G loss: 3.3728654384613037\n",
      "Training Model  ...\n",
      "E loss:  1.6984833478927612\n",
      "G loss: 3.3784732818603516\n",
      "E loss:  1.6478683948516846\n",
      "G loss: 3.383296012878418\n",
      "E loss:  1.6615926027297974\n",
      "G loss: 3.406479597091675\n",
      "E loss:  1.6506189107894897\n",
      "G loss: 3.420111894607544\n",
      "E loss:  1.634770154953003\n",
      "G loss: 3.4489922523498535\n",
      "Training Model  ...\n",
      "E loss:  1.7250175476074219\n",
      "G loss: 3.436260223388672\n",
      "E loss:  1.7299469709396362\n",
      "G loss: 3.441204071044922\n",
      "E loss:  1.7075095176696777\n",
      "G loss: 3.4321298599243164\n",
      "E loss:  1.6914827823638916\n",
      "G loss: 3.426917791366577\n",
      "E loss:  1.675619125366211\n",
      "G loss: 3.4109244346618652\n",
      "Training Model  ...\n",
      "E loss:  1.618384838104248\n",
      "G loss: 3.413761615753174\n",
      "E loss:  1.6324434280395508\n",
      "G loss: 3.4029436111450195\n",
      "E loss:  1.6142951250076294\n",
      "G loss: 3.3889546394348145\n",
      "E loss:  1.6040395498275757\n",
      "G loss: 3.3899054527282715\n",
      "E loss:  1.5859016180038452\n",
      "G loss: 3.371291160583496\n",
      "Training Model  ...\n",
      "E loss:  1.7052509784698486\n",
      "G loss: 3.377631187438965\n",
      "E loss:  1.6958675384521484\n",
      "G loss: 3.374892473220825\n",
      "E loss:  1.7070293426513672\n",
      "G loss: 3.3779609203338623\n",
      "E loss:  1.6973543167114258\n",
      "G loss: 3.3661794662475586\n",
      "E loss:  1.6661678552627563\n",
      "G loss: 3.3639421463012695\n",
      "Training Model  ...\n",
      "E loss:  1.36073899269104\n",
      "G loss: 3.3679206371307373\n",
      "E loss:  1.3778023719787598\n",
      "G loss: 3.3682031631469727\n",
      "E loss:  1.4085314273834229\n",
      "G loss: 3.378636598587036\n",
      "E loss:  1.4159443378448486\n",
      "G loss: 3.3889341354370117\n",
      "E loss:  1.4417160749435425\n",
      "G loss: 3.3967342376708984\n",
      "Training Model  ...\n",
      "E loss:  1.4830708503723145\n",
      "G loss: 3.385120153427124\n",
      "E loss:  1.5057631731033325\n",
      "G loss: 3.3756155967712402\n",
      "E loss:  1.516004204750061\n",
      "G loss: 3.363466501235962\n",
      "E loss:  1.5024218559265137\n",
      "G loss: 3.334264039993286\n",
      "E loss:  1.4987294673919678\n",
      "G loss: 3.322432041168213\n",
      "Training Model  ...\n",
      "E loss:  1.5130817890167236\n",
      "G loss: 3.328904867172241\n",
      "E loss:  1.5267019271850586\n",
      "G loss: 3.342186450958252\n",
      "E loss:  1.5543030500411987\n",
      "G loss: 3.3517422676086426\n",
      "E loss:  1.5930132865905762\n",
      "G loss: 3.363553047180176\n",
      "E loss:  1.551011085510254\n",
      "G loss: 3.379546880722046\n",
      "Training Model  ...\n",
      "E loss:  1.5637887716293335\n",
      "G loss: 3.3801538944244385\n",
      "E loss:  1.572834849357605\n",
      "G loss: 3.3774333000183105\n",
      "E loss:  1.5777441263198853\n",
      "G loss: 3.376819372177124\n",
      "E loss:  1.5849413871765137\n",
      "G loss: 3.3765735626220703\n",
      "E loss:  1.5871367454528809\n",
      "G loss: 3.364544153213501\n",
      "Training Model  ...\n",
      "E loss:  1.6191297769546509\n",
      "G loss: 3.3476674556732178\n",
      "E loss:  1.6132500171661377\n",
      "G loss: 3.327363967895508\n",
      "E loss:  1.5841569900512695\n",
      "G loss: 3.31217622756958\n",
      "E loss:  1.5980337858200073\n",
      "G loss: 3.278132915496826\n",
      "E loss:  1.6320278644561768\n",
      "G loss: 3.241654872894287\n",
      "Training Model  ...\n",
      "E loss:  1.499247431755066\n",
      "G loss: 3.247054100036621\n",
      "E loss:  1.4908801317214966\n",
      "G loss: 3.25502347946167\n",
      "E loss:  1.4887933731079102\n",
      "G loss: 3.2478010654449463\n",
      "E loss:  1.507814645767212\n",
      "G loss: 3.2558369636535645\n",
      "E loss:  1.4923079013824463\n",
      "G loss: 3.25468111038208\n",
      "Training Model  ...\n",
      "E loss:  1.6921039819717407\n",
      "G loss: 3.260230302810669\n",
      "E loss:  1.6955416202545166\n",
      "G loss: 3.2699332237243652\n",
      "E loss:  1.6788980960845947\n",
      "G loss: 3.2635796070098877\n",
      "E loss:  1.6674904823303223\n",
      "G loss: 3.26914644241333\n",
      "E loss:  1.6637800931930542\n",
      "G loss: 3.273143768310547\n",
      "Training Model  ...\n",
      "E loss:  1.5628058910369873\n",
      "G loss: 3.2737557888031006\n",
      "E loss:  1.584741234779358\n",
      "G loss: 3.260645866394043\n",
      "E loss:  1.5607444047927856\n",
      "G loss: 3.2668697834014893\n",
      "E loss:  1.5743733644485474\n",
      "G loss: 3.266390562057495\n",
      "E loss:  1.5938620567321777\n",
      "G loss: 3.2615644931793213\n",
      "Training Model  ...\n",
      "E loss:  1.4742978811264038\n",
      "G loss: 3.266418933868408\n",
      "E loss:  1.485863447189331\n",
      "G loss: 3.2653274536132812\n",
      "E loss:  1.4874329566955566\n",
      "G loss: 3.2819314002990723\n",
      "E loss:  1.4818665981292725\n",
      "G loss: 3.2911038398742676\n",
      "E loss:  1.468575358390808\n",
      "G loss: 3.3080906867980957\n",
      "Training Model  ...\n",
      "E loss:  1.5953083038330078\n",
      "G loss: 3.302982807159424\n",
      "E loss:  1.559433937072754\n",
      "G loss: 3.2951385974884033\n",
      "E loss:  1.5778294801712036\n",
      "G loss: 3.3049302101135254\n",
      "E loss:  1.564854383468628\n",
      "G loss: 3.294327974319458\n",
      "E loss:  1.5602457523345947\n",
      "G loss: 3.2942631244659424\n",
      "Training Model  ...\n",
      "E loss:  1.4831717014312744\n",
      "G loss: 3.3024303913116455\n",
      "E loss:  1.4881691932678223\n",
      "G loss: 3.312908887863159\n",
      "E loss:  1.4851726293563843\n",
      "G loss: 3.321671724319458\n",
      "E loss:  1.4622524976730347\n",
      "G loss: 3.3381547927856445\n",
      "E loss:  1.4827651977539062\n",
      "G loss: 3.362344741821289\n",
      "Training Model  ...\n",
      "E loss:  1.373630166053772\n",
      "G loss: 3.3484086990356445\n",
      "E loss:  1.3956221342086792\n",
      "G loss: 3.3462536334991455\n",
      "E loss:  1.41958487033844\n",
      "G loss: 3.3320209980010986\n",
      "E loss:  1.428877592086792\n",
      "G loss: 3.318948268890381\n",
      "E loss:  1.415769338607788\n",
      "G loss: 3.2961254119873047\n",
      "Training Model  ...\n",
      "E loss:  1.5427918434143066\n",
      "G loss: 3.3115193843841553\n",
      "E loss:  1.4987765550613403\n",
      "G loss: 3.3245763778686523\n",
      "E loss:  1.5033373832702637\n",
      "G loss: 3.3538012504577637\n",
      "E loss:  1.5206921100616455\n",
      "G loss: 3.3825364112854004\n",
      "E loss:  1.5353071689605713\n",
      "G loss: 3.417085647583008\n",
      "Training Model  ...\n",
      "E loss:  1.5183331966400146\n",
      "G loss: 3.417320489883423\n",
      "E loss:  1.5493788719177246\n",
      "G loss: 3.419795036315918\n",
      "E loss:  1.5753443241119385\n",
      "G loss: 3.414762020111084\n",
      "E loss:  1.5573369264602661\n",
      "G loss: 3.406848430633545\n",
      "E loss:  1.542447805404663\n",
      "G loss: 3.4083714485168457\n",
      "Training Model  ...\n",
      "E loss:  1.6199414730072021\n",
      "G loss: 3.3951973915100098\n",
      "E loss:  1.6274337768554688\n",
      "G loss: 3.379307270050049\n",
      "E loss:  1.6154685020446777\n",
      "G loss: 3.3660454750061035\n",
      "E loss:  1.6287246942520142\n",
      "G loss: 3.332988977432251\n",
      "E loss:  1.6121923923492432\n",
      "G loss: 3.302499532699585\n",
      "Training Model  ...\n",
      "E loss:  1.4996168613433838\n",
      "G loss: 3.298027992248535\n",
      "E loss:  1.5013550519943237\n",
      "G loss: 3.2959699630737305\n",
      "E loss:  1.5519816875457764\n",
      "G loss: 3.2888853549957275\n",
      "E loss:  1.5610460042953491\n",
      "G loss: 3.2909398078918457\n",
      "E loss:  1.5746397972106934\n",
      "G loss: 3.295813798904419\n",
      "Training Model  ...\n",
      "E loss:  1.3758958578109741\n",
      "G loss: 3.279874086380005\n",
      "E loss:  1.366501808166504\n",
      "G loss: 3.2810282707214355\n",
      "E loss:  1.391541600227356\n",
      "G loss: 3.2748913764953613\n",
      "E loss:  1.3665388822555542\n",
      "G loss: 3.247013568878174\n",
      "E loss:  1.3611032962799072\n",
      "G loss: 3.223896026611328\n",
      "Training Model  ...\n",
      "E loss:  1.617295503616333\n",
      "G loss: 3.228203773498535\n",
      "E loss:  1.5927399396896362\n",
      "G loss: 3.2168126106262207\n",
      "E loss:  1.5687484741210938\n",
      "G loss: 3.2127065658569336\n",
      "E loss:  1.5793365240097046\n",
      "G loss: 3.1983537673950195\n",
      "E loss:  1.5868333578109741\n",
      "G loss: 3.178865432739258\n",
      "Training Model  ...\n",
      "E loss:  1.7179912328720093\n",
      "G loss: 3.1884803771972656\n",
      "E loss:  1.7113341093063354\n",
      "G loss: 3.1984846591949463\n",
      "E loss:  1.6991660594940186\n",
      "G loss: 3.228539228439331\n",
      "E loss:  1.709518551826477\n",
      "G loss: 3.2542874813079834\n",
      "E loss:  1.6847960948944092\n",
      "G loss: 3.2930591106414795\n",
      "Training Model  ...\n",
      "E loss:  1.5331518650054932\n",
      "G loss: 3.2970643043518066\n",
      "E loss:  1.5429632663726807\n",
      "G loss: 3.2927582263946533\n",
      "E loss:  1.5420000553131104\n",
      "G loss: 3.301647663116455\n",
      "E loss:  1.5360091924667358\n",
      "G loss: 3.312776803970337\n",
      "E loss:  1.539255976676941\n",
      "G loss: 3.313180923461914\n",
      "Training Model  ...\n",
      "E loss:  1.4956711530685425\n",
      "G loss: 3.302088975906372\n",
      "E loss:  1.4684669971466064\n",
      "G loss: 3.2915637493133545\n",
      "E loss:  1.4672211408615112\n",
      "G loss: 3.290692090988159\n",
      "E loss:  1.4752980470657349\n",
      "G loss: 3.2744126319885254\n",
      "E loss:  1.485243797302246\n",
      "G loss: 3.229459047317505\n",
      "Training Model  ...\n",
      "E loss:  1.6925010681152344\n",
      "G loss: 3.242619514465332\n",
      "E loss:  1.7340986728668213\n",
      "G loss: 3.2560477256774902\n",
      "E loss:  1.7493181228637695\n",
      "G loss: 3.267547130584717\n",
      "E loss:  1.7438743114471436\n",
      "G loss: 3.28645396232605\n",
      "E loss:  1.74449622631073\n",
      "G loss: 3.3044626712799072\n",
      "Training Model  ...\n",
      "E loss:  1.4983060359954834\n",
      "G loss: 3.2904293537139893\n",
      "E loss:  1.501718282699585\n",
      "G loss: 3.284924030303955\n",
      "E loss:  1.5011496543884277\n",
      "G loss: 3.2842013835906982\n",
      "E loss:  1.497135043144226\n",
      "G loss: 3.280028820037842\n",
      "E loss:  1.4970688819885254\n",
      "G loss: 3.2806179523468018\n",
      "Training Model  ...\n",
      "E loss:  1.5094949007034302\n",
      "G loss: 3.280425786972046\n",
      "E loss:  1.499324917793274\n",
      "G loss: 3.279893398284912\n",
      "E loss:  1.496392011642456\n",
      "G loss: 3.28361177444458\n",
      "E loss:  1.4860552549362183\n",
      "G loss: 3.2869441509246826\n",
      "E loss:  1.4896730184555054\n",
      "G loss: 3.30237078666687\n",
      "Training Model  ...\n",
      "E loss:  1.7532850503921509\n",
      "G loss: 3.3037238121032715\n",
      "E loss:  1.7273513078689575\n",
      "G loss: 3.312169313430786\n",
      "E loss:  1.732253074645996\n",
      "G loss: 3.327500104904175\n",
      "E loss:  1.6901246309280396\n",
      "G loss: 3.3481285572052\n",
      "E loss:  1.7095404863357544\n",
      "G loss: 3.361246347427368\n",
      "Training Model  ...\n",
      "E loss:  1.6289942264556885\n",
      "G loss: 3.3591063022613525\n",
      "E loss:  1.6792999505996704\n",
      "G loss: 3.350694179534912\n",
      "E loss:  1.6369107961654663\n",
      "G loss: 3.3444747924804688\n",
      "E loss:  1.6090425252914429\n",
      "G loss: 3.3297243118286133\n",
      "E loss:  1.6210788488388062\n",
      "G loss: 3.322070598602295\n",
      "Training Model  ...\n",
      "E loss:  1.5534611940383911\n",
      "G loss: 3.312217950820923\n",
      "E loss:  1.5264540910720825\n",
      "G loss: 3.300614356994629\n",
      "E loss:  1.531020164489746\n",
      "G loss: 3.2952003479003906\n",
      "E loss:  1.5142232179641724\n",
      "G loss: 3.2854771614074707\n",
      "E loss:  1.5332257747650146\n",
      "G loss: 3.2790911197662354\n",
      "Training Model  ...\n",
      "E loss:  1.7246906757354736\n",
      "G loss: 3.2817835807800293\n",
      "E loss:  1.7006404399871826\n",
      "G loss: 3.277679443359375\n",
      "E loss:  1.715080738067627\n",
      "G loss: 3.2821695804595947\n",
      "E loss:  1.716352105140686\n",
      "G loss: 3.2749035358428955\n",
      "E loss:  1.7534793615341187\n",
      "G loss: 3.2806010246276855\n",
      "Training Model  ...\n",
      "E loss:  1.5034825801849365\n",
      "G loss: 3.27689790725708\n",
      "E loss:  1.4943691492080688\n",
      "G loss: 3.2766568660736084\n",
      "E loss:  1.5237482786178589\n",
      "G loss: 3.273460865020752\n",
      "E loss:  1.5273964405059814\n",
      "G loss: 3.263061761856079\n",
      "E loss:  1.5249704122543335\n",
      "G loss: 3.267303466796875\n",
      "Training Model  ...\n",
      "E loss:  1.631943702697754\n",
      "G loss: 3.2614641189575195\n",
      "E loss:  1.61872136592865\n",
      "G loss: 3.253875970840454\n",
      "E loss:  1.6168417930603027\n",
      "G loss: 3.259293794631958\n",
      "E loss:  1.6557624340057373\n",
      "G loss: 3.2451682090759277\n",
      "E loss:  1.6629263162612915\n",
      "G loss: 3.229901075363159\n",
      "Training Model  ...\n",
      "E loss:  1.5256551504135132\n",
      "G loss: 3.2312772274017334\n",
      "E loss:  1.5141265392303467\n",
      "G loss: 3.2309420108795166\n",
      "E loss:  1.5527914762496948\n",
      "G loss: 3.244307041168213\n",
      "E loss:  1.5237878561019897\n",
      "G loss: 3.2558770179748535\n",
      "E loss:  1.5405611991882324\n",
      "G loss: 3.2592387199401855\n",
      "Training Model  ...\n",
      "E loss:  1.5781229734420776\n",
      "G loss: 3.2563445568084717\n",
      "E loss:  1.5924217700958252\n",
      "G loss: 3.2478747367858887\n",
      "E loss:  1.6135530471801758\n",
      "G loss: 3.2326931953430176\n",
      "E loss:  1.5985521078109741\n",
      "G loss: 3.220705986022949\n",
      "E loss:  1.6125202178955078\n",
      "G loss: 3.2121551036834717\n",
      "Training Model  ...\n",
      "E loss:  1.6144131422042847\n",
      "G loss: 3.20247483253479\n",
      "E loss:  1.6014726161956787\n",
      "G loss: 3.1946959495544434\n",
      "E loss:  1.5784385204315186\n",
      "G loss: 3.1675806045532227\n",
      "E loss:  1.572993516921997\n",
      "G loss: 3.158007860183716\n",
      "E loss:  1.5777822732925415\n",
      "G loss: 3.1298751831054688\n",
      "Training Model  ...\n",
      "E loss:  1.705683946609497\n",
      "G loss: 3.134143352508545\n",
      "E loss:  1.6853809356689453\n",
      "G loss: 3.1401050090789795\n",
      "E loss:  1.6832491159439087\n",
      "G loss: 3.140357732772827\n",
      "E loss:  1.683754563331604\n",
      "G loss: 3.1367368698120117\n",
      "E loss:  1.658958077430725\n",
      "G loss: 3.145379066467285\n",
      "Training Model  ...\n",
      "E loss:  1.7301626205444336\n",
      "G loss: 3.140214681625366\n",
      "E loss:  1.72538423538208\n",
      "G loss: 3.141162157058716\n",
      "E loss:  1.7357888221740723\n",
      "G loss: 3.1521923542022705\n",
      "E loss:  1.6963355541229248\n",
      "G loss: 3.157938003540039\n",
      "E loss:  1.708160638809204\n",
      "G loss: 3.160235643386841\n",
      "Training Model  ...\n",
      "E loss:  1.596283197402954\n",
      "G loss: 3.159968614578247\n",
      "E loss:  1.604017734527588\n",
      "G loss: 3.158562421798706\n",
      "E loss:  1.605607271194458\n",
      "G loss: 3.156768798828125\n",
      "E loss:  1.5910365581512451\n",
      "G loss: 3.134403705596924\n",
      "E loss:  1.5896637439727783\n",
      "G loss: 3.129767894744873\n",
      "Training Model  ...\n",
      "E loss:  1.536367654800415\n",
      "G loss: 3.1379640102386475\n",
      "E loss:  1.5321084260940552\n",
      "G loss: 3.1352968215942383\n",
      "E loss:  1.505486011505127\n",
      "G loss: 3.1559815406799316\n",
      "E loss:  1.520330548286438\n",
      "G loss: 3.161044120788574\n",
      "E loss:  1.5306336879730225\n",
      "G loss: 3.18744158744812\n",
      "Training Model  ...\n",
      "E loss:  1.6786351203918457\n",
      "G loss: 3.1912341117858887\n",
      "E loss:  1.7049421072006226\n",
      "G loss: 3.2007253170013428\n",
      "E loss:  1.689551830291748\n",
      "G loss: 3.199718713760376\n",
      "E loss:  1.6726258993148804\n",
      "G loss: 3.221644878387451\n",
      "E loss:  1.6671571731567383\n",
      "G loss: 3.230858087539673\n",
      "Training Model  ...\n",
      "E loss:  1.7345143556594849\n",
      "G loss: 3.224569082260132\n",
      "E loss:  1.6786314249038696\n",
      "G loss: 3.2326087951660156\n",
      "E loss:  1.6751790046691895\n",
      "G loss: 3.2446341514587402\n",
      "E loss:  1.6858296394348145\n",
      "G loss: 3.2591142654418945\n",
      "E loss:  1.6886018514633179\n",
      "G loss: 3.246786594390869\n",
      "Training Model  ...\n",
      "E loss:  1.6361712217330933\n",
      "G loss: 3.2466118335723877\n",
      "E loss:  1.6469258069992065\n",
      "G loss: 3.237776279449463\n",
      "E loss:  1.664955973625183\n",
      "G loss: 3.2235069274902344\n",
      "E loss:  1.6560478210449219\n",
      "G loss: 3.210629940032959\n",
      "E loss:  1.6695046424865723\n",
      "G loss: 3.1899075508117676\n",
      "Training Model  ...\n",
      "E loss:  1.6126701831817627\n",
      "G loss: 3.187566041946411\n",
      "E loss:  1.6285852193832397\n",
      "G loss: 3.1842451095581055\n",
      "E loss:  1.6584492921829224\n",
      "G loss: 3.173954963684082\n",
      "E loss:  1.6479878425598145\n",
      "G loss: 3.155346155166626\n",
      "E loss:  1.6045289039611816\n",
      "G loss: 3.1417837142944336\n",
      "Training Model  ...\n",
      "E loss:  1.535618543624878\n",
      "G loss: 3.139633893966675\n",
      "E loss:  1.5231525897979736\n",
      "G loss: 3.124101400375366\n",
      "E loss:  1.5095213651657104\n",
      "G loss: 3.1367874145507812\n",
      "E loss:  1.5128026008605957\n",
      "G loss: 3.1491756439208984\n",
      "E loss:  1.5210621356964111\n",
      "G loss: 3.141345977783203\n",
      "Training Model  ...\n",
      "E loss:  1.5840425491333008\n",
      "G loss: 3.1532533168792725\n",
      "E loss:  1.6008877754211426\n",
      "G loss: 3.1465563774108887\n",
      "E loss:  1.5973944664001465\n",
      "G loss: 3.1613874435424805\n",
      "E loss:  1.5575852394104004\n",
      "G loss: 3.1611509323120117\n",
      "E loss:  1.558282494544983\n",
      "G loss: 3.173877716064453\n",
      "Training Model  ...\n",
      "E loss:  1.579254150390625\n",
      "G loss: 3.1682798862457275\n",
      "E loss:  1.596927523612976\n",
      "G loss: 3.1654868125915527\n",
      "E loss:  1.5914698839187622\n",
      "G loss: 3.172398567199707\n",
      "E loss:  1.5680595636367798\n",
      "G loss: 3.16536283493042\n",
      "E loss:  1.5306930541992188\n",
      "G loss: 3.1655304431915283\n",
      "Training Model  ...\n",
      "E loss:  1.5902706384658813\n",
      "G loss: 3.169747829437256\n",
      "E loss:  1.5815497636795044\n",
      "G loss: 3.197808265686035\n",
      "E loss:  1.5665801763534546\n",
      "G loss: 3.203678846359253\n",
      "E loss:  1.5804229974746704\n",
      "G loss: 3.220522880554199\n",
      "E loss:  1.5665669441223145\n",
      "G loss: 3.243206262588501\n",
      "Training Model  ...\n",
      "E loss:  1.6086374521255493\n",
      "G loss: 3.24412202835083\n",
      "E loss:  1.5996761322021484\n",
      "G loss: 3.2311320304870605\n",
      "E loss:  1.6184909343719482\n",
      "G loss: 3.226710319519043\n",
      "E loss:  1.585181713104248\n",
      "G loss: 3.2103991508483887\n",
      "E loss:  1.5856130123138428\n",
      "G loss: 3.2134897708892822\n",
      "Training Model  ...\n",
      "E loss:  1.4999494552612305\n",
      "G loss: 3.205319881439209\n",
      "E loss:  1.4934602975845337\n",
      "G loss: 3.2055470943450928\n",
      "E loss:  1.4944798946380615\n",
      "G loss: 3.2190470695495605\n",
      "E loss:  1.484472393989563\n",
      "G loss: 3.233795642852783\n",
      "E loss:  1.4787030220031738\n",
      "G loss: 3.234464406967163\n",
      "Training Model  ...\n",
      "E loss:  1.7758641242980957\n",
      "G loss: 3.247528076171875\n",
      "E loss:  1.756257176399231\n",
      "G loss: 3.257359266281128\n",
      "E loss:  1.7293339967727661\n",
      "G loss: 3.261892557144165\n",
      "E loss:  1.7218656539916992\n",
      "G loss: 3.2783069610595703\n",
      "E loss:  1.7209872007369995\n",
      "G loss: 3.299328327178955\n",
      "Training Model  ...\n",
      "E loss:  1.5488752126693726\n",
      "G loss: 3.290786027908325\n",
      "E loss:  1.5297422409057617\n",
      "G loss: 3.282099962234497\n",
      "E loss:  1.5360504388809204\n",
      "G loss: 3.2597126960754395\n",
      "E loss:  1.5120353698730469\n",
      "G loss: 3.2246859073638916\n",
      "E loss:  1.4995152950286865\n",
      "G loss: 3.202939510345459\n",
      "Training Model  ...\n",
      "E loss:  1.2589404582977295\n",
      "G loss: 3.205049991607666\n",
      "E loss:  1.2899630069732666\n",
      "G loss: 3.2010140419006348\n",
      "E loss:  1.309655785560608\n",
      "G loss: 3.1768932342529297\n",
      "E loss:  1.3330222368240356\n",
      "G loss: 3.1629180908203125\n",
      "E loss:  1.3269749879837036\n",
      "G loss: 3.1492536067962646\n",
      "Training Model  ...\n",
      "E loss:  1.622070074081421\n",
      "G loss: 3.157188892364502\n",
      "E loss:  1.6126558780670166\n",
      "G loss: 3.137214183807373\n",
      "E loss:  1.6312545537948608\n",
      "G loss: 3.1271767616271973\n",
      "E loss:  1.6199747323989868\n",
      "G loss: 3.123504400253296\n",
      "E loss:  1.6137911081314087\n",
      "G loss: 3.1148159503936768\n",
      "Training Model  ...\n",
      "E loss:  1.4239925146102905\n",
      "G loss: 3.1138739585876465\n",
      "E loss:  1.4201228618621826\n",
      "G loss: 3.1088225841522217\n",
      "E loss:  1.429297685623169\n",
      "G loss: 3.1248960494995117\n",
      "E loss:  1.434618353843689\n",
      "G loss: 3.133033514022827\n",
      "E loss:  1.473061203956604\n",
      "G loss: 3.1533524990081787\n",
      "Training Model  ...\n",
      "E loss:  1.4365051984786987\n",
      "G loss: 3.141540050506592\n",
      "E loss:  1.4943504333496094\n",
      "G loss: 3.1470344066619873\n",
      "E loss:  1.5058889389038086\n",
      "G loss: 3.1572248935699463\n",
      "E loss:  1.4730334281921387\n",
      "G loss: 3.1640982627868652\n",
      "E loss:  1.4768409729003906\n",
      "G loss: 3.1809639930725098\n",
      "Training Model  ...\n",
      "E loss:  1.5084465742111206\n",
      "G loss: 3.1633832454681396\n",
      "E loss:  1.4949663877487183\n",
      "G loss: 3.151719093322754\n",
      "E loss:  1.4932457208633423\n",
      "G loss: 3.1309287548065186\n",
      "E loss:  1.5008597373962402\n",
      "G loss: 3.114917278289795\n",
      "E loss:  1.5061326026916504\n",
      "G loss: 3.100534200668335\n",
      "Training Model  ...\n",
      "E loss:  1.5004656314849854\n",
      "G loss: 3.087411880493164\n",
      "E loss:  1.5176399946212769\n",
      "G loss: 3.080799102783203\n",
      "E loss:  1.4896423816680908\n",
      "G loss: 3.0647852420806885\n",
      "E loss:  1.5189106464385986\n",
      "G loss: 3.0547659397125244\n",
      "E loss:  1.4852582216262817\n",
      "G loss: 3.0337862968444824\n",
      "Training Model  ...\n",
      "E loss:  1.6321899890899658\n",
      "G loss: 3.050046443939209\n",
      "E loss:  1.6046876907348633\n",
      "G loss: 3.073286533355713\n",
      "E loss:  1.5934226512908936\n",
      "G loss: 3.115067720413208\n",
      "E loss:  1.5861588716506958\n",
      "G loss: 3.144665002822876\n",
      "E loss:  1.5740599632263184\n",
      "G loss: 3.1952996253967285\n",
      "Training Model  ...\n",
      "E loss:  1.7267910242080688\n",
      "G loss: 3.188654899597168\n",
      "E loss:  1.6993986368179321\n",
      "G loss: 3.1829450130462646\n",
      "E loss:  1.674854040145874\n",
      "G loss: 3.187915325164795\n",
      "E loss:  1.695419430732727\n",
      "G loss: 3.1820197105407715\n",
      "E loss:  1.6858346462249756\n",
      "G loss: 3.1622328758239746\n",
      "Training Model  ...\n",
      "E loss:  1.5817614793777466\n",
      "G loss: 3.1757748126983643\n",
      "E loss:  1.5920650959014893\n",
      "G loss: 3.171903133392334\n",
      "E loss:  1.5697407722473145\n",
      "G loss: 3.1945488452911377\n",
      "E loss:  1.597778081893921\n",
      "G loss: 3.1991777420043945\n",
      "E loss:  1.5907880067825317\n",
      "G loss: 3.205814838409424\n",
      "Training Model  ...\n",
      "E loss:  1.5726267099380493\n",
      "G loss: 3.204857349395752\n",
      "E loss:  1.5921984910964966\n",
      "G loss: 3.2043116092681885\n",
      "E loss:  1.6275537014007568\n",
      "G loss: 3.1822032928466797\n",
      "E loss:  1.6041860580444336\n",
      "G loss: 3.187883138656616\n",
      "E loss:  1.6068137884140015\n",
      "G loss: 3.174359083175659\n",
      "Training Model  ...\n",
      "E loss:  1.433898687362671\n",
      "G loss: 3.1552414894104004\n",
      "E loss:  1.4500045776367188\n",
      "G loss: 3.1259446144104004\n",
      "E loss:  1.4621129035949707\n",
      "G loss: 3.100416421890259\n",
      "E loss:  1.4675120115280151\n",
      "G loss: 3.0501463413238525\n",
      "E loss:  1.4512302875518799\n",
      "G loss: 2.9979937076568604\n",
      "Training Model  ...\n",
      "E loss:  1.611459493637085\n",
      "G loss: 2.9955661296844482\n",
      "E loss:  1.5750113725662231\n",
      "G loss: 2.976015090942383\n",
      "E loss:  1.5718828439712524\n",
      "G loss: 2.971299171447754\n",
      "E loss:  1.5488489866256714\n",
      "G loss: 2.951280355453491\n",
      "E loss:  1.5514286756515503\n",
      "G loss: 2.9507548809051514\n",
      "Training Model  ...\n",
      "E loss:  1.6119420528411865\n",
      "G loss: 2.946979522705078\n",
      "E loss:  1.5756328105926514\n",
      "G loss: 2.957674980163574\n",
      "E loss:  1.6002577543258667\n",
      "G loss: 2.9691476821899414\n",
      "E loss:  1.5883245468139648\n",
      "G loss: 2.9691686630249023\n",
      "E loss:  1.5364936590194702\n",
      "G loss: 2.9849579334259033\n",
      "Training Model  ...\n",
      "E loss:  1.7023905515670776\n",
      "G loss: 2.971409320831299\n",
      "E loss:  1.681060552597046\n",
      "G loss: 2.9840450286865234\n",
      "E loss:  1.6471363306045532\n",
      "G loss: 2.978813409805298\n",
      "E loss:  1.6467640399932861\n",
      "G loss: 2.996098279953003\n",
      "E loss:  1.6057075262069702\n",
      "G loss: 2.99515962600708\n",
      "Training Model  ...\n",
      "E loss:  1.4074699878692627\n",
      "G loss: 2.9980382919311523\n",
      "E loss:  1.4355405569076538\n",
      "G loss: 2.9945690631866455\n",
      "E loss:  1.4572240114212036\n",
      "G loss: 3.01359224319458\n",
      "E loss:  1.454911470413208\n",
      "G loss: 3.0301578044891357\n",
      "E loss:  1.4660563468933105\n",
      "G loss: 3.0407023429870605\n",
      "Training Model  ...\n",
      "E loss:  1.5128005743026733\n",
      "G loss: 3.0373151302337646\n",
      "E loss:  1.4963209629058838\n",
      "G loss: 3.031421661376953\n",
      "E loss:  1.50919771194458\n",
      "G loss: 3.0413804054260254\n",
      "E loss:  1.49337637424469\n",
      "G loss: 3.0431275367736816\n",
      "E loss:  1.4785079956054688\n",
      "G loss: 3.0392301082611084\n",
      "Training Model  ...\n",
      "E loss:  1.630013108253479\n",
      "G loss: 3.044365882873535\n",
      "E loss:  1.6415587663650513\n",
      "G loss: 3.048813819885254\n",
      "E loss:  1.6237545013427734\n",
      "G loss: 3.0441536903381348\n",
      "E loss:  1.587664008140564\n",
      "G loss: 3.0511374473571777\n",
      "E loss:  1.5581473112106323\n",
      "G loss: 3.0493268966674805\n",
      "Training Model  ...\n",
      "E loss:  1.6438119411468506\n",
      "G loss: 3.057466506958008\n",
      "E loss:  1.6326136589050293\n",
      "G loss: 3.0410828590393066\n",
      "E loss:  1.6078617572784424\n",
      "G loss: 3.0340840816497803\n",
      "E loss:  1.5894889831542969\n",
      "G loss: 3.031259536743164\n",
      "E loss:  1.6241250038146973\n",
      "G loss: 3.02243971824646\n",
      "Training Model  ...\n",
      "E loss:  1.6051499843597412\n",
      "G loss: 3.0088062286376953\n",
      "E loss:  1.5758713483810425\n",
      "G loss: 2.99642276763916\n",
      "E loss:  1.6110787391662598\n",
      "G loss: 2.97578501701355\n",
      "E loss:  1.6103726625442505\n",
      "G loss: 2.967881679534912\n",
      "E loss:  1.602360725402832\n",
      "G loss: 2.9449708461761475\n",
      "Training Model  ...\n",
      "E loss:  1.6575978994369507\n",
      "G loss: 2.9498696327209473\n",
      "E loss:  1.6559842824935913\n",
      "G loss: 2.948392391204834\n",
      "E loss:  1.6852625608444214\n",
      "G loss: 2.9455671310424805\n",
      "E loss:  1.6771552562713623\n",
      "G loss: 2.964301586151123\n",
      "E loss:  1.650098204612732\n",
      "G loss: 2.9665257930755615\n",
      "Training Model  ...\n",
      "E loss:  1.4732093811035156\n",
      "G loss: 2.947770357131958\n",
      "E loss:  1.471833348274231\n",
      "G loss: 2.9485855102539062\n",
      "E loss:  1.460456371307373\n",
      "G loss: 2.9391191005706787\n",
      "E loss:  1.4723703861236572\n",
      "G loss: 2.918370246887207\n",
      "E loss:  1.4665776491165161\n",
      "G loss: 2.895570755004883\n",
      "Training Model  ...\n",
      "E loss:  1.5153380632400513\n",
      "G loss: 2.89913272857666\n",
      "E loss:  1.5229191780090332\n",
      "G loss: 2.9170899391174316\n",
      "E loss:  1.5415135622024536\n",
      "G loss: 2.9303340911865234\n",
      "E loss:  1.524459719657898\n",
      "G loss: 2.968667984008789\n",
      "E loss:  1.5361605882644653\n",
      "G loss: 2.9841251373291016\n",
      "Training Model  ...\n",
      "E loss:  1.669658899307251\n",
      "G loss: 2.9846179485321045\n",
      "E loss:  1.6398711204528809\n",
      "G loss: 2.9938974380493164\n",
      "E loss:  1.629591464996338\n",
      "G loss: 3.0086636543273926\n",
      "E loss:  1.6311850547790527\n",
      "G loss: 3.0014100074768066\n",
      "E loss:  1.618482232093811\n",
      "G loss: 3.0295517444610596\n",
      "Training Model  ...\n",
      "E loss:  1.4722623825073242\n",
      "G loss: 3.0279648303985596\n",
      "E loss:  1.4928786754608154\n",
      "G loss: 3.0167758464813232\n",
      "E loss:  1.498181939125061\n",
      "G loss: 3.019681692123413\n",
      "E loss:  1.498579978942871\n",
      "G loss: 3.0172722339630127\n",
      "E loss:  1.4678428173065186\n",
      "G loss: 3.0189666748046875\n",
      "Training Model  ...\n",
      "E loss:  1.4078577756881714\n",
      "G loss: 3.0083935260772705\n",
      "E loss:  1.4045532941818237\n",
      "G loss: 2.993337869644165\n",
      "E loss:  1.425234079360962\n",
      "G loss: 2.9683239459991455\n",
      "E loss:  1.4532002210617065\n",
      "G loss: 2.9417388439178467\n",
      "E loss:  1.4702600240707397\n",
      "G loss: 2.906228542327881\n",
      "Training Model  ...\n",
      "E loss:  1.59269118309021\n",
      "G loss: 2.9022881984710693\n",
      "E loss:  1.5574419498443604\n",
      "G loss: 2.912648916244507\n",
      "E loss:  1.5340043306350708\n",
      "G loss: 2.9141721725463867\n",
      "E loss:  1.5390136241912842\n",
      "G loss: 2.908737897872925\n",
      "E loss:  1.508413553237915\n",
      "G loss: 2.9132752418518066\n",
      "Training Model  ...\n",
      "E loss:  1.6327884197235107\n",
      "G loss: 2.9073610305786133\n",
      "E loss:  1.6546801328659058\n",
      "G loss: 2.895097255706787\n",
      "E loss:  1.6424152851104736\n",
      "G loss: 2.8778748512268066\n",
      "E loss:  1.6410562992095947\n",
      "G loss: 2.839252471923828\n",
      "E loss:  1.6128127574920654\n",
      "G loss: 2.8172786235809326\n",
      "Training Model  ...\n",
      "E loss:  1.466353416442871\n",
      "G loss: 2.8127756118774414\n",
      "E loss:  1.449106216430664\n",
      "G loss: 2.797452211380005\n",
      "E loss:  1.4583947658538818\n",
      "G loss: 2.7745795249938965\n",
      "E loss:  1.4491907358169556\n",
      "G loss: 2.760894775390625\n",
      "E loss:  1.4309784173965454\n",
      "G loss: 2.7279133796691895\n",
      "Training Model  ...\n",
      "E loss:  1.5891916751861572\n",
      "G loss: 2.7307379245758057\n",
      "E loss:  1.6218483448028564\n",
      "G loss: 2.7486300468444824\n",
      "E loss:  1.6225284337997437\n",
      "G loss: 2.7576911449432373\n",
      "E loss:  1.666689395904541\n",
      "G loss: 2.7681093215942383\n",
      "E loss:  1.6559405326843262\n",
      "G loss: 2.787013053894043\n",
      "Training Model  ...\n",
      "E loss:  1.8108233213424683\n",
      "G loss: 2.7962660789489746\n",
      "E loss:  1.8117129802703857\n",
      "G loss: 2.8092098236083984\n",
      "E loss:  1.8006551265716553\n",
      "G loss: 2.828646659851074\n",
      "E loss:  1.7807437181472778\n",
      "G loss: 2.8553810119628906\n",
      "E loss:  1.7718876600265503\n",
      "G loss: 2.899125337600708\n",
      "Training Model  ...\n",
      "E loss:  1.5424072742462158\n",
      "G loss: 2.899365186691284\n",
      "E loss:  1.5077216625213623\n",
      "G loss: 2.9056172370910645\n",
      "E loss:  1.4727659225463867\n",
      "G loss: 2.9165091514587402\n",
      "E loss:  1.4789316654205322\n",
      "G loss: 2.9248833656311035\n",
      "E loss:  1.484778642654419\n",
      "G loss: 2.9371509552001953\n",
      "Training Model  ...\n",
      "E loss:  1.5914006233215332\n",
      "G loss: 2.933917999267578\n",
      "E loss:  1.6152899265289307\n",
      "G loss: 2.945875644683838\n",
      "E loss:  1.6229772567749023\n",
      "G loss: 2.941343069076538\n",
      "E loss:  1.6368846893310547\n",
      "G loss: 2.9467902183532715\n",
      "E loss:  1.6382853984832764\n",
      "G loss: 2.9407477378845215\n",
      "Training Model  ...\n",
      "E loss:  1.3668043613433838\n",
      "G loss: 2.941537618637085\n",
      "E loss:  1.3582969903945923\n",
      "G loss: 2.9252331256866455\n",
      "E loss:  1.3387250900268555\n",
      "G loss: 2.914567470550537\n",
      "E loss:  1.3584579229354858\n",
      "G loss: 2.9087815284729004\n",
      "E loss:  1.335842490196228\n",
      "G loss: 2.8920791149139404\n",
      "Training Model  ...\n",
      "E loss:  1.5147600173950195\n",
      "G loss: 2.90498685836792\n",
      "E loss:  1.497273564338684\n",
      "G loss: 2.8894803524017334\n",
      "E loss:  1.4765914678573608\n",
      "G loss: 2.910392999649048\n",
      "E loss:  1.512319803237915\n",
      "G loss: 2.912189483642578\n",
      "E loss:  1.5079433917999268\n",
      "G loss: 2.9163708686828613\n",
      "Training Model  ...\n",
      "E loss:  1.7361769676208496\n",
      "G loss: 2.9160385131835938\n",
      "E loss:  1.7291029691696167\n",
      "G loss: 2.9269509315490723\n",
      "E loss:  1.7070138454437256\n",
      "G loss: 2.9526708126068115\n",
      "E loss:  1.6479387283325195\n",
      "G loss: 2.963290214538574\n",
      "E loss:  1.6843754053115845\n",
      "G loss: 2.9877688884735107\n",
      "Training Model  ...\n",
      "E loss:  1.6682934761047363\n",
      "G loss: 2.9777169227600098\n",
      "E loss:  1.632608413696289\n",
      "G loss: 2.989508867263794\n",
      "E loss:  1.6343185901641846\n",
      "G loss: 2.9737634658813477\n",
      "E loss:  1.6283847093582153\n",
      "G loss: 2.9751195907592773\n",
      "E loss:  1.6306883096694946\n",
      "G loss: 2.989556074142456\n",
      "Training Model  ...\n",
      "E loss:  1.564467430114746\n",
      "G loss: 2.9973530769348145\n",
      "E loss:  1.5541822910308838\n",
      "G loss: 2.9863834381103516\n",
      "E loss:  1.5498038530349731\n",
      "G loss: 2.980917453765869\n",
      "E loss:  1.5303285121917725\n",
      "G loss: 2.992387294769287\n",
      "E loss:  1.5155552625656128\n",
      "G loss: 2.9946396350860596\n",
      "Training Model  ...\n",
      "E loss:  1.5104329586029053\n",
      "G loss: 2.973153829574585\n",
      "E loss:  1.5138990879058838\n",
      "G loss: 2.963862895965576\n",
      "E loss:  1.5163182020187378\n",
      "G loss: 2.948843002319336\n",
      "E loss:  1.495431900024414\n",
      "G loss: 2.9225802421569824\n",
      "E loss:  1.5130290985107422\n",
      "G loss: 2.8947319984436035\n",
      "Training Model  ...\n",
      "E loss:  1.4951192140579224\n",
      "G loss: 2.8710696697235107\n",
      "E loss:  1.5088762044906616\n",
      "G loss: 2.881088972091675\n",
      "E loss:  1.480022668838501\n",
      "G loss: 2.8687779903411865\n",
      "E loss:  1.5157909393310547\n",
      "G loss: 2.8606173992156982\n",
      "E loss:  1.4759769439697266\n",
      "G loss: 2.839862108230591\n",
      "Training Model  ...\n",
      "E loss:  1.5040311813354492\n",
      "G loss: 2.852370023727417\n",
      "E loss:  1.4832956790924072\n",
      "G loss: 2.8563895225524902\n",
      "E loss:  1.4774770736694336\n",
      "G loss: 2.8692946434020996\n",
      "E loss:  1.4863617420196533\n",
      "G loss: 2.8795416355133057\n",
      "E loss:  1.459140419960022\n",
      "G loss: 2.900757312774658\n",
      "Training Model  ...\n",
      "E loss:  1.5055922269821167\n",
      "G loss: 2.8924458026885986\n",
      "E loss:  1.5091758966445923\n",
      "G loss: 2.887998580932617\n",
      "E loss:  1.4865286350250244\n",
      "G loss: 2.8696630001068115\n",
      "E loss:  1.4853157997131348\n",
      "G loss: 2.857485055923462\n",
      "E loss:  1.4697564840316772\n",
      "G loss: 2.838510751724243\n",
      "Training Model  ...\n",
      "E loss:  1.5058071613311768\n",
      "G loss: 2.828719139099121\n",
      "E loss:  1.5444777011871338\n",
      "G loss: 2.81615948677063\n",
      "E loss:  1.5210403203964233\n",
      "G loss: 2.784545421600342\n",
      "E loss:  1.5179286003112793\n",
      "G loss: 2.779165506362915\n",
      "E loss:  1.5244958400726318\n",
      "G loss: 2.757887125015259\n",
      "Training Model  ...\n",
      "E loss:  1.3639459609985352\n",
      "G loss: 2.76589298248291\n",
      "E loss:  1.352040410041809\n",
      "G loss: 2.76869535446167\n",
      "E loss:  1.3211809396743774\n",
      "G loss: 2.7894973754882812\n",
      "E loss:  1.3228119611740112\n",
      "G loss: 2.8192625045776367\n",
      "E loss:  1.3092023134231567\n",
      "G loss: 2.8457205295562744\n",
      "Training Model  ...\n",
      "E loss:  1.545500636100769\n",
      "G loss: 2.8453426361083984\n",
      "E loss:  1.5036412477493286\n",
      "G loss: 2.8398759365081787\n",
      "E loss:  1.5322988033294678\n",
      "G loss: 2.8280816078186035\n",
      "E loss:  1.5436969995498657\n",
      "G loss: 2.8232579231262207\n",
      "E loss:  1.571529507637024\n",
      "G loss: 2.8095669746398926\n",
      "Training Model  ...\n",
      "E loss:  1.5396089553833008\n",
      "G loss: 2.8249645233154297\n",
      "E loss:  1.5543911457061768\n",
      "G loss: 2.8215959072113037\n",
      "E loss:  1.5144681930541992\n",
      "G loss: 2.827026128768921\n",
      "E loss:  1.5348392724990845\n",
      "G loss: 2.82407808303833\n",
      "E loss:  1.5408498048782349\n",
      "G loss: 2.840078830718994\n",
      "Training Model  ...\n",
      "E loss:  1.409018635749817\n",
      "G loss: 2.8347465991973877\n",
      "E loss:  1.3790440559387207\n",
      "G loss: 2.8443381786346436\n",
      "E loss:  1.3848592042922974\n",
      "G loss: 2.842027187347412\n",
      "E loss:  1.3905234336853027\n",
      "G loss: 2.8449056148529053\n",
      "E loss:  1.3814882040023804\n",
      "G loss: 2.8608593940734863\n",
      "Training Model  ...\n",
      "E loss:  1.625447392463684\n",
      "G loss: 2.8557238578796387\n",
      "E loss:  1.5984928607940674\n",
      "G loss: 2.8687009811401367\n",
      "E loss:  1.621829628944397\n",
      "G loss: 2.8707802295684814\n",
      "E loss:  1.6203721761703491\n",
      "G loss: 2.887356996536255\n",
      "E loss:  1.6105034351348877\n",
      "G loss: 2.8915295600891113\n",
      "Training Model  ...\n",
      "E loss:  1.464563250541687\n",
      "G loss: 2.9075982570648193\n",
      "E loss:  1.4452940225601196\n",
      "G loss: 2.883742094039917\n",
      "E loss:  1.4479584693908691\n",
      "G loss: 2.8800578117370605\n",
      "E loss:  1.4517446756362915\n",
      "G loss: 2.8717312812805176\n",
      "E loss:  1.4526206254959106\n",
      "G loss: 2.844656229019165\n",
      "Training Model  ...\n",
      "E loss:  1.5858780145645142\n",
      "G loss: 2.8495144844055176\n",
      "E loss:  1.5743486881256104\n",
      "G loss: 2.830606460571289\n",
      "E loss:  1.5888679027557373\n",
      "G loss: 2.8069210052490234\n",
      "E loss:  1.5641138553619385\n",
      "G loss: 2.784806251525879\n",
      "E loss:  1.5754413604736328\n",
      "G loss: 2.7663731575012207\n",
      "Training Model  ...\n",
      "E loss:  1.3962655067443848\n",
      "G loss: 2.7631263732910156\n",
      "E loss:  1.4108891487121582\n",
      "G loss: 2.767317533493042\n",
      "E loss:  1.4304447174072266\n",
      "G loss: 2.7580742835998535\n",
      "E loss:  1.445144534111023\n",
      "G loss: 2.750194787979126\n",
      "E loss:  1.439118504524231\n",
      "G loss: 2.749303102493286\n",
      "Training Model  ...\n",
      "E loss:  1.647018551826477\n",
      "G loss: 2.7616219520568848\n",
      "E loss:  1.6266748905181885\n",
      "G loss: 2.7499003410339355\n",
      "E loss:  1.6164524555206299\n",
      "G loss: 2.7543301582336426\n",
      "E loss:  1.583856463432312\n",
      "G loss: 2.7476158142089844\n",
      "E loss:  1.5923223495483398\n",
      "G loss: 2.7587363719940186\n",
      "Training Model  ...\n",
      "E loss:  1.6476914882659912\n",
      "G loss: 2.7572646141052246\n",
      "E loss:  1.638948917388916\n",
      "G loss: 2.7707619667053223\n",
      "E loss:  1.614882469177246\n",
      "G loss: 2.785757064819336\n",
      "E loss:  1.5918598175048828\n",
      "G loss: 2.802626132965088\n",
      "E loss:  1.6180176734924316\n",
      "G loss: 2.8116374015808105\n",
      "Training Model  ...\n",
      "E loss:  1.3228754997253418\n",
      "G loss: 2.817474842071533\n",
      "E loss:  1.2898095846176147\n",
      "G loss: 2.7998151779174805\n",
      "E loss:  1.2981683015823364\n",
      "G loss: 2.8049490451812744\n",
      "E loss:  1.295811653137207\n",
      "G loss: 2.792297840118408\n",
      "E loss:  1.2733873128890991\n",
      "G loss: 2.782710552215576\n",
      "Training Model  ...\n",
      "E loss:  1.636923909187317\n",
      "G loss: 2.7757551670074463\n",
      "E loss:  1.6640843152999878\n",
      "G loss: 2.773695468902588\n",
      "E loss:  1.6569061279296875\n",
      "G loss: 2.7546942234039307\n",
      "E loss:  1.6732813119888306\n",
      "G loss: 2.7411446571350098\n",
      "E loss:  1.6397197246551514\n",
      "G loss: 2.7209229469299316\n",
      "Training Model  ...\n",
      "E loss:  1.4304224252700806\n",
      "G loss: 2.725092649459839\n",
      "E loss:  1.4338099956512451\n",
      "G loss: 2.731652021408081\n",
      "E loss:  1.4459362030029297\n",
      "G loss: 2.731436252593994\n",
      "E loss:  1.441586971282959\n",
      "G loss: 2.7273876667022705\n",
      "E loss:  1.4464279413223267\n",
      "G loss: 2.7272679805755615\n",
      "Training Model  ...\n",
      "E loss:  1.3680864572525024\n",
      "G loss: 2.715866804122925\n",
      "E loss:  1.3753010034561157\n",
      "G loss: 2.712082862854004\n",
      "E loss:  1.321087121963501\n",
      "G loss: 2.705770492553711\n",
      "E loss:  1.3365094661712646\n",
      "G loss: 2.6885085105895996\n",
      "E loss:  1.3486063480377197\n",
      "G loss: 2.6725778579711914\n",
      "Training Model  ...\n",
      "E loss:  1.56588876247406\n",
      "G loss: 2.6831119060516357\n",
      "E loss:  1.5551414489746094\n",
      "G loss: 2.6825287342071533\n",
      "E loss:  1.5456218719482422\n",
      "G loss: 2.6944754123687744\n",
      "E loss:  1.5407214164733887\n",
      "G loss: 2.7101831436157227\n",
      "E loss:  1.5230562686920166\n",
      "G loss: 2.7375240325927734\n",
      "Training Model  ...\n",
      "E loss:  1.5501220226287842\n",
      "G loss: 2.734677314758301\n",
      "E loss:  1.5250334739685059\n",
      "G loss: 2.7401459217071533\n",
      "E loss:  1.4968445301055908\n",
      "G loss: 2.7455458641052246\n",
      "E loss:  1.5184543132781982\n",
      "G loss: 2.757638931274414\n",
      "E loss:  1.5076425075531006\n",
      "G loss: 2.7824277877807617\n",
      "Training Model  ...\n",
      "E loss:  1.3947888612747192\n",
      "G loss: 2.7719616889953613\n",
      "E loss:  1.3873265981674194\n",
      "G loss: 2.765547275543213\n",
      "E loss:  1.3956859111785889\n",
      "G loss: 2.767961025238037\n",
      "E loss:  1.376985788345337\n",
      "G loss: 2.771073341369629\n",
      "E loss:  1.35340416431427\n",
      "G loss: 2.7499356269836426\n",
      "Training Model  ...\n",
      "E loss:  1.6681431531906128\n",
      "G loss: 2.7728025913238525\n",
      "E loss:  1.6574032306671143\n",
      "G loss: 2.8006858825683594\n",
      "E loss:  1.6137852668762207\n",
      "G loss: 2.8212804794311523\n",
      "E loss:  1.61012864112854\n",
      "G loss: 2.851121664047241\n",
      "E loss:  1.5760149955749512\n",
      "G loss: 2.8798646926879883\n",
      "Training Model  ...\n",
      "E loss:  1.4806361198425293\n",
      "G loss: 2.882491111755371\n",
      "E loss:  1.4257785081863403\n",
      "G loss: 2.861527919769287\n",
      "E loss:  1.4121030569076538\n",
      "G loss: 2.8187131881713867\n",
      "E loss:  1.3942095041275024\n",
      "G loss: 2.8035309314727783\n",
      "E loss:  1.400635838508606\n",
      "G loss: 2.7752912044525146\n",
      "Training Model  ...\n",
      "E loss:  1.4343452453613281\n",
      "G loss: 2.760051965713501\n",
      "E loss:  1.4154343605041504\n",
      "G loss: 2.7556116580963135\n",
      "E loss:  1.420028805732727\n",
      "G loss: 2.746760368347168\n",
      "E loss:  1.4351649284362793\n",
      "G loss: 2.727684497833252\n",
      "E loss:  1.4401872158050537\n",
      "G loss: 2.7170825004577637\n",
      "Training Model  ...\n",
      "E loss:  1.6642751693725586\n",
      "G loss: 2.719489097595215\n",
      "E loss:  1.660652756690979\n",
      "G loss: 2.7371127605438232\n",
      "E loss:  1.672838568687439\n",
      "G loss: 2.7683768272399902\n",
      "E loss:  1.690661907196045\n",
      "G loss: 2.7911880016326904\n",
      "E loss:  1.694473385810852\n",
      "G loss: 2.837641716003418\n",
      "Training Model  ...\n",
      "E loss:  1.5453345775604248\n",
      "G loss: 2.830000877380371\n",
      "E loss:  1.537733793258667\n",
      "G loss: 2.8433456420898438\n",
      "E loss:  1.5551849603652954\n",
      "G loss: 2.843280076980591\n",
      "E loss:  1.529704213142395\n",
      "G loss: 2.840639114379883\n",
      "E loss:  1.5132195949554443\n",
      "G loss: 2.852494716644287\n",
      "Training Model  ...\n",
      "E loss:  1.5846545696258545\n",
      "G loss: 2.849433422088623\n",
      "E loss:  1.5915592908859253\n",
      "G loss: 2.8407137393951416\n",
      "E loss:  1.5943753719329834\n",
      "G loss: 2.829237461090088\n",
      "E loss:  1.5967955589294434\n",
      "G loss: 2.826939582824707\n",
      "E loss:  1.5953222513198853\n",
      "G loss: 2.812892198562622\n",
      "Training Model  ...\n",
      "E loss:  1.4814207553863525\n",
      "G loss: 2.8020622730255127\n",
      "E loss:  1.4718159437179565\n",
      "G loss: 2.7980313301086426\n",
      "E loss:  1.4468238353729248\n",
      "G loss: 2.7854385375976562\n",
      "E loss:  1.476681113243103\n",
      "G loss: 2.761275053024292\n",
      "E loss:  1.4750330448150635\n",
      "G loss: 2.7452592849731445\n",
      "Training Model  ...\n",
      "E loss:  1.5135890245437622\n",
      "G loss: 2.7266314029693604\n",
      "E loss:  1.5120476484298706\n",
      "G loss: 2.736973285675049\n",
      "E loss:  1.4930167198181152\n",
      "G loss: 2.749267101287842\n",
      "E loss:  1.4788813591003418\n",
      "G loss: 2.7471749782562256\n",
      "E loss:  1.503990888595581\n",
      "G loss: 2.7446436882019043\n",
      "Training Model  ...\n",
      "E loss:  1.522394061088562\n",
      "G loss: 2.7488296031951904\n",
      "E loss:  1.5357064008712769\n",
      "G loss: 2.7574782371520996\n",
      "E loss:  1.5280143022537231\n",
      "G loss: 2.7576794624328613\n",
      "E loss:  1.5078054666519165\n",
      "G loss: 2.7643911838531494\n",
      "E loss:  1.5210614204406738\n",
      "G loss: 2.7690141201019287\n",
      "Training Model  ...\n",
      "E loss:  1.5675097703933716\n",
      "G loss: 2.772552251815796\n",
      "E loss:  1.5626890659332275\n",
      "G loss: 2.768801689147949\n",
      "E loss:  1.5679216384887695\n",
      "G loss: 2.7813310623168945\n",
      "E loss:  1.5540226697921753\n",
      "G loss: 2.7859644889831543\n",
      "E loss:  1.567771553993225\n",
      "G loss: 2.783536195755005\n",
      "Training Model  ...\n",
      "E loss:  1.6095740795135498\n",
      "G loss: 2.778913736343384\n",
      "E loss:  1.6129353046417236\n",
      "G loss: 2.7747926712036133\n",
      "E loss:  1.612277865409851\n",
      "G loss: 2.7684545516967773\n",
      "E loss:  1.6411995887756348\n",
      "G loss: 2.7686212062835693\n",
      "E loss:  1.6075069904327393\n",
      "G loss: 2.7525815963745117\n",
      "Training Model  ...\n",
      "E loss:  1.4600077867507935\n",
      "G loss: 2.735731363296509\n",
      "E loss:  1.4626926183700562\n",
      "G loss: 2.7340898513793945\n",
      "E loss:  1.4731922149658203\n",
      "G loss: 2.725090742111206\n",
      "E loss:  1.4685827493667603\n",
      "G loss: 2.686586380004883\n",
      "E loss:  1.4592334032058716\n",
      "G loss: 2.67120099067688\n",
      "Training Model  ...\n",
      "E loss:  1.5532366037368774\n",
      "G loss: 2.6747803688049316\n",
      "E loss:  1.5458036661148071\n",
      "G loss: 2.685037136077881\n",
      "E loss:  1.5506418943405151\n",
      "G loss: 2.7074458599090576\n",
      "E loss:  1.5514490604400635\n",
      "G loss: 2.70725154876709\n",
      "E loss:  1.5520813465118408\n",
      "G loss: 2.7290239334106445\n",
      "Training Model  ...\n",
      "E loss:  1.4072257280349731\n",
      "G loss: 2.728935480117798\n",
      "E loss:  1.4195618629455566\n",
      "G loss: 2.7010598182678223\n",
      "E loss:  1.3902708292007446\n",
      "G loss: 2.6735312938690186\n",
      "E loss:  1.3935058116912842\n",
      "G loss: 2.6439764499664307\n",
      "E loss:  1.3830375671386719\n",
      "G loss: 2.6134347915649414\n",
      "Training Model  ...\n",
      "E loss:  1.4685752391815186\n",
      "G loss: 2.606221914291382\n",
      "E loss:  1.495308756828308\n",
      "G loss: 2.6061601638793945\n",
      "E loss:  1.512224793434143\n",
      "G loss: 2.585871696472168\n",
      "E loss:  1.4820365905761719\n",
      "G loss: 2.552888870239258\n",
      "E loss:  1.482435941696167\n",
      "G loss: 2.5327067375183105\n",
      "Training Model  ...\n",
      "E loss:  1.527462124824524\n",
      "G loss: 2.539137363433838\n",
      "E loss:  1.5315908193588257\n",
      "G loss: 2.5500974655151367\n",
      "E loss:  1.514804482460022\n",
      "G loss: 2.5764451026916504\n",
      "E loss:  1.532747745513916\n",
      "G loss: 2.587329864501953\n",
      "E loss:  1.5296928882598877\n",
      "G loss: 2.6304874420166016\n",
      "Training Model  ...\n",
      "E loss:  1.5842969417572021\n",
      "G loss: 2.61885666847229\n",
      "E loss:  1.5508112907409668\n",
      "G loss: 2.634286642074585\n",
      "E loss:  1.548309564590454\n",
      "G loss: 2.640842914581299\n",
      "E loss:  1.5612549781799316\n",
      "G loss: 2.669086456298828\n",
      "E loss:  1.562880516052246\n",
      "G loss: 2.694448947906494\n",
      "Training Model  ...\n",
      "E loss:  1.4258458614349365\n",
      "G loss: 2.6702144145965576\n",
      "E loss:  1.448628306388855\n",
      "G loss: 2.6874942779541016\n",
      "E loss:  1.4555928707122803\n",
      "G loss: 2.674025058746338\n",
      "E loss:  1.4775798320770264\n",
      "G loss: 2.6618430614471436\n",
      "E loss:  1.491213321685791\n",
      "G loss: 2.6535215377807617\n",
      "Training Model  ...\n",
      "E loss:  1.4571397304534912\n",
      "G loss: 2.6564886569976807\n",
      "E loss:  1.463945746421814\n",
      "G loss: 2.6652848720550537\n",
      "E loss:  1.4526005983352661\n",
      "G loss: 2.6754837036132812\n",
      "E loss:  1.4556630849838257\n",
      "G loss: 2.693952798843384\n",
      "E loss:  1.428307056427002\n",
      "G loss: 2.6958231925964355\n",
      "Training Model  ...\n",
      "E loss:  1.5188990831375122\n",
      "G loss: 2.71162748336792\n",
      "E loss:  1.5180833339691162\n",
      "G loss: 2.6998839378356934\n",
      "E loss:  1.4923033714294434\n",
      "G loss: 2.696499824523926\n",
      "E loss:  1.501509428024292\n",
      "G loss: 2.695272445678711\n",
      "E loss:  1.4762992858886719\n",
      "G loss: 2.683595657348633\n",
      "Training Model  ...\n",
      "E loss:  1.6340583562850952\n",
      "G loss: 2.66668701171875\n",
      "E loss:  1.6175168752670288\n",
      "G loss: 2.681776523590088\n",
      "E loss:  1.6126084327697754\n",
      "G loss: 2.7049453258514404\n",
      "E loss:  1.586097002029419\n",
      "G loss: 2.7135567665100098\n",
      "E loss:  1.5887892246246338\n",
      "G loss: 2.722179412841797\n",
      "Training Model  ...\n",
      "E loss:  1.4569499492645264\n",
      "G loss: 2.715456008911133\n",
      "E loss:  1.4689041376113892\n",
      "G loss: 2.6949572563171387\n",
      "E loss:  1.4515135288238525\n",
      "G loss: 2.6929702758789062\n",
      "E loss:  1.4237961769104004\n",
      "G loss: 2.690134048461914\n",
      "E loss:  1.4380347728729248\n",
      "G loss: 2.6861612796783447\n",
      "Training Model  ...\n",
      "E loss:  1.4599790573120117\n",
      "G loss: 2.681201457977295\n",
      "E loss:  1.5005136728286743\n",
      "G loss: 2.6698191165924072\n",
      "E loss:  1.4793071746826172\n",
      "G loss: 2.6572952270507812\n",
      "E loss:  1.4828773736953735\n",
      "G loss: 2.651289701461792\n",
      "E loss:  1.458433747291565\n",
      "G loss: 2.6359825134277344\n",
      "Training Model  ...\n",
      "E loss:  1.485114336013794\n",
      "G loss: 2.629967212677002\n",
      "E loss:  1.4649293422698975\n",
      "G loss: 2.636024236679077\n",
      "E loss:  1.4605484008789062\n",
      "G loss: 2.6310369968414307\n",
      "E loss:  1.490113615989685\n",
      "G loss: 2.639472484588623\n",
      "E loss:  1.4716918468475342\n",
      "G loss: 2.6113381385803223\n",
      "Training Model  ...\n",
      "E loss:  1.5948783159255981\n",
      "G loss: 2.62495756149292\n",
      "E loss:  1.6039934158325195\n",
      "G loss: 2.619675636291504\n",
      "E loss:  1.596274733543396\n",
      "G loss: 2.62557053565979\n",
      "E loss:  1.614171028137207\n",
      "G loss: 2.628476142883301\n",
      "E loss:  1.622091293334961\n",
      "G loss: 2.6338870525360107\n",
      "Training Model  ...\n",
      "E loss:  1.5463266372680664\n",
      "G loss: 2.6297190189361572\n",
      "E loss:  1.5588144063949585\n",
      "G loss: 2.6518290042877197\n",
      "E loss:  1.5687685012817383\n",
      "G loss: 2.6665561199188232\n",
      "E loss:  1.536831021308899\n",
      "G loss: 2.7158727645874023\n",
      "E loss:  1.5539774894714355\n",
      "G loss: 2.7370471954345703\n",
      "Training Model  ...\n",
      "E loss:  1.542299747467041\n",
      "G loss: 2.7252469062805176\n",
      "E loss:  1.5280468463897705\n",
      "G loss: 2.7247819900512695\n",
      "E loss:  1.5483248233795166\n",
      "G loss: 2.7069365978240967\n",
      "E loss:  1.543436884880066\n",
      "G loss: 2.6991024017333984\n",
      "E loss:  1.5604647397994995\n",
      "G loss: 2.692120313644409\n",
      "Training Model  ...\n",
      "E loss:  1.6085668802261353\n",
      "G loss: 2.6744191646575928\n",
      "E loss:  1.6105213165283203\n",
      "G loss: 2.6976897716522217\n",
      "E loss:  1.6046552658081055\n",
      "G loss: 2.6854727268218994\n",
      "E loss:  1.5876615047454834\n",
      "G loss: 2.681833028793335\n",
      "E loss:  1.5862393379211426\n",
      "G loss: 2.704158067703247\n",
      "Training Model  ...\n",
      "E loss:  1.3827046155929565\n",
      "G loss: 2.714899778366089\n",
      "E loss:  1.3572849035263062\n",
      "G loss: 2.71357798576355\n",
      "E loss:  1.344597578048706\n",
      "G loss: 2.7200939655303955\n",
      "E loss:  1.3457118272781372\n",
      "G loss: 2.715186595916748\n",
      "E loss:  1.341816782951355\n",
      "G loss: 2.723146915435791\n",
      "Training Model  ...\n",
      "E loss:  1.4142403602600098\n",
      "G loss: 2.698030710220337\n",
      "E loss:  1.4354567527770996\n",
      "G loss: 2.697277069091797\n",
      "E loss:  1.4449371099472046\n",
      "G loss: 2.691307544708252\n",
      "E loss:  1.4597158432006836\n",
      "G loss: 2.6685290336608887\n",
      "E loss:  1.4433025121688843\n",
      "G loss: 2.652974843978882\n",
      "Training Model  ...\n",
      "E loss:  1.5103554725646973\n",
      "G loss: 2.6570627689361572\n",
      "E loss:  1.499405026435852\n",
      "G loss: 2.661524772644043\n",
      "E loss:  1.5141968727111816\n",
      "G loss: 2.672684907913208\n",
      "E loss:  1.5030540227890015\n",
      "G loss: 2.6834475994110107\n",
      "E loss:  1.495280146598816\n",
      "G loss: 2.7015275955200195\n",
      "Training Model  ...\n",
      "E loss:  1.4717389345169067\n",
      "G loss: 2.7092182636260986\n",
      "E loss:  1.4741742610931396\n",
      "G loss: 2.720233201980591\n",
      "E loss:  1.5019545555114746\n",
      "G loss: 2.732811450958252\n",
      "E loss:  1.4954520463943481\n",
      "G loss: 2.7544260025024414\n",
      "E loss:  1.5162253379821777\n",
      "G loss: 2.7617664337158203\n",
      "Training Model  ...\n",
      "E loss:  1.4567548036575317\n",
      "G loss: 2.762740135192871\n",
      "E loss:  1.4469165802001953\n",
      "G loss: 2.7707433700561523\n",
      "E loss:  1.4187177419662476\n",
      "G loss: 2.7513678073883057\n",
      "E loss:  1.3924027681350708\n",
      "G loss: 2.7388601303100586\n",
      "E loss:  1.3859508037567139\n",
      "G loss: 2.7261152267456055\n",
      "Training Model  ...\n",
      "E loss:  1.4005086421966553\n",
      "G loss: 2.7189667224884033\n",
      "E loss:  1.3988337516784668\n",
      "G loss: 2.7199766635894775\n",
      "E loss:  1.3915523290634155\n",
      "G loss: 2.6942138671875\n",
      "E loss:  1.3672114610671997\n",
      "G loss: 2.691661834716797\n",
      "E loss:  1.3480682373046875\n",
      "G loss: 2.677699565887451\n",
      "Training Model  ...\n",
      "E loss:  1.5192923545837402\n",
      "G loss: 2.683257579803467\n",
      "E loss:  1.5020989179611206\n",
      "G loss: 2.674293041229248\n",
      "E loss:  1.5191434621810913\n",
      "G loss: 2.6952872276306152\n",
      "E loss:  1.519476056098938\n",
      "G loss: 2.696951389312744\n",
      "E loss:  1.513462781906128\n",
      "G loss: 2.695103168487549\n",
      "Training Model  ...\n",
      "E loss:  1.3702908754348755\n",
      "G loss: 2.706524610519409\n",
      "E loss:  1.3529419898986816\n",
      "G loss: 2.715792179107666\n",
      "E loss:  1.3574609756469727\n",
      "G loss: 2.7090775966644287\n",
      "E loss:  1.3217765092849731\n",
      "G loss: 2.70802903175354\n",
      "E loss:  1.3083335161209106\n",
      "G loss: 2.709038019180298\n",
      "Training Model  ...\n",
      "E loss:  1.563116431236267\n",
      "G loss: 2.701646566390991\n",
      "E loss:  1.5780162811279297\n",
      "G loss: 2.6760518550872803\n",
      "E loss:  1.5811234712600708\n",
      "G loss: 2.6572344303131104\n",
      "E loss:  1.5944546461105347\n",
      "G loss: 2.632422924041748\n",
      "E loss:  1.5449028015136719\n",
      "G loss: 2.59873628616333\n",
      "Training Model  ...\n",
      "E loss:  1.261897325515747\n",
      "G loss: 2.5851712226867676\n",
      "E loss:  1.2446720600128174\n",
      "G loss: 2.5814483165740967\n",
      "E loss:  1.2666431665420532\n",
      "G loss: 2.551783561706543\n",
      "E loss:  1.2849676609039307\n",
      "G loss: 2.521134614944458\n",
      "E loss:  1.2903729677200317\n",
      "G loss: 2.491608142852783\n",
      "Training Model  ...\n",
      "E loss:  1.5415282249450684\n",
      "G loss: 2.4952824115753174\n",
      "E loss:  1.5086853504180908\n",
      "G loss: 2.4873006343841553\n",
      "E loss:  1.527012586593628\n",
      "G loss: 2.500478982925415\n",
      "E loss:  1.5394984483718872\n",
      "G loss: 2.517760992050171\n",
      "E loss:  1.4925898313522339\n",
      "G loss: 2.5320258140563965\n",
      "Training Model  ...\n",
      "E loss:  1.5379555225372314\n",
      "G loss: 2.5441222190856934\n",
      "E loss:  1.5133211612701416\n",
      "G loss: 2.540605306625366\n",
      "E loss:  1.5070289373397827\n",
      "G loss: 2.5582058429718018\n",
      "E loss:  1.5541919469833374\n",
      "G loss: 2.561157703399658\n",
      "E loss:  1.538983941078186\n",
      "G loss: 2.5643539428710938\n",
      "Training Model  ...\n",
      "E loss:  1.4165263175964355\n",
      "G loss: 2.557523727416992\n",
      "E loss:  1.418156385421753\n",
      "G loss: 2.5651445388793945\n",
      "E loss:  1.427390694618225\n",
      "G loss: 2.546875\n",
      "E loss:  1.4272806644439697\n",
      "G loss: 2.5353832244873047\n",
      "E loss:  1.4241106510162354\n",
      "G loss: 2.5431106090545654\n",
      "Training Model  ...\n",
      "E loss:  1.5444051027297974\n",
      "G loss: 2.5233945846557617\n",
      "E loss:  1.5365076065063477\n",
      "G loss: 2.518007755279541\n",
      "E loss:  1.5208604335784912\n",
      "G loss: 2.5029075145721436\n",
      "E loss:  1.5334999561309814\n",
      "G loss: 2.4926130771636963\n",
      "E loss:  1.5046857595443726\n",
      "G loss: 2.4709866046905518\n",
      "Training Model  ...\n",
      "E loss:  1.5164525508880615\n",
      "G loss: 2.46695876121521\n",
      "E loss:  1.5383732318878174\n",
      "G loss: 2.4716784954071045\n",
      "E loss:  1.5665186643600464\n",
      "G loss: 2.4682631492614746\n",
      "E loss:  1.5617729425430298\n",
      "G loss: 2.4607157707214355\n",
      "E loss:  1.5623085498809814\n",
      "G loss: 2.4449782371520996\n",
      "Training Model  ...\n",
      "E loss:  1.6689761877059937\n",
      "G loss: 2.4560184478759766\n",
      "E loss:  1.6773489713668823\n",
      "G loss: 2.4616894721984863\n",
      "E loss:  1.666057825088501\n",
      "G loss: 2.452915668487549\n",
      "E loss:  1.6892999410629272\n",
      "G loss: 2.475578784942627\n",
      "E loss:  1.6731820106506348\n",
      "G loss: 2.489285469055176\n",
      "Training Model  ...\n",
      "E loss:  1.2369109392166138\n",
      "G loss: 2.480607509613037\n",
      "E loss:  1.2372848987579346\n",
      "G loss: 2.4788119792938232\n",
      "E loss:  1.2401609420776367\n",
      "G loss: 2.466595411300659\n",
      "E loss:  1.2616422176361084\n",
      "G loss: 2.453082323074341\n",
      "E loss:  1.2535474300384521\n",
      "G loss: 2.433952808380127\n",
      "Training Model  ...\n",
      "E loss:  1.487140417098999\n",
      "G loss: 2.4271984100341797\n",
      "E loss:  1.5033141374588013\n",
      "G loss: 2.4513559341430664\n",
      "E loss:  1.4879274368286133\n",
      "G loss: 2.4617600440979004\n",
      "E loss:  1.5023534297943115\n",
      "G loss: 2.47021746635437\n",
      "E loss:  1.505092740058899\n",
      "G loss: 2.4796080589294434\n",
      "Training Model  ...\n",
      "E loss:  1.6113266944885254\n",
      "G loss: 2.4830286502838135\n",
      "E loss:  1.5850058794021606\n",
      "G loss: 2.4787027835845947\n",
      "E loss:  1.5810894966125488\n",
      "G loss: 2.467475175857544\n",
      "E loss:  1.5810633897781372\n",
      "G loss: 2.4756572246551514\n",
      "E loss:  1.5840282440185547\n",
      "G loss: 2.4662699699401855\n",
      "Training Model  ...\n",
      "E loss:  1.43766188621521\n",
      "G loss: 2.4582395553588867\n",
      "E loss:  1.4637715816497803\n",
      "G loss: 2.4720592498779297\n",
      "E loss:  1.452921748161316\n",
      "G loss: 2.4836184978485107\n",
      "E loss:  1.4859440326690674\n",
      "G loss: 2.479248046875\n",
      "E loss:  1.4848453998565674\n",
      "G loss: 2.4948034286499023\n",
      "Training Model  ...\n",
      "E loss:  1.6397359371185303\n",
      "G loss: 2.478532075881958\n",
      "E loss:  1.6781561374664307\n",
      "G loss: 2.490013599395752\n",
      "E loss:  1.709223747253418\n",
      "G loss: 2.489715337753296\n",
      "E loss:  1.674615740776062\n",
      "G loss: 2.512181282043457\n",
      "E loss:  1.703113079071045\n",
      "G loss: 2.5270400047302246\n",
      "Training Model  ...\n",
      "E loss:  1.5916175842285156\n",
      "G loss: 2.5240323543548584\n",
      "E loss:  1.5897701978683472\n",
      "G loss: 2.525237560272217\n",
      "E loss:  1.5697379112243652\n",
      "G loss: 2.5240018367767334\n",
      "E loss:  1.5548081398010254\n",
      "G loss: 2.5111422538757324\n",
      "E loss:  1.544106364250183\n",
      "G loss: 2.5261430740356445\n",
      "Training Model  ...\n",
      "E loss:  1.422438621520996\n",
      "G loss: 2.524698257446289\n",
      "E loss:  1.3966885805130005\n",
      "G loss: 2.5127739906311035\n",
      "E loss:  1.3977304697036743\n",
      "G loss: 2.504937171936035\n",
      "E loss:  1.3905349969863892\n",
      "G loss: 2.4856066703796387\n",
      "E loss:  1.3895574808120728\n",
      "G loss: 2.4847264289855957\n",
      "Training Model  ...\n",
      "E loss:  1.3426392078399658\n",
      "G loss: 2.4773073196411133\n",
      "E loss:  1.368989109992981\n",
      "G loss: 2.4946067333221436\n",
      "E loss:  1.3507685661315918\n",
      "G loss: 2.467975616455078\n",
      "E loss:  1.384538173675537\n",
      "G loss: 2.462263584136963\n",
      "E loss:  1.3831897974014282\n",
      "G loss: 2.449756145477295\n",
      "Training Model  ...\n",
      "E loss:  1.6488044261932373\n",
      "G loss: 2.4498488903045654\n",
      "E loss:  1.675491452217102\n",
      "G loss: 2.4447550773620605\n",
      "E loss:  1.692901372909546\n",
      "G loss: 2.4208505153656006\n",
      "E loss:  1.7129406929016113\n",
      "G loss: 2.4269564151763916\n",
      "E loss:  1.7353194952011108\n",
      "G loss: 2.3983876705169678\n",
      "Training Model  ...\n",
      "E loss:  1.4170732498168945\n",
      "G loss: 2.4065043926239014\n",
      "E loss:  1.402078628540039\n",
      "G loss: 2.3993077278137207\n",
      "E loss:  1.4020076990127563\n",
      "G loss: 2.4117369651794434\n",
      "E loss:  1.4515513181686401\n",
      "G loss: 2.4222705364227295\n",
      "E loss:  1.4471909999847412\n",
      "G loss: 2.4349052906036377\n",
      "Training Model  ...\n",
      "E loss:  1.301431655883789\n",
      "G loss: 2.4259121417999268\n",
      "E loss:  1.3290709257125854\n",
      "G loss: 2.4574453830718994\n",
      "E loss:  1.314044713973999\n",
      "G loss: 2.4671850204467773\n",
      "E loss:  1.2980237007141113\n",
      "G loss: 2.4801864624023438\n",
      "E loss:  1.3220725059509277\n",
      "G loss: 2.4746596813201904\n",
      "Training Model  ...\n",
      "E loss:  1.5922658443450928\n",
      "G loss: 2.475708484649658\n",
      "E loss:  1.5730658769607544\n",
      "G loss: 2.4620869159698486\n",
      "E loss:  1.5838390588760376\n",
      "G loss: 2.4408576488494873\n",
      "E loss:  1.578387975692749\n",
      "G loss: 2.433878183364868\n",
      "E loss:  1.5809115171432495\n",
      "G loss: 2.3995144367218018\n",
      "Training Model  ...\n",
      "E loss:  1.5881315469741821\n",
      "G loss: 2.4064507484436035\n",
      "E loss:  1.5803093910217285\n",
      "G loss: 2.422149181365967\n",
      "E loss:  1.5834685564041138\n",
      "G loss: 2.415249824523926\n",
      "E loss:  1.5878843069076538\n",
      "G loss: 2.431281805038452\n",
      "E loss:  1.5871140956878662\n",
      "G loss: 2.455894947052002\n",
      "Training Model  ...\n",
      "E loss:  1.4137334823608398\n",
      "G loss: 2.4630651473999023\n",
      "E loss:  1.406065821647644\n",
      "G loss: 2.456970691680908\n",
      "E loss:  1.4135510921478271\n",
      "G loss: 2.4603118896484375\n",
      "E loss:  1.4176051616668701\n",
      "G loss: 2.459218978881836\n",
      "E loss:  1.3916566371917725\n",
      "G loss: 2.4711949825286865\n",
      "Training Model  ...\n",
      "E loss:  1.4210959672927856\n",
      "G loss: 2.4882519245147705\n",
      "E loss:  1.3948239088058472\n",
      "G loss: 2.5027425289154053\n",
      "E loss:  1.4203821420669556\n",
      "G loss: 2.5170493125915527\n",
      "E loss:  1.4016185998916626\n",
      "G loss: 2.5349929332733154\n",
      "E loss:  1.4166823625564575\n",
      "G loss: 2.545682907104492\n",
      "Training Model  ...\n",
      "E loss:  1.4918168783187866\n",
      "G loss: 2.5632784366607666\n",
      "E loss:  1.5251219272613525\n",
      "G loss: 2.569075107574463\n",
      "E loss:  1.53426992893219\n",
      "G loss: 2.541031837463379\n",
      "E loss:  1.5114165544509888\n",
      "G loss: 2.541872501373291\n",
      "E loss:  1.510514259338379\n",
      "G loss: 2.5310781002044678\n",
      "Training Model  ...\n",
      "E loss:  1.578054666519165\n",
      "G loss: 2.523519992828369\n",
      "E loss:  1.5209801197052002\n",
      "G loss: 2.5522687435150146\n",
      "E loss:  1.5142065286636353\n",
      "G loss: 2.540313243865967\n",
      "E loss:  1.4900798797607422\n",
      "G loss: 2.536132335662842\n",
      "E loss:  1.4847705364227295\n",
      "G loss: 2.548668146133423\n",
      "Training Model  ...\n",
      "E loss:  1.3201771974563599\n",
      "G loss: 2.5597758293151855\n",
      "E loss:  1.344146490097046\n",
      "G loss: 2.534215211868286\n",
      "E loss:  1.3543511629104614\n",
      "G loss: 2.5028014183044434\n",
      "E loss:  1.3302868604660034\n",
      "G loss: 2.4782443046569824\n",
      "E loss:  1.3362165689468384\n",
      "G loss: 2.462456464767456\n",
      "Training Model  ...\n",
      "E loss:  1.61294424533844\n",
      "G loss: 2.4395670890808105\n",
      "E loss:  1.601712942123413\n",
      "G loss: 2.4340708255767822\n",
      "E loss:  1.6284170150756836\n",
      "G loss: 2.4535586833953857\n",
      "E loss:  1.6028859615325928\n",
      "G loss: 2.4690306186676025\n",
      "E loss:  1.6106040477752686\n",
      "G loss: 2.4734246730804443\n",
      "Training Model  ...\n",
      "E loss:  1.4007689952850342\n",
      "G loss: 2.4767072200775146\n",
      "E loss:  1.3967005014419556\n",
      "G loss: 2.46396541595459\n",
      "E loss:  1.3900182247161865\n",
      "G loss: 2.4601352214813232\n",
      "E loss:  1.3830727338790894\n",
      "G loss: 2.4523916244506836\n",
      "E loss:  1.360219955444336\n",
      "G loss: 2.4718565940856934\n",
      "Training Model  ...\n",
      "E loss:  1.4397320747375488\n",
      "G loss: 2.4391350746154785\n",
      "E loss:  1.416227102279663\n",
      "G loss: 2.4265079498291016\n",
      "E loss:  1.4257222414016724\n",
      "G loss: 2.423757553100586\n",
      "E loss:  1.4247651100158691\n",
      "G loss: 2.392693519592285\n",
      "E loss:  1.4272388219833374\n",
      "G loss: 2.3791909217834473\n",
      "Training Model  ...\n",
      "E loss:  1.3830885887145996\n",
      "G loss: 2.3789174556732178\n",
      "E loss:  1.3913158178329468\n",
      "G loss: 2.3596856594085693\n",
      "E loss:  1.3980573415756226\n",
      "G loss: 2.3438143730163574\n",
      "E loss:  1.3751107454299927\n",
      "G loss: 2.3462347984313965\n",
      "E loss:  1.3600696325302124\n",
      "G loss: 2.321956157684326\n",
      "Training Model  ...\n",
      "E loss:  1.3214185237884521\n",
      "G loss: 2.342304229736328\n",
      "E loss:  1.348611831665039\n",
      "G loss: 2.3530094623565674\n",
      "E loss:  1.363440752029419\n",
      "G loss: 2.3741679191589355\n",
      "E loss:  1.3583803176879883\n",
      "G loss: 2.3781938552856445\n",
      "E loss:  1.381011724472046\n",
      "G loss: 2.4053590297698975\n",
      "Training Model  ...\n",
      "E loss:  1.492490291595459\n",
      "G loss: 2.397097110748291\n",
      "E loss:  1.4696295261383057\n",
      "G loss: 2.399935483932495\n",
      "E loss:  1.4875801801681519\n",
      "G loss: 2.4363973140716553\n",
      "E loss:  1.5321142673492432\n",
      "G loss: 2.4430463314056396\n",
      "E loss:  1.5540404319763184\n",
      "G loss: 2.4697844982147217\n",
      "Training Model  ...\n",
      "E loss:  1.5547339916229248\n",
      "G loss: 2.4772768020629883\n",
      "E loss:  1.535865306854248\n",
      "G loss: 2.472005844116211\n",
      "E loss:  1.5263113975524902\n",
      "G loss: 2.4843344688415527\n",
      "E loss:  1.5608294010162354\n",
      "G loss: 2.4589216709136963\n",
      "E loss:  1.5582234859466553\n",
      "G loss: 2.4591152667999268\n",
      "Training Model  ...\n",
      "E loss:  1.4297384023666382\n",
      "G loss: 2.4630112648010254\n",
      "E loss:  1.4469083547592163\n",
      "G loss: 2.471156597137451\n",
      "E loss:  1.4389585256576538\n",
      "G loss: 2.4437639713287354\n",
      "E loss:  1.4146175384521484\n",
      "G loss: 2.426340341567993\n",
      "E loss:  1.4256138801574707\n",
      "G loss: 2.4087533950805664\n",
      "Training Model  ...\n",
      "E loss:  1.5500202178955078\n",
      "G loss: 2.4113969802856445\n",
      "E loss:  1.5462850332260132\n",
      "G loss: 2.401350498199463\n",
      "E loss:  1.5263795852661133\n",
      "G loss: 2.4290683269500732\n",
      "E loss:  1.526200294494629\n",
      "G loss: 2.4151153564453125\n",
      "E loss:  1.5362294912338257\n",
      "G loss: 2.4573185443878174\n",
      "Training Model  ...\n",
      "E loss:  1.3640828132629395\n",
      "G loss: 2.4421229362487793\n",
      "E loss:  1.3840672969818115\n",
      "G loss: 2.418179512023926\n",
      "E loss:  1.362732172012329\n",
      "G loss: 2.4092235565185547\n",
      "E loss:  1.3525898456573486\n",
      "G loss: 2.362809896469116\n",
      "E loss:  1.357591986656189\n",
      "G loss: 2.330594778060913\n",
      "Training Model  ...\n",
      "E loss:  1.469728946685791\n",
      "G loss: 2.3374032974243164\n",
      "E loss:  1.4849896430969238\n",
      "G loss: 2.3192086219787598\n",
      "E loss:  1.4936325550079346\n",
      "G loss: 2.318467378616333\n",
      "E loss:  1.443104863166809\n",
      "G loss: 2.291712760925293\n",
      "E loss:  1.451265573501587\n",
      "G loss: 2.3117713928222656\n",
      "Training Model  ...\n",
      "E loss:  1.3103739023208618\n",
      "G loss: 2.312648057937622\n",
      "E loss:  1.2856779098510742\n",
      "G loss: 2.2881295680999756\n",
      "E loss:  1.2907254695892334\n",
      "G loss: 2.2747256755828857\n",
      "E loss:  1.2782970666885376\n",
      "G loss: 2.2978789806365967\n",
      "E loss:  1.2789032459259033\n",
      "G loss: 2.2735061645507812\n",
      "Training Model  ...\n",
      "E loss:  1.2732927799224854\n",
      "G loss: 2.268709421157837\n",
      "E loss:  1.2691162824630737\n",
      "G loss: 2.2739720344543457\n",
      "E loss:  1.2918689250946045\n",
      "G loss: 2.2844340801239014\n",
      "E loss:  1.2603052854537964\n",
      "G loss: 2.29777455329895\n",
      "E loss:  1.2557636499404907\n",
      "G loss: 2.282972574234009\n",
      "Training Model  ...\n",
      "E loss:  1.5368751287460327\n",
      "G loss: 2.2845780849456787\n",
      "E loss:  1.5304595232009888\n",
      "G loss: 2.3026628494262695\n",
      "E loss:  1.539249062538147\n",
      "G loss: 2.3163065910339355\n",
      "E loss:  1.5460413694381714\n",
      "G loss: 2.33001708984375\n",
      "E loss:  1.5376663208007812\n",
      "G loss: 2.345005512237549\n",
      "Training Model  ...\n",
      "E loss:  1.588911771774292\n",
      "G loss: 2.362572431564331\n",
      "E loss:  1.5699551105499268\n",
      "G loss: 2.3518998622894287\n",
      "E loss:  1.577757716178894\n",
      "G loss: 2.3617727756500244\n",
      "E loss:  1.6120624542236328\n",
      "G loss: 2.353935956954956\n",
      "E loss:  1.60245943069458\n",
      "G loss: 2.3456592559814453\n",
      "Training Model  ...\n",
      "E loss:  1.4464499950408936\n",
      "G loss: 2.3537280559539795\n",
      "E loss:  1.4838730096817017\n",
      "G loss: 2.376018524169922\n",
      "E loss:  1.475855827331543\n",
      "G loss: 2.3924734592437744\n",
      "E loss:  1.468116044998169\n",
      "G loss: 2.4133083820343018\n",
      "E loss:  1.4929533004760742\n",
      "G loss: 2.4478657245635986\n",
      "Training Model  ...\n",
      "E loss:  1.2516433000564575\n",
      "G loss: 2.4589195251464844\n",
      "E loss:  1.2550640106201172\n",
      "G loss: 2.4306278228759766\n",
      "E loss:  1.2904411554336548\n",
      "G loss: 2.422252893447876\n",
      "E loss:  1.3006335496902466\n",
      "G loss: 2.410508632659912\n",
      "E loss:  1.3002808094024658\n",
      "G loss: 2.4225947856903076\n",
      "Training Model  ...\n",
      "E loss:  1.536063313484192\n",
      "G loss: 2.3954877853393555\n",
      "E loss:  1.5283048152923584\n",
      "G loss: 2.379643201828003\n",
      "E loss:  1.5518783330917358\n",
      "G loss: 2.4074854850769043\n",
      "E loss:  1.5404621362686157\n",
      "G loss: 2.3944311141967773\n",
      "E loss:  1.553155541419983\n",
      "G loss: 2.3669965267181396\n",
      "Training Model  ...\n",
      "E loss:  1.597701907157898\n",
      "G loss: 2.370424270629883\n",
      "E loss:  1.594217300415039\n",
      "G loss: 2.381896495819092\n",
      "E loss:  1.5967683792114258\n",
      "G loss: 2.3840925693511963\n",
      "E loss:  1.6159378290176392\n",
      "G loss: 2.382652759552002\n",
      "E loss:  1.5970797538757324\n",
      "G loss: 2.3770716190338135\n",
      "Training Model  ...\n",
      "E loss:  1.4864693880081177\n",
      "G loss: 2.386685609817505\n",
      "E loss:  1.5012482404708862\n",
      "G loss: 2.3957297801971436\n",
      "E loss:  1.474221110343933\n",
      "G loss: 2.408998966217041\n",
      "E loss:  1.4852279424667358\n",
      "G loss: 2.4244933128356934\n",
      "E loss:  1.4701727628707886\n",
      "G loss: 2.4272570610046387\n",
      "Training Model  ...\n",
      "E loss:  1.469709873199463\n",
      "G loss: 2.4641923904418945\n",
      "E loss:  1.4509327411651611\n",
      "G loss: 2.4459900856018066\n",
      "E loss:  1.4440717697143555\n",
      "G loss: 2.442138671875\n",
      "E loss:  1.4663431644439697\n",
      "G loss: 2.4244532585144043\n",
      "E loss:  1.4314498901367188\n",
      "G loss: 2.4275498390197754\n",
      "Training Model  ...\n",
      "E loss:  1.4015462398529053\n",
      "G loss: 2.426145076751709\n",
      "E loss:  1.3777105808258057\n",
      "G loss: 2.402585029602051\n",
      "E loss:  1.387531042098999\n",
      "G loss: 2.37664532661438\n",
      "E loss:  1.400792121887207\n",
      "G loss: 2.346766948699951\n",
      "E loss:  1.416418194770813\n",
      "G loss: 2.307893753051758\n",
      "Training Model  ...\n",
      "E loss:  1.4428011178970337\n",
      "G loss: 2.312351942062378\n",
      "E loss:  1.4358060359954834\n",
      "G loss: 2.2842400074005127\n",
      "E loss:  1.4319053888320923\n",
      "G loss: 2.293412923812866\n",
      "E loss:  1.435506820678711\n",
      "G loss: 2.2749032974243164\n",
      "E loss:  1.4118047952651978\n",
      "G loss: 2.2771501541137695\n",
      "Training Model  ...\n",
      "E loss:  1.4530001878738403\n",
      "G loss: 2.2565276622772217\n",
      "E loss:  1.4569458961486816\n",
      "G loss: 2.26931095123291\n",
      "E loss:  1.45919668674469\n",
      "G loss: 2.283754587173462\n",
      "E loss:  1.44601571559906\n",
      "G loss: 2.3171045780181885\n",
      "E loss:  1.4644865989685059\n",
      "G loss: 2.3122124671936035\n",
      "Training Model  ...\n",
      "E loss:  1.620940923690796\n",
      "G loss: 2.332275629043579\n",
      "E loss:  1.584167718887329\n",
      "G loss: 2.347297191619873\n",
      "E loss:  1.5528974533081055\n",
      "G loss: 2.3640239238739014\n",
      "E loss:  1.5438320636749268\n",
      "G loss: 2.3794710636138916\n",
      "E loss:  1.5133483409881592\n",
      "G loss: 2.4089417457580566\n",
      "Training Model  ...\n",
      "E loss:  1.3083579540252686\n",
      "G loss: 2.389702081680298\n",
      "E loss:  1.2957638502120972\n",
      "G loss: 2.388472557067871\n",
      "E loss:  1.3123178482055664\n",
      "G loss: 2.3624634742736816\n",
      "E loss:  1.3380780220031738\n",
      "G loss: 2.3547635078430176\n",
      "E loss:  1.3616396188735962\n",
      "G loss: 2.3219406604766846\n",
      "Training Model  ...\n",
      "E loss:  1.4150300025939941\n",
      "G loss: 2.328051805496216\n",
      "E loss:  1.3959580659866333\n",
      "G loss: 2.3301782608032227\n",
      "E loss:  1.402464747428894\n",
      "G loss: 2.328889846801758\n",
      "E loss:  1.41499662399292\n",
      "G loss: 2.332336902618408\n",
      "E loss:  1.3872430324554443\n",
      "G loss: 2.3435726165771484\n",
      "Training Model  ...\n",
      "E loss:  1.4027431011199951\n",
      "G loss: 2.33133602142334\n",
      "E loss:  1.369632363319397\n",
      "G loss: 2.317880392074585\n",
      "E loss:  1.3481054306030273\n",
      "G loss: 2.308317184448242\n",
      "E loss:  1.3335833549499512\n",
      "G loss: 2.283592462539673\n",
      "E loss:  1.334876537322998\n",
      "G loss: 2.2538154125213623\n",
      "Training Model  ...\n",
      "E loss:  1.4794871807098389\n",
      "G loss: 2.2603325843811035\n",
      "E loss:  1.464167594909668\n",
      "G loss: 2.2461562156677246\n",
      "E loss:  1.4643412828445435\n",
      "G loss: 2.2599122524261475\n",
      "E loss:  1.4645214080810547\n",
      "G loss: 2.27610182762146\n",
      "E loss:  1.422670841217041\n",
      "G loss: 2.250401258468628\n",
      "Training Model  ...\n",
      "E loss:  1.549168348312378\n",
      "G loss: 2.264389991760254\n",
      "E loss:  1.5482144355773926\n",
      "G loss: 2.252392292022705\n",
      "E loss:  1.5321911573410034\n",
      "G loss: 2.263101816177368\n",
      "E loss:  1.5120748281478882\n",
      "G loss: 2.2563817501068115\n",
      "E loss:  1.5481984615325928\n",
      "G loss: 2.2509827613830566\n",
      "Training Model  ...\n",
      "E loss:  1.268297791481018\n",
      "G loss: 2.261082410812378\n",
      "E loss:  1.2748321294784546\n",
      "G loss: 2.252140998840332\n",
      "E loss:  1.3011143207550049\n",
      "G loss: 2.2652432918548584\n",
      "E loss:  1.3246288299560547\n",
      "G loss: 2.266141176223755\n",
      "E loss:  1.3074148893356323\n",
      "G loss: 2.2547028064727783\n",
      "Training Model  ...\n",
      "E loss:  1.339311957359314\n",
      "G loss: 2.2747185230255127\n",
      "E loss:  1.354631781578064\n",
      "G loss: 2.2722315788269043\n",
      "E loss:  1.3712882995605469\n",
      "G loss: 2.25250506401062\n",
      "E loss:  1.3759335279464722\n",
      "G loss: 2.2631804943084717\n",
      "E loss:  1.3821903467178345\n",
      "G loss: 2.2826485633850098\n",
      "Training Model  ...\n",
      "E loss:  1.3183280229568481\n",
      "G loss: 2.281294345855713\n",
      "E loss:  1.3047049045562744\n",
      "G loss: 2.293588638305664\n",
      "E loss:  1.300368309020996\n",
      "G loss: 2.26407790184021\n",
      "E loss:  1.2954175472259521\n",
      "G loss: 2.257589817047119\n",
      "E loss:  1.2956498861312866\n",
      "G loss: 2.290675163269043\n",
      "Training Model  ...\n",
      "E loss:  1.3416962623596191\n",
      "G loss: 2.26448917388916\n",
      "E loss:  1.3375757932662964\n",
      "G loss: 2.2850842475891113\n",
      "E loss:  1.3467521667480469\n",
      "G loss: 2.2505195140838623\n",
      "E loss:  1.33624267578125\n",
      "G loss: 2.2499425411224365\n",
      "E loss:  1.3523445129394531\n",
      "G loss: 2.2147881984710693\n",
      "Training Model  ...\n",
      "E loss:  1.3725996017456055\n",
      "G loss: 2.233092784881592\n",
      "E loss:  1.3872411251068115\n",
      "G loss: 2.2244677543640137\n",
      "E loss:  1.3946623802185059\n",
      "G loss: 2.2131152153015137\n",
      "E loss:  1.400455355644226\n",
      "G loss: 2.2101662158966064\n",
      "E loss:  1.3995094299316406\n",
      "G loss: 2.18786883354187\n",
      "Training Model  ...\n",
      "E loss:  1.440134882926941\n",
      "G loss: 2.219175338745117\n",
      "E loss:  1.4246768951416016\n",
      "G loss: 2.2088959217071533\n",
      "E loss:  1.4281419515609741\n",
      "G loss: 2.228621244430542\n",
      "E loss:  1.4178504943847656\n",
      "G loss: 2.2325475215911865\n",
      "E loss:  1.4360921382904053\n",
      "G loss: 2.2290215492248535\n",
      "Training Model  ...\n",
      "E loss:  1.4547594785690308\n",
      "G loss: 2.242760181427002\n",
      "E loss:  1.4426928758621216\n",
      "G loss: 2.265094757080078\n",
      "E loss:  1.4401320219039917\n",
      "G loss: 2.257852077484131\n",
      "E loss:  1.4702388048171997\n",
      "G loss: 2.271918296813965\n",
      "E loss:  1.4574856758117676\n",
      "G loss: 2.292555332183838\n",
      "Training Model  ...\n",
      "E loss:  1.2408168315887451\n",
      "G loss: 2.274993419647217\n",
      "E loss:  1.261583685874939\n",
      "G loss: 2.2538766860961914\n",
      "E loss:  1.2561089992523193\n",
      "G loss: 2.203555107116699\n",
      "E loss:  1.2486932277679443\n",
      "G loss: 2.156669855117798\n",
      "E loss:  1.222058653831482\n",
      "G loss: 2.1142642498016357\n",
      "Training Model  ...\n",
      "E loss:  1.4422848224639893\n",
      "G loss: 2.1102726459503174\n",
      "E loss:  1.4218732118606567\n",
      "G loss: 2.138256549835205\n",
      "E loss:  1.4100009202957153\n",
      "G loss: 2.123417377471924\n",
      "E loss:  1.4164893627166748\n",
      "G loss: 2.1253724098205566\n",
      "E loss:  1.4329999685287476\n",
      "G loss: 2.148158073425293\n",
      "Training Model  ...\n",
      "E loss:  1.3534576892852783\n",
      "G loss: 2.1561670303344727\n",
      "E loss:  1.3398427963256836\n",
      "G loss: 2.156553030014038\n",
      "E loss:  1.3660582304000854\n",
      "G loss: 2.135169506072998\n",
      "E loss:  1.3773143291473389\n",
      "G loss: 2.158379554748535\n",
      "E loss:  1.3710498809814453\n",
      "G loss: 2.1654913425445557\n",
      "Training Model  ...\n",
      "E loss:  1.474928379058838\n",
      "G loss: 2.178365707397461\n",
      "E loss:  1.4733819961547852\n",
      "G loss: 2.156555652618408\n",
      "E loss:  1.446345329284668\n",
      "G loss: 2.198367118835449\n",
      "E loss:  1.4537750482559204\n",
      "G loss: 2.174025058746338\n",
      "E loss:  1.4441343545913696\n",
      "G loss: 2.186328649520874\n",
      "Training Model  ...\n",
      "E loss:  1.3994251489639282\n",
      "G loss: 2.183884620666504\n",
      "E loss:  1.3952581882476807\n",
      "G loss: 2.1832332611083984\n",
      "E loss:  1.4050116539001465\n",
      "G loss: 2.181124210357666\n",
      "E loss:  1.4294241666793823\n",
      "G loss: 2.14628005027771\n",
      "E loss:  1.4096965789794922\n",
      "G loss: 2.1385672092437744\n",
      "Training Model  ...\n",
      "E loss:  1.4829895496368408\n",
      "G loss: 2.140505313873291\n",
      "E loss:  1.4505897760391235\n",
      "G loss: 2.1621439456939697\n",
      "E loss:  1.4768725633621216\n",
      "G loss: 2.1922147274017334\n",
      "E loss:  1.4561102390289307\n",
      "G loss: 2.187136650085449\n",
      "E loss:  1.441996693611145\n",
      "G loss: 2.2136499881744385\n",
      "Training Model  ...\n",
      "E loss:  1.5523732900619507\n",
      "G loss: 2.2363836765289307\n",
      "E loss:  1.517336368560791\n",
      "G loss: 2.2450807094573975\n",
      "E loss:  1.5136833190917969\n",
      "G loss: 2.2367327213287354\n",
      "E loss:  1.4913734197616577\n",
      "G loss: 2.2645323276519775\n",
      "E loss:  1.5013129711151123\n",
      "G loss: 2.288137912750244\n",
      "Training Model  ...\n",
      "E loss:  1.441482663154602\n",
      "G loss: 2.2859296798706055\n",
      "E loss:  1.4749342203140259\n",
      "G loss: 2.2658729553222656\n",
      "E loss:  1.4745858907699585\n",
      "G loss: 2.2491142749786377\n",
      "E loss:  1.4953055381774902\n",
      "G loss: 2.245619773864746\n",
      "E loss:  1.473563551902771\n",
      "G loss: 2.198086977005005\n",
      "Training Model  ...\n",
      "E loss:  1.4440686702728271\n",
      "G loss: 2.236395835876465\n",
      "E loss:  1.4515342712402344\n",
      "G loss: 2.2018256187438965\n",
      "E loss:  1.4403393268585205\n",
      "G loss: 2.2082738876342773\n",
      "E loss:  1.4563183784484863\n",
      "G loss: 2.2321431636810303\n",
      "E loss:  1.4626190662384033\n",
      "G loss: 2.223520517349243\n",
      "Training Model  ...\n",
      "E loss:  1.5596938133239746\n",
      "G loss: 2.2246265411376953\n",
      "E loss:  1.5909816026687622\n",
      "G loss: 2.2314510345458984\n",
      "E loss:  1.5581520795822144\n",
      "G loss: 2.2224299907684326\n",
      "E loss:  1.5092133283615112\n",
      "G loss: 2.2300493717193604\n",
      "E loss:  1.5111385583877563\n",
      "G loss: 2.2018814086914062\n",
      "Training Model  ...\n",
      "E loss:  1.318910837173462\n",
      "G loss: 2.217272996902466\n",
      "E loss:  1.3231098651885986\n",
      "G loss: 2.2326698303222656\n",
      "E loss:  1.2955840826034546\n",
      "G loss: 2.2162017822265625\n",
      "E loss:  1.313941478729248\n",
      "G loss: 2.1934521198272705\n",
      "E loss:  1.3267090320587158\n",
      "G loss: 2.188127279281616\n",
      "Training Model  ...\n",
      "E loss:  1.3791898488998413\n",
      "G loss: 2.203836441040039\n",
      "E loss:  1.3795416355133057\n",
      "G loss: 2.182119131088257\n",
      "E loss:  1.3638160228729248\n",
      "G loss: 2.2292628288269043\n",
      "E loss:  1.3852230310440063\n",
      "G loss: 2.2106475830078125\n",
      "E loss:  1.3857651948928833\n",
      "G loss: 2.221330165863037\n",
      "Training Model  ...\n",
      "E loss:  1.3371756076812744\n",
      "G loss: 2.2226696014404297\n",
      "E loss:  1.3407132625579834\n",
      "G loss: 2.211970090866089\n",
      "E loss:  1.360169768333435\n",
      "G loss: 2.2198073863983154\n",
      "E loss:  1.371657371520996\n",
      "G loss: 2.212623119354248\n",
      "E loss:  1.3535510301589966\n",
      "G loss: 2.1824951171875\n",
      "Training Model  ...\n",
      "E loss:  1.356965184211731\n",
      "G loss: 2.1890406608581543\n",
      "E loss:  1.3300176858901978\n",
      "G loss: 2.1789848804473877\n",
      "E loss:  1.3561393022537231\n",
      "G loss: 2.1508030891418457\n",
      "E loss:  1.3535813093185425\n",
      "G loss: 2.145639419555664\n",
      "E loss:  1.3516744375228882\n",
      "G loss: 2.101166009902954\n",
      "Training Model  ...\n",
      "E loss:  1.3060181140899658\n",
      "G loss: 2.111577033996582\n",
      "E loss:  1.3183144330978394\n",
      "G loss: 2.0952792167663574\n",
      "E loss:  1.3482251167297363\n",
      "G loss: 2.110011100769043\n",
      "E loss:  1.345359444618225\n",
      "G loss: 2.119241714477539\n",
      "E loss:  1.359771728515625\n",
      "G loss: 2.120551586151123\n",
      "Training Model  ...\n",
      "E loss:  1.2288373708724976\n",
      "G loss: 2.0990331172943115\n",
      "E loss:  1.2386162281036377\n",
      "G loss: 2.097209930419922\n",
      "E loss:  1.2763540744781494\n",
      "G loss: 2.103700637817383\n",
      "E loss:  1.2656705379486084\n",
      "G loss: 2.0746240615844727\n",
      "E loss:  1.2640186548233032\n",
      "G loss: 2.0640783309936523\n",
      "Training Model  ...\n",
      "E loss:  1.517940640449524\n",
      "G loss: 2.0671093463897705\n",
      "E loss:  1.4888988733291626\n",
      "G loss: 2.0702271461486816\n",
      "E loss:  1.4693472385406494\n",
      "G loss: 2.089399814605713\n",
      "E loss:  1.4631853103637695\n",
      "G loss: 2.123486280441284\n",
      "E loss:  1.455039381980896\n",
      "G loss: 2.1400508880615234\n",
      "Training Model  ...\n",
      "E loss:  1.4015686511993408\n",
      "G loss: 2.1500751972198486\n",
      "E loss:  1.4105043411254883\n",
      "G loss: 2.1350831985473633\n",
      "E loss:  1.3837177753448486\n",
      "G loss: 2.142361640930176\n",
      "E loss:  1.3808445930480957\n",
      "G loss: 2.170433521270752\n",
      "E loss:  1.3960069417953491\n",
      "G loss: 2.189906597137451\n",
      "Training Model  ...\n",
      "E loss:  1.299261212348938\n",
      "G loss: 2.171955108642578\n",
      "E loss:  1.3059883117675781\n",
      "G loss: 2.1626880168914795\n",
      "E loss:  1.3066859245300293\n",
      "G loss: 2.1658716201782227\n",
      "E loss:  1.316336750984192\n",
      "G loss: 2.1527099609375\n",
      "E loss:  1.337052345275879\n",
      "G loss: 2.1461665630340576\n",
      "Training Model  ...\n",
      "E loss:  1.4527451992034912\n",
      "G loss: 2.125002384185791\n",
      "E loss:  1.4337449073791504\n",
      "G loss: 2.106494188308716\n",
      "E loss:  1.4289495944976807\n",
      "G loss: 2.087218999862671\n",
      "E loss:  1.4249006509780884\n",
      "G loss: 2.069631576538086\n",
      "E loss:  1.4009625911712646\n",
      "G loss: 2.0418200492858887\n",
      "Training Model  ...\n",
      "E loss:  1.2759565114974976\n",
      "G loss: 2.0357131958007812\n",
      "E loss:  1.294020175933838\n",
      "G loss: 2.0334203243255615\n",
      "E loss:  1.2778609991073608\n",
      "G loss: 2.0228967666625977\n",
      "E loss:  1.27604079246521\n",
      "G loss: 2.0212080478668213\n",
      "E loss:  1.299989104270935\n",
      "G loss: 2.013972759246826\n",
      "Training Model  ...\n",
      "E loss:  1.5319314002990723\n",
      "G loss: 1.9813120365142822\n",
      "E loss:  1.5613588094711304\n",
      "G loss: 2.015115737915039\n",
      "E loss:  1.5549052953720093\n",
      "G loss: 2.016792058944702\n",
      "E loss:  1.5747215747833252\n",
      "G loss: 2.039458751678467\n",
      "E loss:  1.5877892971038818\n",
      "G loss: 2.0350914001464844\n",
      "Training Model  ...\n",
      "E loss:  1.3056913614273071\n",
      "G loss: 2.020620822906494\n",
      "E loss:  1.3089711666107178\n",
      "G loss: 2.031329870223999\n",
      "E loss:  1.3183791637420654\n",
      "G loss: 2.0513672828674316\n",
      "E loss:  1.338019609451294\n",
      "G loss: 2.0600228309631348\n",
      "E loss:  1.3534460067749023\n",
      "G loss: 2.0542123317718506\n",
      "Training Model  ...\n",
      "E loss:  1.2230271100997925\n",
      "G loss: 2.0743372440338135\n",
      "E loss:  1.2631696462631226\n",
      "G loss: 2.0711803436279297\n",
      "E loss:  1.2565999031066895\n",
      "G loss: 2.054147243499756\n",
      "E loss:  1.2487860918045044\n",
      "G loss: 2.065579414367676\n",
      "E loss:  1.2646465301513672\n",
      "G loss: 2.0937743186950684\n",
      "Training Model  ...\n",
      "E loss:  1.3300573825836182\n",
      "G loss: 2.0670180320739746\n",
      "E loss:  1.3050702810287476\n",
      "G loss: 2.0598702430725098\n",
      "E loss:  1.29844331741333\n",
      "G loss: 2.0320427417755127\n",
      "E loss:  1.2931301593780518\n",
      "G loss: 2.000781774520874\n",
      "E loss:  1.2968559265136719\n",
      "G loss: 1.9523720741271973\n",
      "Training Model  ...\n",
      "E loss:  1.3718016147613525\n",
      "G loss: 1.960737943649292\n",
      "E loss:  1.3453128337860107\n",
      "G loss: 1.9770301580429077\n",
      "E loss:  1.3641678094863892\n",
      "G loss: 2.002363681793213\n",
      "E loss:  1.3509061336517334\n",
      "G loss: 2.0198559761047363\n",
      "E loss:  1.3502155542373657\n",
      "G loss: 2.057723045349121\n",
      "Training Model  ...\n",
      "E loss:  1.3630517721176147\n",
      "G loss: 2.065927743911743\n",
      "E loss:  1.3381656408309937\n",
      "G loss: 2.0499377250671387\n",
      "E loss:  1.3123193979263306\n",
      "G loss: 2.0609896183013916\n",
      "E loss:  1.2845884561538696\n",
      "G loss: 2.041944742202759\n",
      "E loss:  1.2880380153656006\n",
      "G loss: 2.044997215270996\n",
      "Training Model  ...\n",
      "E loss:  1.3605600595474243\n",
      "G loss: 2.0346882343292236\n",
      "E loss:  1.355254888534546\n",
      "G loss: 2.052210569381714\n",
      "E loss:  1.3372211456298828\n",
      "G loss: 2.0399699211120605\n",
      "E loss:  1.318657398223877\n",
      "G loss: 2.0402090549468994\n",
      "E loss:  1.3241254091262817\n",
      "G loss: 2.032841920852661\n",
      "Training Model  ...\n",
      "E loss:  1.2301582098007202\n",
      "G loss: 2.0386009216308594\n",
      "E loss:  1.2160981893539429\n",
      "G loss: 2.0329458713531494\n",
      "E loss:  1.246936321258545\n",
      "G loss: 2.0053701400756836\n",
      "E loss:  1.2484138011932373\n",
      "G loss: 2.006479024887085\n",
      "E loss:  1.2225008010864258\n",
      "G loss: 1.9851678609848022\n",
      "Training Model  ...\n",
      "E loss:  1.344692587852478\n",
      "G loss: 1.981583595275879\n",
      "E loss:  1.3631125688552856\n",
      "G loss: 1.9755306243896484\n",
      "E loss:  1.3874074220657349\n",
      "G loss: 1.9770280122756958\n",
      "E loss:  1.370327115058899\n",
      "G loss: 1.9957764148712158\n",
      "E loss:  1.3844350576400757\n",
      "G loss: 1.9800374507904053\n",
      "Training Model  ...\n",
      "E loss:  1.432908296585083\n",
      "G loss: 1.9969921112060547\n",
      "E loss:  1.4192392826080322\n",
      "G loss: 2.019232988357544\n",
      "E loss:  1.3953797817230225\n",
      "G loss: 2.0208728313446045\n",
      "E loss:  1.4113460779190063\n",
      "G loss: 2.0536458492279053\n",
      "E loss:  1.3802653551101685\n",
      "G loss: 2.0665314197540283\n",
      "Training Model  ...\n",
      "E loss:  1.3945398330688477\n",
      "G loss: 2.053867816925049\n",
      "E loss:  1.3992512226104736\n",
      "G loss: 2.013230323791504\n",
      "E loss:  1.3751208782196045\n",
      "G loss: 2.0607850551605225\n",
      "E loss:  1.3762108087539673\n",
      "G loss: 2.0181074142456055\n",
      "E loss:  1.384321928024292\n",
      "G loss: 2.0140538215637207\n",
      "Training Model  ...\n",
      "E loss:  1.5145021677017212\n",
      "G loss: 2.00738525390625\n",
      "E loss:  1.5536525249481201\n",
      "G loss: 2.020843029022217\n",
      "E loss:  1.560589075088501\n",
      "G loss: 2.0527734756469727\n",
      "E loss:  1.5521793365478516\n",
      "G loss: 2.0648810863494873\n",
      "E loss:  1.5370123386383057\n",
      "G loss: 2.086608648300171\n",
      "Training Model  ...\n",
      "E loss:  1.2959872484207153\n",
      "G loss: 2.0599899291992188\n",
      "E loss:  1.2908693552017212\n",
      "G loss: 2.0621657371520996\n",
      "E loss:  1.2913696765899658\n",
      "G loss: 2.0592753887176514\n",
      "E loss:  1.2727322578430176\n",
      "G loss: 2.0532641410827637\n",
      "E loss:  1.2814443111419678\n",
      "G loss: 2.0308828353881836\n",
      "Training Model  ...\n",
      "E loss:  1.4332826137542725\n",
      "G loss: 2.0250914096832275\n",
      "E loss:  1.415176510810852\n",
      "G loss: 2.016716241836548\n",
      "E loss:  1.4056494235992432\n",
      "G loss: 2.022185802459717\n",
      "E loss:  1.4074268341064453\n",
      "G loss: 2.01625919342041\n",
      "E loss:  1.4264838695526123\n",
      "G loss: 1.9980762004852295\n",
      "Training Model  ...\n",
      "E loss:  1.387921690940857\n",
      "G loss: 2.042421817779541\n",
      "E loss:  1.3825857639312744\n",
      "G loss: 2.008176803588867\n",
      "E loss:  1.370042085647583\n",
      "G loss: 2.003845691680908\n",
      "E loss:  1.3500704765319824\n",
      "G loss: 2.0119194984436035\n",
      "E loss:  1.3696714639663696\n",
      "G loss: 1.9947837591171265\n",
      "Training Model  ...\n",
      "E loss:  1.3968544006347656\n",
      "G loss: 1.9953880310058594\n",
      "E loss:  1.385370135307312\n",
      "G loss: 1.9813121557235718\n",
      "E loss:  1.3869572877883911\n",
      "G loss: 1.9645516872406006\n",
      "E loss:  1.3956776857376099\n",
      "G loss: 1.9380946159362793\n",
      "E loss:  1.4079402685165405\n",
      "G loss: 1.8951283693313599\n",
      "Training Model  ...\n",
      "E loss:  1.2453502416610718\n",
      "G loss: 1.926828384399414\n",
      "E loss:  1.2413134574890137\n",
      "G loss: 1.89263117313385\n",
      "E loss:  1.2510615587234497\n",
      "G loss: 1.911989450454712\n",
      "E loss:  1.2380259037017822\n",
      "G loss: 1.861610770225525\n",
      "E loss:  1.2490200996398926\n",
      "G loss: 1.8768283128738403\n",
      "Training Model  ...\n",
      "E loss:  1.3492164611816406\n",
      "G loss: 1.8601782321929932\n",
      "E loss:  1.3615729808807373\n",
      "G loss: 1.8807878494262695\n",
      "E loss:  1.3592170476913452\n",
      "G loss: 1.905254602432251\n",
      "E loss:  1.3436877727508545\n",
      "G loss: 1.9387774467468262\n",
      "E loss:  1.3549115657806396\n",
      "G loss: 1.9553180932998657\n",
      "Training Model  ...\n",
      "E loss:  1.4546698331832886\n",
      "G loss: 1.9751263856887817\n",
      "E loss:  1.449029564857483\n",
      "G loss: 1.9563127756118774\n",
      "E loss:  1.4636296033859253\n",
      "G loss: 1.9807024002075195\n",
      "E loss:  1.4733926057815552\n",
      "G loss: 2.009572982788086\n",
      "E loss:  1.460781216621399\n",
      "G loss: 2.046591281890869\n",
      "Training Model  ...\n",
      "E loss:  1.3247838020324707\n",
      "G loss: 2.0276191234588623\n",
      "E loss:  1.3109326362609863\n",
      "G loss: 2.0341641902923584\n",
      "E loss:  1.3151659965515137\n",
      "G loss: 1.992190957069397\n",
      "E loss:  1.3076186180114746\n",
      "G loss: 1.9592747688293457\n",
      "E loss:  1.289107084274292\n",
      "G loss: 1.9555270671844482\n",
      "Training Model  ...\n",
      "E loss:  1.386965274810791\n",
      "G loss: 1.9577558040618896\n",
      "E loss:  1.3762226104736328\n",
      "G loss: 1.9410285949707031\n",
      "E loss:  1.3487989902496338\n",
      "G loss: 1.985979437828064\n",
      "E loss:  1.308062195777893\n",
      "G loss: 1.9702943563461304\n",
      "E loss:  1.3157315254211426\n",
      "G loss: 1.98575758934021\n",
      "Training Model  ...\n",
      "E loss:  1.56739342212677\n",
      "G loss: 2.009187936782837\n",
      "E loss:  1.5512319803237915\n",
      "G loss: 1.9806153774261475\n",
      "E loss:  1.5493797063827515\n",
      "G loss: 2.028892993927002\n",
      "E loss:  1.5058951377868652\n",
      "G loss: 2.058290958404541\n",
      "E loss:  1.5002455711364746\n",
      "G loss: 2.095832109451294\n",
      "Training Model  ...\n",
      "E loss:  1.4000585079193115\n",
      "G loss: 2.085839033126831\n",
      "E loss:  1.393507719039917\n",
      "G loss: 2.04005765914917\n",
      "E loss:  1.4457333087921143\n",
      "G loss: 2.0524368286132812\n",
      "E loss:  1.4461772441864014\n",
      "G loss: 2.0121021270751953\n",
      "E loss:  1.4439187049865723\n",
      "G loss: 1.9746527671813965\n",
      "Training Model  ...\n",
      "E loss:  1.3088754415512085\n",
      "G loss: 1.9872058629989624\n",
      "E loss:  1.3206567764282227\n",
      "G loss: 1.977852702140808\n",
      "E loss:  1.3351913690567017\n",
      "G loss: 2.004852771759033\n",
      "E loss:  1.3239877223968506\n",
      "G loss: 1.9772710800170898\n",
      "E loss:  1.3224449157714844\n",
      "G loss: 1.988795518875122\n",
      "Training Model  ...\n",
      "E loss:  1.2447662353515625\n",
      "G loss: 1.9429339170455933\n",
      "E loss:  1.2348158359527588\n",
      "G loss: 1.9580873250961304\n",
      "E loss:  1.2562856674194336\n",
      "G loss: 1.9739253520965576\n",
      "E loss:  1.2526447772979736\n",
      "G loss: 1.9854559898376465\n",
      "E loss:  1.2736296653747559\n",
      "G loss: 1.9670610427856445\n",
      "Training Model  ...\n",
      "E loss:  1.263461947441101\n",
      "G loss: 1.949521541595459\n",
      "E loss:  1.2613540887832642\n",
      "G loss: 1.9400150775909424\n",
      "E loss:  1.2798432111740112\n",
      "G loss: 1.9332188367843628\n",
      "E loss:  1.282245397567749\n",
      "G loss: 1.8957509994506836\n",
      "E loss:  1.233949899673462\n",
      "G loss: 1.8747291564941406\n",
      "Training Model  ...\n",
      "E loss:  1.2571489810943604\n",
      "G loss: 1.8793519735336304\n",
      "E loss:  1.2573491334915161\n",
      "G loss: 1.8745720386505127\n",
      "E loss:  1.2871687412261963\n",
      "G loss: 1.8846499919891357\n",
      "E loss:  1.2801589965820312\n",
      "G loss: 1.8631913661956787\n",
      "E loss:  1.2835794687271118\n",
      "G loss: 1.8581429719924927\n",
      "Training Model  ...\n",
      "E loss:  1.279724359512329\n",
      "G loss: 1.85750412940979\n",
      "E loss:  1.2921518087387085\n",
      "G loss: 1.8615705966949463\n",
      "E loss:  1.2670986652374268\n",
      "G loss: 1.8306013345718384\n",
      "E loss:  1.2867686748504639\n",
      "G loss: 1.882358193397522\n",
      "E loss:  1.3159148693084717\n",
      "G loss: 1.8729251623153687\n",
      "Training Model  ...\n",
      "E loss:  1.5120548009872437\n",
      "G loss: 1.8903553485870361\n",
      "E loss:  1.5362573862075806\n",
      "G loss: 1.8797690868377686\n",
      "E loss:  1.5307420492172241\n",
      "G loss: 1.8894773721694946\n",
      "E loss:  1.5226421356201172\n",
      "G loss: 1.89181649684906\n",
      "E loss:  1.4932126998901367\n",
      "G loss: 1.9159568548202515\n",
      "Training Model  ...\n",
      "E loss:  1.3840296268463135\n",
      "G loss: 1.8946994543075562\n",
      "E loss:  1.3806977272033691\n",
      "G loss: 1.8827909231185913\n",
      "E loss:  1.3912794589996338\n",
      "G loss: 1.879256248474121\n",
      "E loss:  1.378442406654358\n",
      "G loss: 1.9184004068374634\n",
      "E loss:  1.394439458847046\n",
      "G loss: 1.8654260635375977\n",
      "Training Model  ...\n",
      "E loss:  1.419140338897705\n",
      "G loss: 1.8757405281066895\n",
      "E loss:  1.4167250394821167\n",
      "G loss: 1.8578535318374634\n",
      "E loss:  1.4566868543624878\n",
      "G loss: 1.8680275678634644\n",
      "E loss:  1.4639948606491089\n",
      "G loss: 1.9005262851715088\n",
      "E loss:  1.4700323343276978\n",
      "G loss: 1.894318699836731\n",
      "Training Model  ...\n",
      "E loss:  1.2632399797439575\n",
      "G loss: 1.8857170343399048\n",
      "E loss:  1.29020094871521\n",
      "G loss: 1.8943464756011963\n",
      "E loss:  1.306594729423523\n",
      "G loss: 1.9099903106689453\n",
      "E loss:  1.329776406288147\n",
      "G loss: 1.9196672439575195\n",
      "E loss:  1.3342385292053223\n",
      "G loss: 1.9223016500473022\n",
      "Training Model  ...\n",
      "E loss:  1.441441535949707\n",
      "G loss: 1.9261201620101929\n",
      "E loss:  1.4383844137191772\n",
      "G loss: 1.9539227485656738\n",
      "E loss:  1.421911597251892\n",
      "G loss: 1.9656659364700317\n",
      "E loss:  1.4091737270355225\n",
      "G loss: 1.972061276435852\n",
      "E loss:  1.4118109941482544\n",
      "G loss: 2.0279529094696045\n",
      "Training Model  ...\n",
      "E loss:  1.304559588432312\n",
      "G loss: 1.9804606437683105\n",
      "E loss:  1.3307701349258423\n",
      "G loss: 2.0469160079956055\n",
      "E loss:  1.3467596769332886\n",
      "G loss: 2.0184009075164795\n",
      "E loss:  1.3101730346679688\n",
      "G loss: 2.015632390975952\n",
      "E loss:  1.3179954290390015\n",
      "G loss: 2.044231414794922\n",
      "Training Model  ...\n",
      "E loss:  1.2561109066009521\n",
      "G loss: 2.0455801486968994\n",
      "E loss:  1.240357518196106\n",
      "G loss: 2.0363340377807617\n",
      "E loss:  1.2135709524154663\n",
      "G loss: 2.0043022632598877\n",
      "E loss:  1.1985293626785278\n",
      "G loss: 2.006845712661743\n",
      "E loss:  1.2168211936950684\n",
      "G loss: 1.9542784690856934\n",
      "Training Model  ...\n",
      "E loss:  1.2419068813323975\n",
      "G loss: 1.9736617803573608\n",
      "E loss:  1.2407160997390747\n",
      "G loss: 1.9457345008850098\n",
      "E loss:  1.2345834970474243\n",
      "G loss: 1.9594734907150269\n",
      "E loss:  1.2632509469985962\n",
      "G loss: 1.974806308746338\n",
      "E loss:  1.251232624053955\n",
      "G loss: 1.954766035079956\n",
      "Training Model  ...\n",
      "E loss:  1.3340167999267578\n",
      "G loss: 1.9459501504898071\n",
      "E loss:  1.3225831985473633\n",
      "G loss: 1.9492863416671753\n",
      "E loss:  1.328456163406372\n",
      "G loss: 1.9462547302246094\n",
      "E loss:  1.3613579273223877\n",
      "G loss: 1.9592273235321045\n",
      "E loss:  1.407991886138916\n",
      "G loss: 1.9909744262695312\n",
      "Training Model  ...\n",
      "E loss:  1.415900468826294\n",
      "G loss: 1.9537392854690552\n",
      "E loss:  1.370288372039795\n",
      "G loss: 1.973625659942627\n",
      "E loss:  1.3627541065216064\n",
      "G loss: 1.9249744415283203\n",
      "E loss:  1.3743093013763428\n",
      "G loss: 1.9308992624282837\n",
      "E loss:  1.3665404319763184\n",
      "G loss: 1.9203152656555176\n",
      "Training Model  ...\n",
      "E loss:  1.3217871189117432\n",
      "G loss: 1.9132726192474365\n",
      "E loss:  1.3007303476333618\n",
      "G loss: 1.9195013046264648\n",
      "E loss:  1.2982581853866577\n",
      "G loss: 1.8993818759918213\n",
      "E loss:  1.285601258277893\n",
      "G loss: 1.911116361618042\n",
      "E loss:  1.2613505125045776\n",
      "G loss: 1.9266835451126099\n",
      "Training Model  ...\n",
      "E loss:  1.3687546253204346\n",
      "G loss: 1.8998608589172363\n",
      "E loss:  1.371159553527832\n",
      "G loss: 1.8931041955947876\n",
      "E loss:  1.3625205755233765\n",
      "G loss: 1.883115530014038\n",
      "E loss:  1.348003625869751\n",
      "G loss: 1.8643648624420166\n",
      "E loss:  1.3423893451690674\n",
      "G loss: 1.8735013008117676\n",
      "Training Model  ...\n",
      "E loss:  1.293778896331787\n",
      "G loss: 1.8217546939849854\n",
      "E loss:  1.3089699745178223\n",
      "G loss: 1.8607065677642822\n",
      "E loss:  1.2763181924819946\n",
      "G loss: 1.8615405559539795\n",
      "E loss:  1.2756717205047607\n",
      "G loss: 1.8644158840179443\n",
      "E loss:  1.2754932641983032\n",
      "G loss: 1.8683867454528809\n",
      "Training Model  ...\n",
      "E loss:  1.3760424852371216\n",
      "G loss: 1.8766217231750488\n",
      "E loss:  1.3741194009780884\n",
      "G loss: 1.868910551071167\n",
      "E loss:  1.3543949127197266\n",
      "G loss: 1.9332823753356934\n",
      "E loss:  1.3745324611663818\n",
      "G loss: 1.9235613346099854\n",
      "E loss:  1.3731796741485596\n",
      "G loss: 1.9764962196350098\n",
      "Training Model  ...\n",
      "E loss:  1.3759664297103882\n",
      "G loss: 1.9858702421188354\n",
      "E loss:  1.3641036748886108\n",
      "G loss: 2.0180065631866455\n",
      "E loss:  1.4100022315979004\n",
      "G loss: 1.960593342781067\n",
      "E loss:  1.3940670490264893\n",
      "G loss: 1.9419633150100708\n",
      "E loss:  1.375539779663086\n",
      "G loss: 1.9575035572052002\n",
      "Training Model  ...\n",
      "E loss:  1.3012968301773071\n",
      "G loss: 1.934120535850525\n",
      "E loss:  1.3210768699645996\n",
      "G loss: 1.933181881904602\n",
      "E loss:  1.311790943145752\n",
      "G loss: 1.8805768489837646\n",
      "E loss:  1.3322834968566895\n",
      "G loss: 1.849617838859558\n",
      "E loss:  1.3351175785064697\n",
      "G loss: 1.8131811618804932\n",
      "Training Model  ...\n",
      "E loss:  1.3270189762115479\n",
      "G loss: 1.8202426433563232\n",
      "E loss:  1.3182480335235596\n",
      "G loss: 1.8047901391983032\n",
      "E loss:  1.3539718389511108\n",
      "G loss: 1.7926926612854004\n",
      "E loss:  1.3364533185958862\n",
      "G loss: 1.8282179832458496\n",
      "E loss:  1.3461344242095947\n",
      "G loss: 1.8122785091400146\n",
      "Training Model  ...\n",
      "E loss:  1.3842562437057495\n",
      "G loss: 1.8241252899169922\n",
      "E loss:  1.3873803615570068\n",
      "G loss: 1.8264683485031128\n",
      "E loss:  1.3769268989562988\n",
      "G loss: 1.7847838401794434\n",
      "E loss:  1.3925626277923584\n",
      "G loss: 1.7791154384613037\n",
      "E loss:  1.3899790048599243\n",
      "G loss: 1.8015962839126587\n",
      "Training Model  ...\n",
      "E loss:  1.204221248626709\n",
      "G loss: 1.822480320930481\n",
      "E loss:  1.1839213371276855\n",
      "G loss: 1.8189496994018555\n",
      "E loss:  1.2191503047943115\n",
      "G loss: 1.8086824417114258\n",
      "E loss:  1.2287578582763672\n",
      "G loss: 1.8265591859817505\n",
      "E loss:  1.2257672548294067\n",
      "G loss: 1.8603609800338745\n",
      "Training Model  ...\n",
      "E loss:  1.3131928443908691\n",
      "G loss: 1.8757089376449585\n",
      "E loss:  1.3034489154815674\n",
      "G loss: 1.8768326044082642\n",
      "E loss:  1.2771456241607666\n",
      "G loss: 1.935258150100708\n",
      "E loss:  1.2928482294082642\n",
      "G loss: 1.9649194478988647\n",
      "E loss:  1.311173677444458\n",
      "G loss: 2.0037732124328613\n",
      "Training Model  ...\n",
      "E loss:  1.2299165725708008\n",
      "G loss: 1.9971567392349243\n",
      "E loss:  1.2586874961853027\n",
      "G loss: 1.952303409576416\n",
      "E loss:  1.2302649021148682\n",
      "G loss: 1.9123802185058594\n",
      "E loss:  1.2348220348358154\n",
      "G loss: 1.876803994178772\n",
      "E loss:  1.2110686302185059\n",
      "G loss: 1.8220096826553345\n",
      "Training Model  ...\n",
      "E loss:  1.269436240196228\n",
      "G loss: 1.7926743030548096\n",
      "E loss:  1.2726480960845947\n",
      "G loss: 1.814302921295166\n",
      "E loss:  1.2322622537612915\n",
      "G loss: 1.8201922178268433\n",
      "E loss:  1.256036639213562\n",
      "G loss: 1.8359427452087402\n",
      "E loss:  1.230435848236084\n",
      "G loss: 1.8822863101959229\n",
      "Training Model  ...\n",
      "E loss:  1.3424859046936035\n",
      "G loss: 1.884354591369629\n",
      "E loss:  1.3220850229263306\n",
      "G loss: 1.8846572637557983\n",
      "E loss:  1.3391762971878052\n",
      "G loss: 1.884572982788086\n",
      "E loss:  1.3422921895980835\n",
      "G loss: 1.9049214124679565\n",
      "E loss:  1.3374996185302734\n",
      "G loss: 1.9025603532791138\n",
      "Training Model  ...\n",
      "E loss:  1.3265122175216675\n",
      "G loss: 1.9134243726730347\n",
      "E loss:  1.3267409801483154\n",
      "G loss: 1.8771270513534546\n",
      "E loss:  1.3383313417434692\n",
      "G loss: 1.8572477102279663\n",
      "E loss:  1.3581538200378418\n",
      "G loss: 1.8908740282058716\n",
      "E loss:  1.349373698234558\n",
      "G loss: 1.7986353635787964\n",
      "Training Model  ...\n",
      "E loss:  1.3759562969207764\n",
      "G loss: 1.8298829793930054\n",
      "E loss:  1.3575844764709473\n",
      "G loss: 1.8595850467681885\n",
      "E loss:  1.3492436408996582\n",
      "G loss: 1.889760136604309\n",
      "E loss:  1.3561965227127075\n",
      "G loss: 1.9348015785217285\n",
      "E loss:  1.3721630573272705\n",
      "G loss: 1.942749261856079\n",
      "Training Model  ...\n",
      "E loss:  1.348242998123169\n",
      "G loss: 1.975132703781128\n",
      "E loss:  1.3416101932525635\n",
      "G loss: 1.914811611175537\n",
      "E loss:  1.3359216451644897\n",
      "G loss: 1.9113935232162476\n",
      "E loss:  1.357364535331726\n",
      "G loss: 1.84676194190979\n",
      "E loss:  1.3257660865783691\n",
      "G loss: 1.7926878929138184\n",
      "Training Model  ...\n",
      "E loss:  1.2834045886993408\n",
      "G loss: 1.8301844596862793\n",
      "E loss:  1.2816414833068848\n",
      "G loss: 1.811368465423584\n",
      "E loss:  1.2751398086547852\n",
      "G loss: 1.8106377124786377\n",
      "E loss:  1.2720003128051758\n",
      "G loss: 1.8281358480453491\n",
      "E loss:  1.249206781387329\n",
      "G loss: 1.803108811378479\n",
      "Training Model  ...\n",
      "E loss:  1.2829430103302002\n",
      "G loss: 1.8219988346099854\n",
      "E loss:  1.289926528930664\n",
      "G loss: 1.7975742816925049\n",
      "E loss:  1.3010916709899902\n",
      "G loss: 1.816083312034607\n",
      "E loss:  1.2816249132156372\n",
      "G loss: 1.85110604763031\n",
      "E loss:  1.2951908111572266\n",
      "G loss: 1.8375020027160645\n",
      "Training Model  ...\n",
      "E loss:  1.1921875476837158\n",
      "G loss: 1.859053134918213\n",
      "E loss:  1.1680190563201904\n",
      "G loss: 1.8147172927856445\n",
      "E loss:  1.164072036743164\n",
      "G loss: 1.843491554260254\n",
      "E loss:  1.1658471822738647\n",
      "G loss: 1.8133268356323242\n",
      "E loss:  1.144015908241272\n",
      "G loss: 1.7698872089385986\n",
      "Training Model  ...\n",
      "E loss:  1.1818127632141113\n",
      "G loss: 1.7339794635772705\n",
      "E loss:  1.163360595703125\n",
      "G loss: 1.77109694480896\n",
      "E loss:  1.1724340915679932\n",
      "G loss: 1.764139175415039\n",
      "E loss:  1.1604093313217163\n",
      "G loss: 1.7734678983688354\n",
      "E loss:  1.182509422302246\n",
      "G loss: 1.7537193298339844\n",
      "Training Model  ...\n",
      "E loss:  1.205478549003601\n",
      "G loss: 1.7534884214401245\n",
      "E loss:  1.1860541105270386\n",
      "G loss: 1.7269771099090576\n",
      "E loss:  1.1659218072891235\n",
      "G loss: 1.7358098030090332\n",
      "E loss:  1.198068618774414\n",
      "G loss: 1.7299880981445312\n",
      "E loss:  1.1872020959854126\n",
      "G loss: 1.7418057918548584\n",
      "Training Model  ...\n",
      "E loss:  1.3438009023666382\n",
      "G loss: 1.7518341541290283\n",
      "E loss:  1.331817865371704\n",
      "G loss: 1.755688190460205\n",
      "E loss:  1.3275607824325562\n",
      "G loss: 1.773859977722168\n",
      "E loss:  1.3037623167037964\n",
      "G loss: 1.8000361919403076\n",
      "E loss:  1.3198994398117065\n",
      "G loss: 1.7904146909713745\n",
      "Training Model  ...\n",
      "E loss:  1.2404406070709229\n",
      "G loss: 1.8113361597061157\n",
      "E loss:  1.2476952075958252\n",
      "G loss: 1.7894203662872314\n",
      "E loss:  1.2369669675827026\n",
      "G loss: 1.7672865390777588\n",
      "E loss:  1.269652247428894\n",
      "G loss: 1.7605035305023193\n",
      "E loss:  1.2557899951934814\n",
      "G loss: 1.7720398902893066\n",
      "Training Model  ...\n",
      "E loss:  1.1428855657577515\n",
      "G loss: 1.7673426866531372\n",
      "E loss:  1.1403241157531738\n",
      "G loss: 1.7903999090194702\n",
      "E loss:  1.1357958316802979\n",
      "G loss: 1.8341025114059448\n",
      "E loss:  1.152574062347412\n",
      "G loss: 1.8323054313659668\n",
      "E loss:  1.1316273212432861\n",
      "G loss: 1.8632577657699585\n",
      "Training Model  ...\n",
      "E loss:  1.1972072124481201\n",
      "G loss: 1.8679533004760742\n",
      "E loss:  1.1828347444534302\n",
      "G loss: 1.905310869216919\n",
      "E loss:  1.1784049272537231\n",
      "G loss: 1.87882399559021\n",
      "E loss:  1.2194863557815552\n",
      "G loss: 1.902054786682129\n",
      "E loss:  1.2251086235046387\n",
      "G loss: 1.883211612701416\n",
      "Training Model  ...\n",
      "E loss:  1.36918044090271\n",
      "G loss: 1.8771510124206543\n",
      "E loss:  1.3777308464050293\n",
      "G loss: 1.8857321739196777\n",
      "E loss:  1.408372402191162\n",
      "G loss: 1.862630009651184\n",
      "E loss:  1.4131158590316772\n",
      "G loss: 1.9150002002716064\n",
      "E loss:  1.4049270153045654\n",
      "G loss: 1.8827488422393799\n",
      "Training Model  ...\n",
      "E loss:  1.1250159740447998\n",
      "G loss: 1.8598904609680176\n",
      "E loss:  1.1361387968063354\n",
      "G loss: 1.8727748394012451\n",
      "E loss:  1.1352969408035278\n",
      "G loss: 1.848533034324646\n",
      "E loss:  1.0977327823638916\n",
      "G loss: 1.8055877685546875\n",
      "E loss:  1.1038413047790527\n",
      "G loss: 1.7671833038330078\n",
      "Training Model  ...\n",
      "E loss:  1.1972105503082275\n",
      "G loss: 1.758040189743042\n",
      "E loss:  1.1629254817962646\n",
      "G loss: 1.7264572381973267\n",
      "E loss:  1.1510727405548096\n",
      "G loss: 1.761362910270691\n",
      "E loss:  1.1609084606170654\n",
      "G loss: 1.7413055896759033\n",
      "E loss:  1.1574665307998657\n",
      "G loss: 1.6906116008758545\n",
      "Training Model  ...\n",
      "E loss:  1.34010910987854\n",
      "G loss: 1.7172404527664185\n",
      "E loss:  1.3293911218643188\n",
      "G loss: 1.6748474836349487\n",
      "E loss:  1.3286162614822388\n",
      "G loss: 1.6820123195648193\n",
      "E loss:  1.2966773509979248\n",
      "G loss: 1.6825016736984253\n",
      "E loss:  1.3108290433883667\n",
      "G loss: 1.6995797157287598\n",
      "Training Model  ...\n",
      "E loss:  1.3967795372009277\n",
      "G loss: 1.681845784187317\n",
      "E loss:  1.3608626127243042\n",
      "G loss: 1.7312064170837402\n",
      "E loss:  1.3392102718353271\n",
      "G loss: 1.699231505393982\n",
      "E loss:  1.3028849363327026\n",
      "G loss: 1.7086725234985352\n",
      "E loss:  1.3597056865692139\n",
      "G loss: 1.7442982196807861\n",
      "Training Model  ...\n",
      "E loss:  1.178810715675354\n",
      "G loss: 1.7378886938095093\n",
      "E loss:  1.1643545627593994\n",
      "G loss: 1.7432020902633667\n",
      "E loss:  1.167186975479126\n",
      "G loss: 1.7893754243850708\n",
      "E loss:  1.1644389629364014\n",
      "G loss: 1.7806179523468018\n",
      "E loss:  1.1533913612365723\n",
      "G loss: 1.7591317892074585\n",
      "Training Model  ...\n",
      "E loss:  1.3601303100585938\n",
      "G loss: 1.7732138633728027\n",
      "E loss:  1.3438959121704102\n",
      "G loss: 1.8162181377410889\n",
      "E loss:  1.3606194257736206\n",
      "G loss: 1.8014637231826782\n",
      "E loss:  1.3617736101150513\n",
      "G loss: 1.83240807056427\n",
      "E loss:  1.3521816730499268\n",
      "G loss: 1.8502918481826782\n",
      "Training Model  ...\n",
      "E loss:  1.3165770769119263\n",
      "G loss: 1.8599482774734497\n",
      "E loss:  1.2753207683563232\n",
      "G loss: 1.8621774911880493\n",
      "E loss:  1.2772655487060547\n",
      "G loss: 1.8803166151046753\n",
      "E loss:  1.256850242614746\n",
      "G loss: 1.8849619626998901\n",
      "E loss:  1.2422337532043457\n",
      "G loss: 1.9369077682495117\n",
      "Training Model  ...\n",
      "E loss:  1.3997451066970825\n",
      "G loss: 1.9292107820510864\n",
      "E loss:  1.3889962434768677\n",
      "G loss: 1.9260488748550415\n",
      "E loss:  1.3625339269638062\n",
      "G loss: 1.9403434991836548\n",
      "E loss:  1.3533296585083008\n",
      "G loss: 1.8892594575881958\n",
      "E loss:  1.3531415462493896\n",
      "G loss: 1.88132905960083\n",
      "Training Model  ...\n",
      "E loss:  1.2206799983978271\n",
      "G loss: 1.9121885299682617\n",
      "E loss:  1.2213761806488037\n",
      "G loss: 1.8893629312515259\n",
      "E loss:  1.2497098445892334\n",
      "G loss: 1.9403942823410034\n",
      "E loss:  1.2199599742889404\n",
      "G loss: 1.93168306350708\n",
      "E loss:  1.2175943851470947\n",
      "G loss: 1.9411754608154297\n",
      "Training Model  ...\n",
      "E loss:  1.2901219129562378\n",
      "G loss: 1.9390952587127686\n",
      "E loss:  1.2899905443191528\n",
      "G loss: 1.900962233543396\n",
      "E loss:  1.300929069519043\n",
      "G loss: 1.8315235376358032\n",
      "E loss:  1.287814736366272\n",
      "G loss: 1.7785534858703613\n",
      "E loss:  1.2987785339355469\n",
      "G loss: 1.739555835723877\n",
      "Training Model  ...\n",
      "E loss:  1.3754345178604126\n",
      "G loss: 1.7470958232879639\n",
      "E loss:  1.38714599609375\n",
      "G loss: 1.8344806432724\n",
      "E loss:  1.3727792501449585\n",
      "G loss: 1.8198997974395752\n",
      "E loss:  1.3740803003311157\n",
      "G loss: 1.8562583923339844\n",
      "E loss:  1.3459415435791016\n",
      "G loss: 1.9387391805648804\n",
      "Training Model  ...\n",
      "E loss:  1.2558743953704834\n",
      "G loss: 1.9039959907531738\n",
      "E loss:  1.2651454210281372\n",
      "G loss: 1.8453280925750732\n",
      "E loss:  1.2653532028198242\n",
      "G loss: 1.8307125568389893\n",
      "E loss:  1.2380765676498413\n",
      "G loss: 1.8311476707458496\n",
      "E loss:  1.2362594604492188\n",
      "G loss: 1.7176170349121094\n",
      "Training Model  ...\n",
      "E loss:  1.3471343517303467\n",
      "G loss: 1.7731294631958008\n",
      "E loss:  1.3412480354309082\n",
      "G loss: 1.7750664949417114\n",
      "E loss:  1.3364334106445312\n",
      "G loss: 1.7642730474472046\n",
      "E loss:  1.3553886413574219\n",
      "G loss: 1.707504153251648\n",
      "E loss:  1.360486626625061\n",
      "G loss: 1.6719812154769897\n",
      "Training Model  ...\n",
      "E loss:  1.2274372577667236\n",
      "G loss: 1.763645052909851\n",
      "E loss:  1.2045146226882935\n",
      "G loss: 1.7229479551315308\n",
      "E loss:  1.2159162759780884\n",
      "G loss: 1.7182469367980957\n",
      "E loss:  1.231338620185852\n",
      "G loss: 1.6934491395950317\n",
      "E loss:  1.218345046043396\n",
      "G loss: 1.68361496925354\n",
      "Training Model  ...\n",
      "E loss:  1.2290267944335938\n",
      "G loss: 1.7006487846374512\n",
      "E loss:  1.2416421175003052\n",
      "G loss: 1.7142143249511719\n",
      "E loss:  1.2422881126403809\n",
      "G loss: 1.7548691034317017\n",
      "E loss:  1.2559267282485962\n",
      "G loss: 1.7310370206832886\n",
      "E loss:  1.2770756483078003\n",
      "G loss: 1.7590124607086182\n",
      "Training Model  ...\n",
      "E loss:  1.1607286930084229\n",
      "G loss: 1.7026957273483276\n",
      "E loss:  1.1319212913513184\n",
      "G loss: 1.7539373636245728\n",
      "E loss:  1.1549742221832275\n",
      "G loss: 1.79035222530365\n",
      "E loss:  1.1521724462509155\n",
      "G loss: 1.7741674184799194\n",
      "E loss:  1.1317648887634277\n",
      "G loss: 1.8049120903015137\n",
      "Training Model  ...\n",
      "E loss:  1.2000640630722046\n",
      "G loss: 1.806620478630066\n",
      "E loss:  1.238341212272644\n",
      "G loss: 1.8002066612243652\n",
      "E loss:  1.240923523902893\n",
      "G loss: 1.8169329166412354\n",
      "E loss:  1.2258992195129395\n",
      "G loss: 1.8303897380828857\n",
      "E loss:  1.194079875946045\n",
      "G loss: 1.858380675315857\n",
      "Training Model  ...\n",
      "E loss:  1.2491259574890137\n",
      "G loss: 1.8466370105743408\n",
      "E loss:  1.2179639339447021\n",
      "G loss: 1.897796630859375\n",
      "E loss:  1.228549599647522\n",
      "G loss: 1.8652153015136719\n",
      "E loss:  1.230756163597107\n",
      "G loss: 1.7858295440673828\n",
      "E loss:  1.2164517641067505\n",
      "G loss: 1.7813763618469238\n",
      "Training Model  ...\n",
      "E loss:  1.2144334316253662\n",
      "G loss: 1.801743507385254\n",
      "E loss:  1.2051268815994263\n",
      "G loss: 1.8043224811553955\n",
      "E loss:  1.1699117422103882\n",
      "G loss: 1.815340518951416\n",
      "E loss:  1.145959734916687\n",
      "G loss: 1.782222867012024\n",
      "E loss:  1.1203802824020386\n",
      "G loss: 1.8151966333389282\n",
      "Training Model  ...\n",
      "E loss:  1.044837236404419\n",
      "G loss: 1.8039507865905762\n",
      "E loss:  1.0660359859466553\n",
      "G loss: 1.7851263284683228\n",
      "E loss:  1.0769548416137695\n",
      "G loss: 1.767296314239502\n",
      "E loss:  1.093078851699829\n",
      "G loss: 1.7661757469177246\n",
      "E loss:  1.0919119119644165\n",
      "G loss: 1.7198026180267334\n",
      "Training Model  ...\n",
      "E loss:  1.183850646018982\n",
      "G loss: 1.7200640439987183\n",
      "E loss:  1.1852049827575684\n",
      "G loss: 1.721120834350586\n",
      "E loss:  1.174062728881836\n",
      "G loss: 1.771317720413208\n",
      "E loss:  1.178605556488037\n",
      "G loss: 1.8039668798446655\n",
      "E loss:  1.173920750617981\n",
      "G loss: 1.750749111175537\n",
      "Training Model  ...\n",
      "E loss:  1.2708394527435303\n",
      "G loss: 1.7640528678894043\n",
      "E loss:  1.2650688886642456\n",
      "G loss: 1.7931668758392334\n",
      "E loss:  1.2600706815719604\n",
      "G loss: 1.7993329763412476\n",
      "E loss:  1.2574636936187744\n",
      "G loss: 1.8040478229522705\n",
      "E loss:  1.2414474487304688\n",
      "G loss: 1.7672216892242432\n",
      "Training Model  ...\n",
      "E loss:  1.1900651454925537\n",
      "G loss: 1.8256479501724243\n",
      "E loss:  1.1679388284683228\n",
      "G loss: 1.7757716178894043\n",
      "E loss:  1.173844575881958\n",
      "G loss: 1.722057580947876\n",
      "E loss:  1.1783320903778076\n",
      "G loss: 1.7101656198501587\n",
      "E loss:  1.163204550743103\n",
      "G loss: 1.6870732307434082\n",
      "Training Model  ...\n",
      "E loss:  1.0661523342132568\n",
      "G loss: 1.7297948598861694\n",
      "E loss:  1.0737725496292114\n",
      "G loss: 1.7052022218704224\n",
      "E loss:  1.0928751230239868\n",
      "G loss: 1.7124016284942627\n",
      "E loss:  1.0930395126342773\n",
      "G loss: 1.7095770835876465\n",
      "E loss:  1.0890707969665527\n",
      "G loss: 1.713056206703186\n",
      "Training Model  ...\n",
      "E loss:  1.1373289823532104\n",
      "G loss: 1.7008379697799683\n",
      "E loss:  1.1141451597213745\n",
      "G loss: 1.758015513420105\n",
      "E loss:  1.1075525283813477\n",
      "G loss: 1.7708969116210938\n",
      "E loss:  1.1022732257843018\n",
      "G loss: 1.777614712715149\n",
      "E loss:  1.080775260925293\n",
      "G loss: 1.759781837463379\n",
      "Training Model  ...\n",
      "E loss:  1.1961538791656494\n",
      "G loss: 1.7474327087402344\n",
      "E loss:  1.2004246711730957\n",
      "G loss: 1.7778130769729614\n",
      "E loss:  1.1969413757324219\n",
      "G loss: 1.7868881225585938\n",
      "E loss:  1.2148096561431885\n",
      "G loss: 1.7933557033538818\n",
      "E loss:  1.2047748565673828\n",
      "G loss: 1.7849335670471191\n",
      "Training Model  ...\n",
      "E loss:  1.0926058292388916\n",
      "G loss: 1.8049086332321167\n",
      "E loss:  1.0901877880096436\n",
      "G loss: 1.7827427387237549\n",
      "E loss:  1.1097323894500732\n",
      "G loss: 1.7420084476470947\n",
      "E loss:  1.1093530654907227\n",
      "G loss: 1.7442890405654907\n",
      "E loss:  1.0990350246429443\n",
      "G loss: 1.649305820465088\n",
      "Training Model  ...\n",
      "E loss:  1.22690749168396\n",
      "G loss: 1.704092264175415\n",
      "E loss:  1.221792221069336\n",
      "G loss: 1.6982531547546387\n",
      "E loss:  1.214755654335022\n",
      "G loss: 1.6799505949020386\n",
      "E loss:  1.2222224473953247\n",
      "G loss: 1.7287603616714478\n",
      "E loss:  1.246777057647705\n",
      "G loss: 1.7939146757125854\n",
      "Training Model  ...\n",
      "E loss:  1.1803134679794312\n",
      "G loss: 1.7887592315673828\n",
      "E loss:  1.1603999137878418\n",
      "G loss: 1.7318384647369385\n",
      "E loss:  1.1628451347351074\n",
      "G loss: 1.71160089969635\n",
      "E loss:  1.1887457370758057\n",
      "G loss: 1.7790918350219727\n",
      "E loss:  1.171350121498108\n",
      "G loss: 1.779054880142212\n",
      "Training Model  ...\n",
      "E loss:  1.1486709117889404\n",
      "G loss: 1.7485512495040894\n",
      "E loss:  1.146559238433838\n",
      "G loss: 1.750190258026123\n",
      "E loss:  1.1543415784835815\n",
      "G loss: 1.7309892177581787\n",
      "E loss:  1.1460375785827637\n",
      "G loss: 1.7295035123825073\n",
      "E loss:  1.1538493633270264\n",
      "G loss: 1.7205134630203247\n",
      "Training Model  ...\n",
      "E loss:  1.2132418155670166\n",
      "G loss: 1.6898040771484375\n",
      "E loss:  1.2233749628067017\n",
      "G loss: 1.7280210256576538\n",
      "E loss:  1.1989463567733765\n",
      "G loss: 1.712328314781189\n",
      "E loss:  1.2204890251159668\n",
      "G loss: 1.7432005405426025\n",
      "E loss:  1.2189586162567139\n",
      "G loss: 1.7461214065551758\n",
      "Training Model  ...\n",
      "E loss:  1.2415608167648315\n",
      "G loss: 1.7584651708602905\n",
      "E loss:  1.23382568359375\n",
      "G loss: 1.7071070671081543\n",
      "E loss:  1.214963674545288\n",
      "G loss: 1.752669334411621\n",
      "E loss:  1.2244038581848145\n",
      "G loss: 1.708276391029358\n",
      "E loss:  1.2087020874023438\n",
      "G loss: 1.6805810928344727\n",
      "Training Model  ...\n",
      "E loss:  1.1668126583099365\n",
      "G loss: 1.7229969501495361\n",
      "E loss:  1.1751223802566528\n",
      "G loss: 1.7690348625183105\n",
      "E loss:  1.1725099086761475\n",
      "G loss: 1.746564269065857\n",
      "E loss:  1.1659514904022217\n",
      "G loss: 1.781392216682434\n",
      "E loss:  1.150851845741272\n",
      "G loss: 1.8633289337158203\n",
      "Training Model  ...\n",
      "E loss:  1.1058789491653442\n",
      "G loss: 1.8031253814697266\n",
      "E loss:  1.0919026136398315\n",
      "G loss: 1.7897663116455078\n",
      "E loss:  1.1102395057678223\n",
      "G loss: 1.7764461040496826\n",
      "E loss:  1.1403379440307617\n",
      "G loss: 1.8284584283828735\n",
      "E loss:  1.172327995300293\n",
      "G loss: 1.774651288986206\n",
      "Training Model  ...\n",
      "E loss:  1.1756840944290161\n",
      "G loss: 1.7908896207809448\n",
      "E loss:  1.16953706741333\n",
      "G loss: 1.8166533708572388\n",
      "E loss:  1.1644152402877808\n",
      "G loss: 1.8020192384719849\n",
      "E loss:  1.139560341835022\n",
      "G loss: 1.8238511085510254\n",
      "E loss:  1.1290284395217896\n",
      "G loss: 1.8180320262908936\n",
      "Training Model  ...\n",
      "E loss:  1.3060822486877441\n",
      "G loss: 1.836151361465454\n",
      "E loss:  1.307599663734436\n",
      "G loss: 1.8355188369750977\n",
      "E loss:  1.3071398735046387\n",
      "G loss: 1.9122610092163086\n",
      "E loss:  1.2871023416519165\n",
      "G loss: 1.906327724456787\n",
      "E loss:  1.2851160764694214\n",
      "G loss: 1.9836039543151855\n",
      "Training Model  ...\n",
      "E loss:  1.1236350536346436\n",
      "G loss: 1.8968682289123535\n",
      "E loss:  1.1394890546798706\n",
      "G loss: 1.9468584060668945\n",
      "E loss:  1.11091947555542\n",
      "G loss: 1.9512159824371338\n",
      "E loss:  1.1164497137069702\n",
      "G loss: 1.8504644632339478\n",
      "E loss:  1.1186450719833374\n",
      "G loss: 1.886654257774353\n",
      "Training Model  ...\n",
      "E loss:  1.151883602142334\n",
      "G loss: 1.8751763105392456\n",
      "E loss:  1.1562352180480957\n",
      "G loss: 1.8482065200805664\n",
      "E loss:  1.1643370389938354\n",
      "G loss: 1.8497650623321533\n",
      "E loss:  1.1475036144256592\n",
      "G loss: 1.8376742601394653\n",
      "E loss:  1.1208386421203613\n",
      "G loss: 1.819413185119629\n",
      "Training Model  ...\n",
      "E loss:  1.3291243314743042\n",
      "G loss: 1.8740073442459106\n",
      "E loss:  1.336077094078064\n",
      "G loss: 1.839109182357788\n",
      "E loss:  1.313822865486145\n",
      "G loss: 1.884895920753479\n",
      "E loss:  1.3238019943237305\n",
      "G loss: 1.8630236387252808\n",
      "E loss:  1.332861304283142\n",
      "G loss: 1.8614354133605957\n",
      "Training Model  ...\n",
      "E loss:  1.2370704412460327\n",
      "G loss: 1.8581171035766602\n",
      "E loss:  1.223498821258545\n",
      "G loss: 1.8171823024749756\n",
      "E loss:  1.2333438396453857\n",
      "G loss: 1.8412253856658936\n",
      "E loss:  1.2158966064453125\n",
      "G loss: 1.8131582736968994\n",
      "E loss:  1.2077734470367432\n",
      "G loss: 1.809449553489685\n",
      "Training Model  ...\n",
      "E loss:  1.0757780075073242\n",
      "G loss: 1.7595996856689453\n",
      "E loss:  1.0928503274917603\n",
      "G loss: 1.811735987663269\n",
      "E loss:  1.072523832321167\n",
      "G loss: 1.8437410593032837\n",
      "E loss:  1.0696507692337036\n",
      "G loss: 1.7950712442398071\n",
      "E loss:  1.0499614477157593\n",
      "G loss: 1.8223403692245483\n",
      "Training Model  ...\n",
      "E loss:  1.0363829135894775\n",
      "G loss: 1.7897006273269653\n",
      "E loss:  1.048818826675415\n",
      "G loss: 1.7315845489501953\n",
      "E loss:  1.0605376958847046\n",
      "G loss: 1.7335485219955444\n",
      "E loss:  1.0472780466079712\n",
      "G loss: 1.758982539176941\n",
      "E loss:  1.0715806484222412\n",
      "G loss: 1.7586915493011475\n",
      "Training Model  ...\n",
      "E loss:  1.0369033813476562\n",
      "G loss: 1.7056236267089844\n",
      "E loss:  1.039520263671875\n",
      "G loss: 1.7468328475952148\n",
      "E loss:  1.0753893852233887\n",
      "G loss: 1.750937819480896\n",
      "E loss:  1.0743720531463623\n",
      "G loss: 1.7488776445388794\n",
      "E loss:  1.05262291431427\n",
      "G loss: 1.7408781051635742\n",
      "Training Model  ...\n",
      "E loss:  1.2773998975753784\n",
      "G loss: 1.705094814300537\n",
      "E loss:  1.2569581270217896\n",
      "G loss: 1.7501717805862427\n",
      "E loss:  1.2288013696670532\n",
      "G loss: 1.7199316024780273\n",
      "E loss:  1.2152224779129028\n",
      "G loss: 1.7415516376495361\n",
      "E loss:  1.2072126865386963\n",
      "G loss: 1.755574345588684\n",
      "Training Model  ...\n",
      "E loss:  1.1227971315383911\n",
      "G loss: 1.7621097564697266\n",
      "E loss:  1.1658467054367065\n",
      "G loss: 1.772481918334961\n",
      "E loss:  1.1643500328063965\n",
      "G loss: 1.7994177341461182\n",
      "E loss:  1.1577426195144653\n",
      "G loss: 1.74568772315979\n",
      "E loss:  1.1538363695144653\n",
      "G loss: 1.7063194513320923\n",
      "Training Model  ...\n",
      "E loss:  1.182010531425476\n",
      "G loss: 1.7705426216125488\n",
      "E loss:  1.1627552509307861\n",
      "G loss: 1.7438523769378662\n",
      "E loss:  1.1349207162857056\n",
      "G loss: 1.782326340675354\n",
      "E loss:  1.1487642526626587\n",
      "G loss: 1.8105041980743408\n",
      "E loss:  1.147437334060669\n",
      "G loss: 1.809687614440918\n",
      "Training Model  ...\n",
      "E loss:  1.0990548133850098\n",
      "G loss: 1.8912259340286255\n",
      "E loss:  1.1000968217849731\n",
      "G loss: 1.782657504081726\n",
      "E loss:  1.1160398721694946\n",
      "G loss: 1.8208136558532715\n",
      "E loss:  1.1114487648010254\n",
      "G loss: 1.815205693244934\n",
      "E loss:  1.104577660560608\n",
      "G loss: 1.8310375213623047\n",
      "Training Model  ...\n",
      "E loss:  1.097244143486023\n",
      "G loss: 1.8341054916381836\n",
      "E loss:  1.0603622198104858\n",
      "G loss: 1.7758721113204956\n",
      "E loss:  1.0523799657821655\n",
      "G loss: 1.8062645196914673\n",
      "E loss:  1.0602242946624756\n",
      "G loss: 1.8899198770523071\n",
      "E loss:  1.0587211847305298\n",
      "G loss: 1.8691657781600952\n",
      "Training Model  ...\n",
      "E loss:  1.123194932937622\n",
      "G loss: 1.808487057685852\n",
      "E loss:  1.1160764694213867\n",
      "G loss: 1.9001669883728027\n",
      "E loss:  1.1090071201324463\n",
      "G loss: 1.8114691972732544\n",
      "E loss:  1.1107298135757446\n",
      "G loss: 1.7833223342895508\n",
      "E loss:  1.1191242933273315\n",
      "G loss: 1.7737984657287598\n",
      "Training Model  ...\n",
      "E loss:  1.2276761531829834\n",
      "G loss: 1.8061612844467163\n",
      "E loss:  1.2335681915283203\n",
      "G loss: 1.8380274772644043\n",
      "E loss:  1.2385121583938599\n",
      "G loss: 1.8152014017105103\n",
      "E loss:  1.225578784942627\n",
      "G loss: 1.8358607292175293\n",
      "E loss:  1.2112489938735962\n",
      "G loss: 1.8344165086746216\n",
      "Training Model  ...\n",
      "E loss:  1.2457177639007568\n",
      "G loss: 1.8640220165252686\n",
      "E loss:  1.237492322921753\n",
      "G loss: 1.8325241804122925\n",
      "E loss:  1.2360272407531738\n",
      "G loss: 1.8430719375610352\n",
      "E loss:  1.2282416820526123\n",
      "G loss: 1.8506861925125122\n",
      "E loss:  1.2154850959777832\n",
      "G loss: 1.777856469154358\n",
      "Training Model  ...\n",
      "E loss:  1.1579632759094238\n",
      "G loss: 1.8109056949615479\n",
      "E loss:  1.1485822200775146\n",
      "G loss: 1.8030977249145508\n",
      "E loss:  1.144393801689148\n",
      "G loss: 1.7808520793914795\n",
      "E loss:  1.1387717723846436\n",
      "G loss: 1.7684681415557861\n",
      "E loss:  1.128548264503479\n",
      "G loss: 1.7327044010162354\n",
      "Training Model  ...\n",
      "E loss:  1.0928680896759033\n",
      "G loss: 1.7151986360549927\n",
      "E loss:  1.1086922883987427\n",
      "G loss: 1.7847435474395752\n",
      "E loss:  1.0971697568893433\n",
      "G loss: 1.7987465858459473\n",
      "E loss:  1.1204546689987183\n",
      "G loss: 1.7854588031768799\n",
      "E loss:  1.1306077241897583\n",
      "G loss: 1.823456048965454\n",
      "Training Model  ...\n",
      "E loss:  1.009901523590088\n",
      "G loss: 1.8508487939834595\n",
      "E loss:  0.9737428426742554\n",
      "G loss: 1.856683373451233\n",
      "E loss:  0.986809492111206\n",
      "G loss: 1.8554823398590088\n",
      "E loss:  0.9970865249633789\n",
      "G loss: 1.8605718612670898\n",
      "E loss:  0.9917652606964111\n",
      "G loss: 1.9498995542526245\n",
      "Training Model  ...\n",
      "E loss:  1.1301592588424683\n",
      "G loss: 1.874466061592102\n",
      "E loss:  1.1055718660354614\n",
      "G loss: 1.876390814781189\n",
      "E loss:  1.0928168296813965\n",
      "G loss: 1.8822598457336426\n",
      "E loss:  1.1046361923217773\n",
      "G loss: 1.7848421335220337\n",
      "E loss:  1.092737078666687\n",
      "G loss: 1.7801276445388794\n",
      "Training Model  ...\n",
      "E loss:  1.1567904949188232\n",
      "G loss: 1.831092357635498\n",
      "E loss:  1.1524931192398071\n",
      "G loss: 1.8144195079803467\n",
      "E loss:  1.1454808712005615\n",
      "G loss: 1.8533799648284912\n",
      "E loss:  1.1466525793075562\n",
      "G loss: 1.8573856353759766\n",
      "E loss:  1.1670498847961426\n",
      "G loss: 1.8850921392440796\n",
      "Training Model  ...\n",
      "E loss:  1.0678712129592896\n",
      "G loss: 1.8927695751190186\n",
      "E loss:  1.0506234169006348\n",
      "G loss: 1.8925669193267822\n",
      "E loss:  1.029660940170288\n",
      "G loss: 1.9091604948043823\n",
      "E loss:  1.0490481853485107\n",
      "G loss: 1.8387274742126465\n",
      "E loss:  1.031808853149414\n",
      "G loss: 1.8280404806137085\n",
      "Training Model  ...\n",
      "E loss:  1.1490628719329834\n",
      "G loss: 1.8684113025665283\n",
      "E loss:  1.16080904006958\n",
      "G loss: 1.821169376373291\n",
      "E loss:  1.182101845741272\n",
      "G loss: 1.832326054573059\n",
      "E loss:  1.1860207319259644\n",
      "G loss: 1.8052523136138916\n",
      "E loss:  1.1814312934875488\n",
      "G loss: 1.7763031721115112\n",
      "Training Model  ...\n",
      "E loss:  1.0611298084259033\n",
      "G loss: 1.7633521556854248\n",
      "E loss:  1.0570749044418335\n",
      "G loss: 1.7538518905639648\n",
      "E loss:  1.059507966041565\n",
      "G loss: 1.8122434616088867\n",
      "E loss:  1.0615930557250977\n",
      "G loss: 1.79097318649292\n",
      "E loss:  1.0676252841949463\n",
      "G loss: 1.7323834896087646\n",
      "Training Model  ...\n",
      "E loss:  1.010048270225525\n",
      "G loss: 1.7868329286575317\n",
      "E loss:  0.9978036880493164\n",
      "G loss: 1.7085481882095337\n",
      "E loss:  0.9877921342849731\n",
      "G loss: 1.7741827964782715\n",
      "E loss:  0.9925658106803894\n",
      "G loss: 1.7538179159164429\n",
      "E loss:  0.984994113445282\n",
      "G loss: 1.8199079036712646\n",
      "Training Model  ...\n",
      "E loss:  1.1598687171936035\n",
      "G loss: 1.7606245279312134\n",
      "E loss:  1.1800787448883057\n",
      "G loss: 1.7079763412475586\n",
      "E loss:  1.1691970825195312\n",
      "G loss: 1.7605435848236084\n",
      "E loss:  1.1597845554351807\n",
      "G loss: 1.7463569641113281\n",
      "E loss:  1.1508573293685913\n",
      "G loss: 1.6839814186096191\n",
      "Training Model  ...\n",
      "E loss:  1.2520447969436646\n",
      "G loss: 1.7319462299346924\n",
      "E loss:  1.2274479866027832\n",
      "G loss: 1.7582721710205078\n",
      "E loss:  1.2128826379776\n",
      "G loss: 1.7424767017364502\n",
      "E loss:  1.2083723545074463\n",
      "G loss: 1.7601962089538574\n",
      "E loss:  1.1999200582504272\n",
      "G loss: 1.7721329927444458\n",
      "Training Model  ...\n",
      "E loss:  1.1002693176269531\n",
      "G loss: 1.7578762769699097\n",
      "E loss:  1.0866199731826782\n",
      "G loss: 1.8016321659088135\n",
      "E loss:  1.082717776298523\n",
      "G loss: 1.7311933040618896\n",
      "E loss:  1.0749187469482422\n",
      "G loss: 1.7625271081924438\n",
      "E loss:  1.0940378904342651\n",
      "G loss: 1.770761251449585\n",
      "Training Model  ...\n",
      "E loss:  1.0645382404327393\n",
      "G loss: 1.7110469341278076\n",
      "E loss:  1.0740883350372314\n",
      "G loss: 1.7096803188323975\n",
      "E loss:  1.0852187871932983\n",
      "G loss: 1.7868772745132446\n",
      "E loss:  1.0622872114181519\n",
      "G loss: 1.7502110004425049\n",
      "E loss:  1.0445607900619507\n",
      "G loss: 1.7779724597930908\n",
      "Training Model  ...\n",
      "E loss:  1.1424649953842163\n",
      "G loss: 1.7155017852783203\n",
      "E loss:  1.1571605205535889\n",
      "G loss: 1.8027691841125488\n",
      "E loss:  1.1632003784179688\n",
      "G loss: 1.7523839473724365\n",
      "E loss:  1.1427457332611084\n",
      "G loss: 1.7430579662322998\n",
      "E loss:  1.1067155599594116\n",
      "G loss: 1.8207478523254395\n",
      "Training Model  ...\n",
      "E loss:  1.0450263023376465\n",
      "G loss: 1.7444874048233032\n",
      "E loss:  1.019818663597107\n",
      "G loss: 1.7572214603424072\n",
      "E loss:  1.0221694707870483\n",
      "G loss: 1.8101435899734497\n",
      "E loss:  1.0272754430770874\n",
      "G loss: 1.8163914680480957\n",
      "E loss:  0.9855854511260986\n",
      "G loss: 1.796553373336792\n",
      "Training Model  ...\n",
      "E loss:  0.9661059379577637\n",
      "G loss: 1.750680923461914\n",
      "E loss:  0.9561920762062073\n",
      "G loss: 1.8065614700317383\n",
      "E loss:  0.966269850730896\n",
      "G loss: 1.8248839378356934\n",
      "E loss:  0.9627687335014343\n",
      "G loss: 1.8037314414978027\n",
      "E loss:  0.9902890920639038\n",
      "G loss: 1.7994028329849243\n",
      "Training Model  ...\n",
      "E loss:  1.0847797393798828\n",
      "G loss: 1.8391162157058716\n",
      "E loss:  1.077324390411377\n",
      "G loss: 1.8338313102722168\n",
      "E loss:  1.0581108331680298\n",
      "G loss: 1.8125407695770264\n",
      "E loss:  1.0650802850723267\n",
      "G loss: 1.8617639541625977\n",
      "E loss:  1.0339289903640747\n",
      "G loss: 1.9645075798034668\n",
      "Training Model  ...\n",
      "E loss:  1.0714489221572876\n",
      "G loss: 1.8557000160217285\n",
      "E loss:  1.0580568313598633\n",
      "G loss: 1.9342527389526367\n",
      "E loss:  1.0566682815551758\n",
      "G loss: 2.0171573162078857\n",
      "E loss:  1.08980393409729\n",
      "G loss: 2.01644229888916\n",
      "E loss:  1.0809690952301025\n",
      "G loss: 2.02115797996521\n",
      "Training Model  ...\n",
      "E loss:  1.215522289276123\n",
      "G loss: 2.0001301765441895\n",
      "E loss:  1.1927489042282104\n",
      "G loss: 2.031602144241333\n",
      "E loss:  1.1995502710342407\n",
      "G loss: 1.942745566368103\n",
      "E loss:  1.1826510429382324\n",
      "G loss: 1.8981037139892578\n",
      "E loss:  1.144514799118042\n",
      "G loss: 1.82646906375885\n",
      "Training Model  ...\n",
      "E loss:  1.049385666847229\n",
      "G loss: 1.9182324409484863\n",
      "E loss:  1.0495047569274902\n",
      "G loss: 1.779773473739624\n",
      "E loss:  1.070364236831665\n",
      "G loss: 1.7982232570648193\n",
      "E loss:  1.065476417541504\n",
      "G loss: 1.7782193422317505\n",
      "E loss:  1.0786900520324707\n",
      "G loss: 1.7542805671691895\n",
      "Training Model  ...\n",
      "E loss:  1.0391960144042969\n",
      "G loss: 1.709894061088562\n",
      "E loss:  1.0546762943267822\n",
      "G loss: 1.7477433681488037\n",
      "E loss:  1.0421299934387207\n",
      "G loss: 1.739886999130249\n",
      "E loss:  1.0398951768875122\n",
      "G loss: 1.7575632333755493\n",
      "E loss:  1.0546886920928955\n",
      "G loss: 1.8487606048583984\n",
      "Training Model  ...\n",
      "E loss:  1.0461089611053467\n",
      "G loss: 1.8416324853897095\n",
      "E loss:  1.0581284761428833\n",
      "G loss: 1.8814566135406494\n",
      "E loss:  1.0514957904815674\n",
      "G loss: 1.8677732944488525\n",
      "E loss:  1.0670862197875977\n",
      "G loss: 1.9149672985076904\n",
      "E loss:  1.0650662183761597\n",
      "G loss: 1.8894503116607666\n",
      "Training Model  ...\n",
      "E loss:  1.1538195610046387\n",
      "G loss: 1.9444315433502197\n",
      "E loss:  1.1448817253112793\n",
      "G loss: 1.927091360092163\n",
      "E loss:  1.1146572828292847\n",
      "G loss: 1.8790491819381714\n",
      "E loss:  1.12734055519104\n",
      "G loss: 1.9273579120635986\n",
      "E loss:  1.1160922050476074\n",
      "G loss: 1.8973252773284912\n",
      "Training Model  ...\n",
      "E loss:  1.031893253326416\n",
      "G loss: 1.8637959957122803\n",
      "E loss:  1.0368419885635376\n",
      "G loss: 1.9268150329589844\n",
      "E loss:  1.0291787385940552\n",
      "G loss: 1.905084252357483\n",
      "E loss:  1.0303481817245483\n",
      "G loss: 1.8240454196929932\n",
      "E loss:  1.0254535675048828\n",
      "G loss: 1.8396161794662476\n",
      "Training Model  ...\n",
      "E loss:  1.0399155616760254\n",
      "G loss: 1.822551965713501\n",
      "E loss:  1.0770068168640137\n",
      "G loss: 1.743570327758789\n",
      "E loss:  1.0709463357925415\n",
      "G loss: 1.6949596405029297\n",
      "E loss:  1.0898164510726929\n",
      "G loss: 1.7250561714172363\n",
      "E loss:  1.0884208679199219\n",
      "G loss: 1.6853768825531006\n",
      "Training Model  ...\n",
      "E loss:  0.98033607006073\n",
      "G loss: 1.678375244140625\n",
      "E loss:  0.9826537370681763\n",
      "G loss: 1.7427763938903809\n",
      "E loss:  0.9786447882652283\n",
      "G loss: 1.7159843444824219\n",
      "E loss:  0.9900832176208496\n",
      "G loss: 1.6334640979766846\n",
      "E loss:  0.987842857837677\n",
      "G loss: 1.7081525325775146\n",
      "Training Model  ...\n",
      "E loss:  0.9291201829910278\n",
      "G loss: 1.7135688066482544\n",
      "E loss:  0.9300352931022644\n",
      "G loss: 1.697561502456665\n",
      "E loss:  0.9132493734359741\n",
      "G loss: 1.7808423042297363\n",
      "E loss:  0.9115384817123413\n",
      "G loss: 1.9110374450683594\n",
      "E loss:  0.9053249359130859\n",
      "G loss: 1.8717429637908936\n",
      "Training Model  ...\n",
      "E loss:  0.9802507162094116\n",
      "G loss: 1.9122411012649536\n",
      "E loss:  0.9829878211021423\n",
      "G loss: 1.8651456832885742\n",
      "E loss:  0.9921200275421143\n",
      "G loss: 1.8589493036270142\n",
      "E loss:  1.0035954713821411\n",
      "G loss: 1.810768961906433\n",
      "E loss:  0.9992666244506836\n",
      "G loss: 1.7006052732467651\n",
      "Training Model  ...\n",
      "E loss:  1.068040370941162\n",
      "G loss: 1.7358460426330566\n",
      "E loss:  1.049048900604248\n",
      "G loss: 1.8505960702896118\n",
      "E loss:  1.0565974712371826\n",
      "G loss: 1.8222662210464478\n",
      "E loss:  1.0416316986083984\n",
      "G loss: 1.9340564012527466\n",
      "E loss:  1.0545827150344849\n",
      "G loss: 2.0030081272125244\n",
      "Training Model  ...\n",
      "E loss:  1.1357632875442505\n",
      "G loss: 2.0117695331573486\n",
      "E loss:  1.153311848640442\n",
      "G loss: 1.906416416168213\n",
      "E loss:  1.1531696319580078\n",
      "G loss: 1.8889480829238892\n",
      "E loss:  1.1569757461547852\n",
      "G loss: 1.9045454263687134\n",
      "E loss:  1.1570427417755127\n",
      "G loss: 1.8860739469528198\n",
      "Training Model  ...\n",
      "E loss:  1.1129179000854492\n",
      "G loss: 1.897325873374939\n",
      "E loss:  1.0843509435653687\n",
      "G loss: 1.9049488306045532\n",
      "E loss:  1.044036626815796\n",
      "G loss: 1.8610060214996338\n",
      "E loss:  1.0640242099761963\n",
      "G loss: 1.8993438482284546\n",
      "E loss:  1.0523455142974854\n",
      "G loss: 1.9122426509857178\n",
      "Training Model  ...\n",
      "E loss:  0.9785367250442505\n",
      "G loss: 1.8936470746994019\n",
      "E loss:  0.9996066093444824\n",
      "G loss: 1.9340637922286987\n",
      "E loss:  0.991929292678833\n",
      "G loss: 1.8463037014007568\n",
      "E loss:  0.9809910655021667\n",
      "G loss: 1.9028295278549194\n",
      "E loss:  0.9662274122238159\n",
      "G loss: 1.9219284057617188\n",
      "Training Model  ...\n",
      "E loss:  0.9237436056137085\n",
      "G loss: 1.8429409265518188\n",
      "E loss:  0.9556630849838257\n",
      "G loss: 1.8900245428085327\n",
      "E loss:  0.9483555555343628\n",
      "G loss: 1.9128373861312866\n",
      "E loss:  0.9620399475097656\n",
      "G loss: 1.9457224607467651\n",
      "E loss:  0.9585059881210327\n",
      "G loss: 1.9446601867675781\n",
      "Training Model  ...\n",
      "E loss:  0.9364551901817322\n",
      "G loss: 1.915987253189087\n",
      "E loss:  0.9284559488296509\n",
      "G loss: 1.8801671266555786\n",
      "E loss:  0.9203240871429443\n",
      "G loss: 1.8781312704086304\n",
      "E loss:  0.9354808926582336\n",
      "G loss: 1.8434230089187622\n",
      "E loss:  0.9306093454360962\n",
      "G loss: 1.8077707290649414\n",
      "Training Model  ...\n",
      "E loss:  1.0106215476989746\n",
      "G loss: 1.8408526182174683\n",
      "E loss:  1.0005450248718262\n",
      "G loss: 1.888226866722107\n",
      "E loss:  0.9884915351867676\n",
      "G loss: 1.8912770748138428\n",
      "E loss:  0.9772393703460693\n",
      "G loss: 1.8789094686508179\n",
      "E loss:  0.9789981245994568\n",
      "G loss: 1.9710489511489868\n",
      "Training Model  ...\n",
      "E loss:  0.9272080659866333\n",
      "G loss: 1.9046967029571533\n",
      "E loss:  0.9279304146766663\n",
      "G loss: 1.9735965728759766\n",
      "E loss:  0.9079731702804565\n",
      "G loss: 1.997741460800171\n",
      "E loss:  0.9105767011642456\n",
      "G loss: 2.059067726135254\n",
      "E loss:  0.9146743416786194\n",
      "G loss: 2.123462438583374\n",
      "Training Model  ...\n",
      "E loss:  1.0285964012145996\n",
      "G loss: 2.078904151916504\n",
      "E loss:  1.0070970058441162\n",
      "G loss: 2.0481531620025635\n",
      "E loss:  0.9982221722602844\n",
      "G loss: 1.9648023843765259\n",
      "E loss:  1.0239956378936768\n",
      "G loss: 1.9615888595581055\n",
      "E loss:  0.9887117147445679\n",
      "G loss: 1.9407212734222412\n",
      "Training Model  ...\n",
      "E loss:  0.9903208017349243\n",
      "G loss: 2.017103672027588\n",
      "E loss:  0.9776188731193542\n",
      "G loss: 1.953548550605774\n",
      "E loss:  0.9678853154182434\n",
      "G loss: 1.894652009010315\n",
      "E loss:  0.9782006740570068\n",
      "G loss: 1.940510869026184\n",
      "E loss:  0.9775121808052063\n",
      "G loss: 1.8614561557769775\n",
      "Training Model  ...\n",
      "E loss:  0.9925804734230042\n",
      "G loss: 1.8972537517547607\n",
      "E loss:  0.9848124980926514\n",
      "G loss: 1.8196742534637451\n",
      "E loss:  1.0155694484710693\n",
      "G loss: 1.8218568563461304\n",
      "E loss:  1.0146301984786987\n",
      "G loss: 1.8087352514266968\n",
      "E loss:  0.9883630275726318\n",
      "G loss: 1.7432091236114502\n",
      "Training Model  ...\n",
      "E loss:  1.0304553508758545\n",
      "G loss: 1.7295093536376953\n",
      "E loss:  1.0453510284423828\n",
      "G loss: 1.7019836902618408\n",
      "E loss:  1.0310664176940918\n",
      "G loss: 1.7300266027450562\n",
      "E loss:  1.016183614730835\n",
      "G loss: 1.7272558212280273\n",
      "E loss:  1.0158846378326416\n",
      "G loss: 1.7365857362747192\n",
      "Training Model  ...\n",
      "E loss:  1.0983086824417114\n",
      "G loss: 1.702978491783142\n",
      "E loss:  1.0950474739074707\n",
      "G loss: 1.7662968635559082\n",
      "E loss:  1.0801008939743042\n",
      "G loss: 1.8666479587554932\n",
      "E loss:  1.062504768371582\n",
      "G loss: 1.905868411064148\n",
      "E loss:  1.0614464282989502\n",
      "G loss: 1.9917157888412476\n",
      "Training Model  ...\n",
      "E loss:  0.9011273384094238\n",
      "G loss: 1.9446322917938232\n",
      "E loss:  0.892844021320343\n",
      "G loss: 1.9299474954605103\n",
      "E loss:  0.8959723114967346\n",
      "G loss: 1.8800908327102661\n",
      "E loss:  0.8968526721000671\n",
      "G loss: 1.9180166721343994\n",
      "E loss:  0.9037124514579773\n",
      "G loss: 1.8671060800552368\n",
      "Training Model  ...\n",
      "E loss:  0.8676055073738098\n",
      "G loss: 1.8701940774917603\n",
      "E loss:  0.8725185990333557\n",
      "G loss: 1.814329743385315\n",
      "E loss:  0.8669378161430359\n",
      "G loss: 1.8765586614608765\n",
      "E loss:  0.8725056648254395\n",
      "G loss: 1.8632018566131592\n",
      "E loss:  0.8939605951309204\n",
      "G loss: 1.8757060766220093\n",
      "Training Model  ...\n",
      "E loss:  1.0768814086914062\n",
      "G loss: 1.9042452573776245\n",
      "E loss:  1.0566637516021729\n",
      "G loss: 1.997338056564331\n",
      "E loss:  1.0610768795013428\n",
      "G loss: 1.9682533740997314\n",
      "E loss:  1.0358610153198242\n",
      "G loss: 2.034207820892334\n",
      "E loss:  1.0526254177093506\n",
      "G loss: 2.0339558124542236\n",
      "Training Model  ...\n",
      "E loss:  0.9438773393630981\n",
      "G loss: 1.9886374473571777\n",
      "E loss:  0.9584757089614868\n",
      "G loss: 1.9533183574676514\n",
      "E loss:  0.9623390436172485\n",
      "G loss: 1.9209630489349365\n",
      "E loss:  0.9684438109397888\n",
      "G loss: 1.7906274795532227\n",
      "E loss:  0.9689506888389587\n",
      "G loss: 1.7088712453842163\n",
      "Training Model  ...\n",
      "E loss:  0.9804731607437134\n",
      "G loss: 1.7961829900741577\n",
      "E loss:  0.9750136733055115\n",
      "G loss: 1.7416499853134155\n",
      "E loss:  0.9881635308265686\n",
      "G loss: 1.789495825767517\n",
      "E loss:  0.9867958426475525\n",
      "G loss: 1.8478275537490845\n",
      "E loss:  0.9754494428634644\n",
      "G loss: 1.968993902206421\n",
      "Training Model  ...\n",
      "E loss:  0.9226022362709045\n",
      "G loss: 1.8689920902252197\n",
      "E loss:  0.9475060105323792\n",
      "G loss: 1.8833483457565308\n",
      "E loss:  0.9378239512443542\n",
      "G loss: 1.9454667568206787\n",
      "E loss:  0.9217227101325989\n",
      "G loss: 1.8780415058135986\n",
      "E loss:  0.9423912167549133\n",
      "G loss: 1.8420881032943726\n",
      "Training Model  ...\n",
      "E loss:  1.0074175596237183\n",
      "G loss: 1.8509776592254639\n",
      "E loss:  0.9908542037010193\n",
      "G loss: 1.859675407409668\n",
      "E loss:  0.9924770593643188\n",
      "G loss: 1.9566500186920166\n",
      "E loss:  0.9786801338195801\n",
      "G loss: 1.9824844598770142\n",
      "E loss:  0.9625717997550964\n",
      "G loss: 1.9862847328186035\n",
      "Training Model  ...\n",
      "E loss:  1.1081875562667847\n",
      "G loss: 1.9800375699996948\n",
      "E loss:  1.1225895881652832\n",
      "G loss: 2.047767400741577\n",
      "E loss:  1.1168506145477295\n",
      "G loss: 2.0958328247070312\n",
      "E loss:  1.1201562881469727\n",
      "G loss: 1.996227741241455\n",
      "E loss:  1.0990920066833496\n",
      "G loss: 2.0378470420837402\n",
      "Training Model  ...\n",
      "E loss:  0.9419598579406738\n",
      "G loss: 2.045684337615967\n",
      "E loss:  0.937147855758667\n",
      "G loss: 2.0085158348083496\n",
      "E loss:  0.951187252998352\n",
      "G loss: 1.9481899738311768\n",
      "E loss:  0.9558536410331726\n",
      "G loss: 1.8570204973220825\n",
      "E loss:  0.957360029220581\n",
      "G loss: 1.8041818141937256\n",
      "Training Model  ...\n",
      "E loss:  0.9213649034500122\n",
      "G loss: 1.7960230112075806\n",
      "E loss:  0.9198105335235596\n",
      "G loss: 1.8193693161010742\n",
      "E loss:  0.944598913192749\n",
      "G loss: 1.8958697319030762\n",
      "E loss:  0.9441221356391907\n",
      "G loss: 1.9192276000976562\n",
      "E loss:  0.9449707269668579\n",
      "G loss: 1.9136605262756348\n",
      "Training Model  ...\n",
      "E loss:  1.0180249214172363\n",
      "G loss: 1.8755446672439575\n",
      "E loss:  1.0048086643218994\n",
      "G loss: 1.8847715854644775\n",
      "E loss:  0.9846659898757935\n",
      "G loss: 1.9086575508117676\n",
      "E loss:  0.9794963598251343\n",
      "G loss: 1.8882702589035034\n",
      "E loss:  0.9732233285903931\n",
      "G loss: 1.851420283317566\n",
      "Training Model  ...\n",
      "E loss:  0.9746509790420532\n",
      "G loss: 1.8575537204742432\n",
      "E loss:  0.9832442402839661\n",
      "G loss: 1.8095380067825317\n",
      "E loss:  1.0097153186798096\n",
      "G loss: 1.8719912767410278\n",
      "E loss:  1.015008807182312\n",
      "G loss: 1.855042815208435\n",
      "E loss:  1.0105375051498413\n",
      "G loss: 1.9489474296569824\n",
      "Training Model  ...\n",
      "E loss:  0.9623701572418213\n",
      "G loss: 1.8465856313705444\n",
      "E loss:  0.9625672698020935\n",
      "G loss: 1.919684648513794\n",
      "E loss:  0.9569339156150818\n",
      "G loss: 1.8126002550125122\n",
      "E loss:  0.9681265950202942\n",
      "G loss: 1.9070852994918823\n",
      "E loss:  0.9690373539924622\n",
      "G loss: 1.8511755466461182\n",
      "Training Model  ...\n",
      "E loss:  0.9494833946228027\n",
      "G loss: 1.8720874786376953\n",
      "E loss:  0.9431263208389282\n",
      "G loss: 1.8219380378723145\n",
      "E loss:  0.9556347131729126\n",
      "G loss: 1.7855615615844727\n",
      "E loss:  0.930429995059967\n",
      "G loss: 1.7549412250518799\n",
      "E loss:  0.9112473130226135\n",
      "G loss: 1.7659246921539307\n",
      "Training Model  ...\n",
      "E loss:  0.9150379300117493\n",
      "G loss: 1.8151520490646362\n",
      "E loss:  0.9122515916824341\n",
      "G loss: 1.7483686208724976\n",
      "E loss:  0.9096783399581909\n",
      "G loss: 1.7870850563049316\n",
      "E loss:  0.9014735817909241\n",
      "G loss: 1.9465603828430176\n",
      "E loss:  0.8994626998901367\n",
      "G loss: 1.7664345502853394\n",
      "Training Model  ...\n",
      "E loss:  0.8991482257843018\n",
      "G loss: 1.852768063545227\n",
      "E loss:  0.9013285636901855\n",
      "G loss: 1.7437524795532227\n",
      "E loss:  0.8878880143165588\n",
      "G loss: 1.820156455039978\n",
      "E loss:  0.8901568651199341\n",
      "G loss: 1.688204050064087\n",
      "E loss:  0.8961176872253418\n",
      "G loss: 1.697822093963623\n",
      "Training Model  ...\n",
      "E loss:  0.8503797054290771\n",
      "G loss: 1.7212265729904175\n",
      "E loss:  0.8486371040344238\n",
      "G loss: 1.6746981143951416\n",
      "E loss:  0.8495936393737793\n",
      "G loss: 1.7692725658416748\n",
      "E loss:  0.8462153673171997\n",
      "G loss: 1.7617467641830444\n",
      "E loss:  0.8346449732780457\n",
      "G loss: 1.740368127822876\n",
      "Training Model  ...\n",
      "E loss:  0.9121440052986145\n",
      "G loss: 1.830151081085205\n",
      "E loss:  0.896597146987915\n",
      "G loss: 1.7526074647903442\n",
      "E loss:  0.9058024883270264\n",
      "G loss: 1.7908896207809448\n",
      "E loss:  0.9104061722755432\n",
      "G loss: 1.776145100593567\n",
      "E loss:  0.8996776938438416\n",
      "G loss: 1.815383791923523\n",
      "Training Model  ...\n",
      "E loss:  0.9675805568695068\n",
      "G loss: 1.821152687072754\n",
      "E loss:  0.9358698725700378\n",
      "G loss: 1.7891490459442139\n",
      "E loss:  0.947137176990509\n",
      "G loss: 1.8456439971923828\n",
      "E loss:  0.943573534488678\n",
      "G loss: 1.813664436340332\n",
      "E loss:  0.9308053851127625\n",
      "G loss: 1.8924229145050049\n",
      "Training Model  ...\n",
      "E loss:  0.875863254070282\n",
      "G loss: 1.8751864433288574\n",
      "E loss:  0.8762543797492981\n",
      "G loss: 1.8399920463562012\n",
      "E loss:  0.8720633387565613\n",
      "G loss: 1.8981627225875854\n",
      "E loss:  0.8451849818229675\n",
      "G loss: 1.9019126892089844\n",
      "E loss:  0.8417671918869019\n",
      "G loss: 1.8119335174560547\n",
      "Training Model  ...\n",
      "E loss:  0.9332882761955261\n",
      "G loss: 1.8780202865600586\n",
      "E loss:  0.9489096999168396\n",
      "G loss: 1.8490259647369385\n",
      "E loss:  0.9355254173278809\n",
      "G loss: 1.8293596506118774\n",
      "E loss:  0.927290678024292\n",
      "G loss: 1.8405669927597046\n",
      "E loss:  0.9395219683647156\n",
      "G loss: 1.7357462644577026\n",
      "Training Model  ...\n",
      "E loss:  0.9026929140090942\n",
      "G loss: 1.7753849029541016\n",
      "E loss:  0.8903620839118958\n",
      "G loss: 1.776619791984558\n",
      "E loss:  0.8768346905708313\n",
      "G loss: 1.7241649627685547\n",
      "E loss:  0.8617762327194214\n",
      "G loss: 1.7689332962036133\n",
      "E loss:  0.8588244915008545\n",
      "G loss: 1.712577223777771\n",
      "Training Model  ...\n",
      "E loss:  0.998772382736206\n",
      "G loss: 1.7666667699813843\n",
      "E loss:  0.9813293814659119\n",
      "G loss: 1.7111274003982544\n",
      "E loss:  0.9703872203826904\n",
      "G loss: 1.7696490287780762\n",
      "E loss:  0.9500729441642761\n",
      "G loss: 1.8838891983032227\n",
      "E loss:  0.9649484157562256\n",
      "G loss: 1.9499523639678955\n",
      "Training Model  ...\n",
      "E loss:  0.8975229859352112\n",
      "G loss: 1.8620102405548096\n",
      "E loss:  0.887327253818512\n",
      "G loss: 1.780807375907898\n",
      "E loss:  0.9074194431304932\n",
      "G loss: 1.8634462356567383\n",
      "E loss:  0.8959133625030518\n",
      "G loss: 1.7601847648620605\n",
      "E loss:  0.8699002265930176\n",
      "G loss: 1.7725729942321777\n",
      "Training Model  ...\n",
      "E loss:  0.8255727887153625\n",
      "G loss: 1.8440098762512207\n",
      "E loss:  0.8389065861701965\n",
      "G loss: 1.7338207960128784\n",
      "E loss:  0.8398618102073669\n",
      "G loss: 1.8033859729766846\n",
      "E loss:  0.8534533977508545\n",
      "G loss: 1.9273947477340698\n",
      "E loss:  0.8376021385192871\n",
      "G loss: 1.956196665763855\n",
      "Training Model  ...\n",
      "E loss:  0.9226440191268921\n",
      "G loss: 1.87442946434021\n",
      "E loss:  0.9237672090530396\n",
      "G loss: 1.8789663314819336\n",
      "E loss:  0.9048033952713013\n",
      "G loss: 1.8309475183486938\n",
      "E loss:  0.8968117833137512\n",
      "G loss: 1.8028697967529297\n",
      "E loss:  0.8910676836967468\n",
      "G loss: 1.8838192224502563\n",
      "Training Model  ...\n",
      "E loss:  0.8849530220031738\n",
      "G loss: 1.7947121858596802\n",
      "E loss:  0.8947943449020386\n",
      "G loss: 1.8358763456344604\n",
      "E loss:  0.8907036781311035\n",
      "G loss: 1.8974578380584717\n",
      "E loss:  0.9238505363464355\n",
      "G loss: 1.931443691253662\n",
      "E loss:  0.9006178379058838\n",
      "G loss: 1.9156570434570312\n",
      "Training Model  ...\n",
      "E loss:  1.0371733903884888\n",
      "G loss: 1.9486585855484009\n",
      "E loss:  1.0446525812149048\n",
      "G loss: 1.9546923637390137\n",
      "E loss:  1.0409504175186157\n",
      "G loss: 1.8675529956817627\n",
      "E loss:  1.02102792263031\n",
      "G loss: 1.9203320741653442\n",
      "E loss:  1.0250968933105469\n",
      "G loss: 1.9138598442077637\n",
      "Training Model  ...\n",
      "E loss:  0.9199671745300293\n",
      "G loss: 1.8952665328979492\n",
      "E loss:  0.9353060722351074\n",
      "G loss: 1.935882806777954\n",
      "E loss:  0.9072778224945068\n",
      "G loss: 1.8745745420455933\n",
      "E loss:  0.9309832453727722\n",
      "G loss: 1.8927137851715088\n",
      "E loss:  0.9289867877960205\n",
      "G loss: 1.8366875648498535\n",
      "Training Model  ...\n",
      "E loss:  0.9489696025848389\n",
      "G loss: 1.8682992458343506\n",
      "E loss:  0.9561207890510559\n",
      "G loss: 1.930126428604126\n",
      "E loss:  0.9665406346321106\n",
      "G loss: 2.0142948627471924\n",
      "E loss:  0.9676195979118347\n",
      "G loss: 2.0112786293029785\n",
      "E loss:  0.9560067653656006\n",
      "G loss: 2.0799508094787598\n",
      "Training Model  ...\n",
      "E loss:  0.9343833923339844\n",
      "G loss: 2.0291154384613037\n",
      "E loss:  0.9187063574790955\n",
      "G loss: 1.957301139831543\n",
      "E loss:  0.9395525455474854\n",
      "G loss: 1.9962990283966064\n",
      "E loss:  0.9195126891136169\n",
      "G loss: 1.9439102411270142\n",
      "E loss:  0.9261939525604248\n",
      "G loss: 1.8793590068817139\n",
      "Training Model  ...\n",
      "E loss:  0.9390478730201721\n",
      "G loss: 1.8415606021881104\n",
      "E loss:  0.9354459047317505\n",
      "G loss: 1.8169368505477905\n",
      "E loss:  0.9332282543182373\n",
      "G loss: 1.8441472053527832\n",
      "E loss:  0.9230520129203796\n",
      "G loss: 1.7704224586486816\n",
      "E loss:  0.9347048997879028\n",
      "G loss: 1.7213164567947388\n",
      "Training Model  ...\n",
      "E loss:  0.9082051515579224\n",
      "G loss: 1.771382212638855\n",
      "E loss:  0.9262412786483765\n",
      "G loss: 1.8705902099609375\n",
      "E loss:  0.9288002252578735\n",
      "G loss: 1.8619270324707031\n",
      "E loss:  0.92159104347229\n",
      "G loss: 1.967602252960205\n",
      "E loss:  0.9424012899398804\n",
      "G loss: 2.0790421962738037\n",
      "Training Model  ...\n",
      "E loss:  0.8423643112182617\n",
      "G loss: 2.038848400115967\n",
      "E loss:  0.8504003286361694\n",
      "G loss: 2.0330960750579834\n",
      "E loss:  0.844882071018219\n",
      "G loss: 1.9723470211029053\n",
      "E loss:  0.8432153463363647\n",
      "G loss: 1.9891300201416016\n",
      "E loss:  0.8326979279518127\n",
      "G loss: 1.9444608688354492\n",
      "Training Model  ...\n",
      "E loss:  0.971469521522522\n",
      "G loss: 2.005758047103882\n",
      "E loss:  0.9671973586082458\n",
      "G loss: 1.9348337650299072\n",
      "E loss:  0.9539209604263306\n",
      "G loss: 1.8365399837493896\n",
      "E loss:  0.9265057444572449\n",
      "G loss: 1.9333903789520264\n",
      "E loss:  0.9161232709884644\n",
      "G loss: 1.915958046913147\n",
      "Training Model  ...\n",
      "E loss:  0.8764649629592896\n",
      "G loss: 1.8731672763824463\n",
      "E loss:  0.8755521178245544\n",
      "G loss: 1.8274552822113037\n",
      "E loss:  0.8757156133651733\n",
      "G loss: 1.8130474090576172\n",
      "E loss:  0.877478837966919\n",
      "G loss: 1.784923791885376\n",
      "E loss:  0.8696800470352173\n",
      "G loss: 1.745970368385315\n",
      "Training Model  ...\n",
      "E loss:  0.8435477614402771\n",
      "G loss: 1.8363829851150513\n",
      "E loss:  0.8402908444404602\n",
      "G loss: 1.825010061264038\n",
      "E loss:  0.8229172229766846\n",
      "G loss: 1.8497627973556519\n",
      "E loss:  0.8394768238067627\n",
      "G loss: 1.9513015747070312\n",
      "E loss:  0.8440256714820862\n",
      "G loss: 1.8501263856887817\n",
      "Training Model  ...\n",
      "E loss:  0.9663745164871216\n",
      "G loss: 1.8540167808532715\n",
      "E loss:  0.9658814072608948\n",
      "G loss: 1.9100145101547241\n",
      "E loss:  0.9558177590370178\n",
      "G loss: 1.9496368169784546\n",
      "E loss:  0.9450806975364685\n",
      "G loss: 1.9015462398529053\n",
      "E loss:  0.9353236556053162\n",
      "G loss: 1.941056251525879\n",
      "Training Model  ...\n",
      "E loss:  1.002522587776184\n",
      "G loss: 1.9877262115478516\n",
      "E loss:  0.9946699142456055\n",
      "G loss: 1.9323279857635498\n",
      "E loss:  0.9683923125267029\n",
      "G loss: 1.9352340698242188\n",
      "E loss:  0.9676238894462585\n",
      "G loss: 1.7976547479629517\n",
      "E loss:  0.9449401497840881\n",
      "G loss: 1.9030994176864624\n",
      "Training Model  ...\n",
      "E loss:  0.9410840272903442\n",
      "G loss: 1.8467087745666504\n",
      "E loss:  0.9427226781845093\n",
      "G loss: 1.8203763961791992\n",
      "E loss:  0.9443824291229248\n",
      "G loss: 1.8149828910827637\n",
      "E loss:  0.9779428839683533\n",
      "G loss: 1.8423391580581665\n",
      "E loss:  0.99586021900177\n",
      "G loss: 1.8594183921813965\n",
      "Training Model  ...\n",
      "E loss:  0.9174062013626099\n",
      "G loss: 1.7646780014038086\n",
      "E loss:  0.8962570428848267\n",
      "G loss: 1.7415411472320557\n",
      "E loss:  0.9260963797569275\n",
      "G loss: 1.876914620399475\n",
      "E loss:  0.9166159629821777\n",
      "G loss: 1.966827630996704\n",
      "E loss:  0.9055178761482239\n",
      "G loss: 2.0417494773864746\n",
      "Training Model  ...\n",
      "E loss:  0.9152665734291077\n",
      "G loss: 1.9856395721435547\n",
      "E loss:  0.9109458327293396\n",
      "G loss: 1.877395749092102\n",
      "E loss:  0.9182072281837463\n",
      "G loss: 1.79390287399292\n",
      "E loss:  0.9012345671653748\n",
      "G loss: 1.675036072731018\n",
      "E loss:  0.8849791288375854\n",
      "G loss: 1.6654736995697021\n",
      "Training Model  ...\n",
      "E loss:  1.0047941207885742\n",
      "G loss: 1.7021241188049316\n",
      "E loss:  0.9865446090698242\n",
      "G loss: 1.7654236555099487\n",
      "E loss:  0.9779443740844727\n",
      "G loss: 1.8232481479644775\n",
      "E loss:  0.9684246778488159\n",
      "G loss: 1.8999884128570557\n",
      "E loss:  0.962821364402771\n",
      "G loss: 2.0861377716064453\n",
      "Training Model  ...\n",
      "E loss:  0.8950458765029907\n",
      "G loss: 2.011596202850342\n",
      "E loss:  0.897621214389801\n",
      "G loss: 1.985180377960205\n",
      "E loss:  0.9067034721374512\n",
      "G loss: 1.9657580852508545\n",
      "E loss:  0.9049516916275024\n",
      "G loss: 2.0294547080993652\n",
      "E loss:  0.9076931476593018\n",
      "G loss: 2.0496089458465576\n",
      "Training Model  ...\n",
      "E loss:  0.8996286988258362\n",
      "G loss: 2.0840811729431152\n",
      "E loss:  0.8868157863616943\n",
      "G loss: 2.0789196491241455\n",
      "E loss:  0.9237060546875\n",
      "G loss: 2.118980646133423\n",
      "E loss:  0.893841564655304\n",
      "G loss: 2.0908892154693604\n",
      "E loss:  0.875576913356781\n",
      "G loss: 2.128999948501587\n",
      "Training Model  ...\n",
      "E loss:  0.9681757688522339\n",
      "G loss: 2.1170806884765625\n",
      "E loss:  1.0035338401794434\n",
      "G loss: 2.0313334465026855\n",
      "E loss:  0.9913252592086792\n",
      "G loss: 1.9328219890594482\n",
      "E loss:  0.9705617427825928\n",
      "G loss: 1.8187429904937744\n",
      "E loss:  0.9697078466415405\n",
      "G loss: 1.690683364868164\n",
      "Training Model  ...\n",
      "E loss:  0.8848520517349243\n",
      "G loss: 1.7566227912902832\n",
      "E loss:  0.9028236865997314\n",
      "G loss: 1.72471284866333\n",
      "E loss:  0.9070886373519897\n",
      "G loss: 1.7799495458602905\n",
      "E loss:  0.8809226751327515\n",
      "G loss: 1.802601933479309\n",
      "E loss:  0.8665079474449158\n",
      "G loss: 1.822106122970581\n",
      "Training Model  ...\n",
      "E loss:  0.9762986898422241\n",
      "G loss: 1.8393949270248413\n",
      "E loss:  0.9560074210166931\n",
      "G loss: 1.8865742683410645\n",
      "E loss:  0.9768847227096558\n",
      "G loss: 1.9282225370407104\n",
      "E loss:  0.9746503829956055\n",
      "G loss: 1.8931424617767334\n",
      "E loss:  0.9789459109306335\n",
      "G loss: 1.982752799987793\n",
      "Training Model  ...\n",
      "E loss:  0.9980946779251099\n",
      "G loss: 2.093937873840332\n",
      "E loss:  0.9642230868339539\n",
      "G loss: 2.0742814540863037\n",
      "E loss:  0.9792298674583435\n",
      "G loss: 2.0305063724517822\n",
      "E loss:  0.9847921133041382\n",
      "G loss: 2.1357343196868896\n",
      "E loss:  1.0000903606414795\n",
      "G loss: 2.134431838989258\n",
      "Training Model  ...\n",
      "E loss:  0.8758480548858643\n",
      "G loss: 2.1389405727386475\n",
      "E loss:  0.8527371287345886\n",
      "G loss: 2.1121044158935547\n",
      "E loss:  0.8520815372467041\n",
      "G loss: 2.0029358863830566\n",
      "E loss:  0.8465036153793335\n",
      "G loss: 1.9891068935394287\n",
      "E loss:  0.828506588935852\n",
      "G loss: 1.9518500566482544\n",
      "Training Model  ...\n",
      "E loss:  0.9264705181121826\n",
      "G loss: 1.9045912027359009\n",
      "E loss:  0.9067226648330688\n",
      "G loss: 1.8488926887512207\n",
      "E loss:  0.9256088733673096\n",
      "G loss: 1.9296156167984009\n",
      "E loss:  0.9271366596221924\n",
      "G loss: 1.9295284748077393\n",
      "E loss:  0.9146497249603271\n",
      "G loss: 1.962281584739685\n",
      "Training Model  ...\n",
      "E loss:  0.9249948859214783\n",
      "G loss: 1.9518214464187622\n",
      "E loss:  0.9153487086296082\n",
      "G loss: 1.900058627128601\n",
      "E loss:  0.9080963134765625\n",
      "G loss: 2.0604076385498047\n",
      "E loss:  0.922131359577179\n",
      "G loss: 2.007852554321289\n",
      "E loss:  0.9194155931472778\n",
      "G loss: 2.0327537059783936\n",
      "Training Model  ...\n",
      "E loss:  0.8140373229980469\n",
      "G loss: 2.02193021774292\n",
      "E loss:  0.8158478140830994\n",
      "G loss: 2.0339810848236084\n",
      "E loss:  0.8108290433883667\n",
      "G loss: 1.9728550910949707\n",
      "E loss:  0.8163003325462341\n",
      "G loss: 1.9788142442703247\n",
      "E loss:  0.8215476274490356\n",
      "G loss: 2.007484197616577\n",
      "Training Model  ...\n",
      "E loss:  0.9515174031257629\n",
      "G loss: 1.8936904668807983\n",
      "E loss:  0.9450617432594299\n",
      "G loss: 1.8951225280761719\n",
      "E loss:  0.937315046787262\n",
      "G loss: 1.904134750366211\n",
      "E loss:  0.9468063712120056\n",
      "G loss: 1.8428269624710083\n",
      "E loss:  0.9272566437721252\n",
      "G loss: 1.8266751766204834\n",
      "Training Model  ...\n",
      "E loss:  0.849445641040802\n",
      "G loss: 1.857085943222046\n",
      "E loss:  0.8459523320198059\n",
      "G loss: 1.8616507053375244\n",
      "E loss:  0.8337867259979248\n",
      "G loss: 1.8470699787139893\n",
      "E loss:  0.8434382677078247\n",
      "G loss: 1.954576849937439\n",
      "E loss:  0.8401017785072327\n",
      "G loss: 1.9369699954986572\n",
      "Training Model  ...\n",
      "E loss:  0.8447124361991882\n",
      "G loss: 1.9527677297592163\n",
      "E loss:  0.8354064226150513\n",
      "G loss: 1.9975205659866333\n",
      "E loss:  0.8412892818450928\n",
      "G loss: 1.8520163297653198\n",
      "E loss:  0.8543708920478821\n",
      "G loss: 1.9412195682525635\n",
      "E loss:  0.8638814091682434\n",
      "G loss: 2.034604072570801\n",
      "Training Model  ...\n",
      "E loss:  0.8785607218742371\n",
      "G loss: 1.9922289848327637\n",
      "E loss:  0.87264084815979\n",
      "G loss: 1.9965591430664062\n",
      "E loss:  0.8565895557403564\n",
      "G loss: 1.9269789457321167\n",
      "E loss:  0.8557635545730591\n",
      "G loss: 1.9153248071670532\n",
      "E loss:  0.8492072224617004\n",
      "G loss: 1.9742488861083984\n",
      "Training Model  ...\n",
      "E loss:  0.8586176037788391\n",
      "G loss: 2.0054173469543457\n",
      "E loss:  0.858449399471283\n",
      "G loss: 1.9873958826065063\n",
      "E loss:  0.8543155193328857\n",
      "G loss: 1.9631714820861816\n",
      "E loss:  0.8637136816978455\n",
      "G loss: 1.9524072408676147\n",
      "E loss:  0.8656975030899048\n",
      "G loss: 1.9070981740951538\n",
      "Training Model  ...\n",
      "E loss:  0.935890793800354\n",
      "G loss: 1.9653291702270508\n",
      "E loss:  0.9341610670089722\n",
      "G loss: 1.9235398769378662\n",
      "E loss:  0.9391965270042419\n",
      "G loss: 1.8237168788909912\n",
      "E loss:  0.9463973045349121\n",
      "G loss: 1.7096288204193115\n",
      "E loss:  0.9432860612869263\n",
      "G loss: 1.7212326526641846\n",
      "Training Model  ...\n",
      "E loss:  0.8107374906539917\n",
      "G loss: 1.6827788352966309\n",
      "E loss:  0.8214936852455139\n",
      "G loss: 1.645801067352295\n",
      "E loss:  0.8154983520507812\n",
      "G loss: 1.696906328201294\n",
      "E loss:  0.822526216506958\n",
      "G loss: 1.6048842668533325\n",
      "E loss:  0.8400012254714966\n",
      "G loss: 1.7163974046707153\n",
      "Training Model  ...\n",
      "E loss:  0.906394898891449\n",
      "G loss: 1.6549460887908936\n",
      "E loss:  0.9307750463485718\n",
      "G loss: 1.6831107139587402\n",
      "E loss:  0.9364042282104492\n",
      "G loss: 1.7003921270370483\n",
      "E loss:  0.9174844622612\n",
      "G loss: 1.743646264076233\n",
      "E loss:  0.9177353382110596\n",
      "G loss: 1.8288724422454834\n",
      "Training Model  ...\n",
      "E loss:  0.9851855635643005\n",
      "G loss: 1.831824779510498\n",
      "E loss:  0.9702690243721008\n",
      "G loss: 1.8146584033966064\n",
      "E loss:  0.9650881290435791\n",
      "G loss: 1.9233765602111816\n",
      "E loss:  0.9802044630050659\n",
      "G loss: 1.9621970653533936\n",
      "E loss:  0.956960916519165\n",
      "G loss: 1.9581589698791504\n",
      "Training Model  ...\n",
      "E loss:  0.9788761138916016\n",
      "G loss: 1.9102789163589478\n",
      "E loss:  0.9543977975845337\n",
      "G loss: 1.9748530387878418\n",
      "E loss:  0.9538629651069641\n",
      "G loss: 1.9665182828903198\n",
      "E loss:  0.9534051418304443\n",
      "G loss: 1.928942084312439\n",
      "E loss:  0.945567786693573\n",
      "G loss: 1.9408252239227295\n",
      "Training Model  ...\n",
      "E loss:  0.8810725212097168\n",
      "G loss: 1.9865882396697998\n",
      "E loss:  0.8643413782119751\n",
      "G loss: 1.9656516313552856\n",
      "E loss:  0.8664032220840454\n",
      "G loss: 1.9869722127914429\n",
      "E loss:  0.8673383593559265\n",
      "G loss: 1.9266197681427002\n",
      "E loss:  0.8883463144302368\n",
      "G loss: 1.928620457649231\n",
      "Training Model  ...\n",
      "E loss:  0.9261606931686401\n",
      "G loss: 1.8765923976898193\n",
      "E loss:  0.9239798188209534\n",
      "G loss: 1.8387091159820557\n",
      "E loss:  0.9357001781463623\n",
      "G loss: 1.7399208545684814\n",
      "E loss:  0.9086397290229797\n",
      "G loss: 1.6698582172393799\n",
      "E loss:  0.9145305156707764\n",
      "G loss: 1.6019939184188843\n",
      "Training Model  ...\n",
      "E loss:  1.0423249006271362\n",
      "G loss: 1.6847283840179443\n",
      "E loss:  1.0373870134353638\n",
      "G loss: 1.715505599975586\n",
      "E loss:  1.0248684883117676\n",
      "G loss: 1.8153643608093262\n",
      "E loss:  0.9939947128295898\n",
      "G loss: 1.8896687030792236\n",
      "E loss:  0.9987326860427856\n",
      "G loss: 2.096851348876953\n",
      "Training Model  ...\n",
      "E loss:  0.95835942029953\n",
      "G loss: 2.0149285793304443\n",
      "E loss:  0.9387719631195068\n",
      "G loss: 1.9751787185668945\n",
      "E loss:  0.9335529208183289\n",
      "G loss: 1.7976244688034058\n",
      "E loss:  0.929944634437561\n",
      "G loss: 1.76711905002594\n",
      "E loss:  0.9188849925994873\n",
      "G loss: 1.7556185722351074\n",
      "Training Model  ...\n",
      "E loss:  0.8914002180099487\n",
      "G loss: 1.7555066347122192\n",
      "E loss:  0.8972575664520264\n",
      "G loss: 1.7770226001739502\n",
      "E loss:  0.8837680816650391\n",
      "G loss: 1.9022448062896729\n",
      "E loss:  0.8884727954864502\n",
      "G loss: 1.9817616939544678\n",
      "E loss:  0.8700006604194641\n",
      "G loss: 2.1142046451568604\n",
      "Training Model  ...\n",
      "E loss:  0.9331443309783936\n",
      "G loss: 2.074326992034912\n",
      "E loss:  0.9138551354408264\n",
      "G loss: 2.066499710083008\n",
      "E loss:  0.9196414947509766\n",
      "G loss: 2.044245719909668\n",
      "E loss:  0.9136454463005066\n",
      "G loss: 1.8327724933624268\n",
      "E loss:  0.9055251479148865\n",
      "G loss: 1.8551084995269775\n",
      "Training Model  ...\n",
      "E loss:  0.9523442983627319\n",
      "G loss: 1.9371305704116821\n",
      "E loss:  0.9450535774230957\n",
      "G loss: 1.9064064025878906\n",
      "E loss:  0.9329358339309692\n",
      "G loss: 1.8724435567855835\n",
      "E loss:  0.9308675527572632\n",
      "G loss: 1.875333309173584\n",
      "E loss:  0.9056366682052612\n",
      "G loss: 1.88462495803833\n",
      "Training Model  ...\n",
      "E loss:  0.8371701240539551\n",
      "G loss: 1.8454186916351318\n",
      "E loss:  0.8127163648605347\n",
      "G loss: 1.9193638563156128\n",
      "E loss:  0.8217765092849731\n",
      "G loss: 1.993179440498352\n",
      "E loss:  0.8303967714309692\n",
      "G loss: 1.9415209293365479\n",
      "E loss:  0.8385244607925415\n",
      "G loss: 1.9379935264587402\n",
      "Training Model  ...\n",
      "E loss:  0.8696452975273132\n",
      "G loss: 1.831026554107666\n",
      "E loss:  0.8684890866279602\n",
      "G loss: 1.972930669784546\n",
      "E loss:  0.8532708883285522\n",
      "G loss: 1.9445630311965942\n",
      "E loss:  0.8527095317840576\n",
      "G loss: 1.9289863109588623\n",
      "E loss:  0.8397843241691589\n",
      "G loss: 2.00630521774292\n",
      "Training Model  ...\n",
      "E loss:  0.9331410527229309\n",
      "G loss: 1.9283466339111328\n",
      "E loss:  0.9274866580963135\n",
      "G loss: 1.9664247035980225\n",
      "E loss:  0.9311552047729492\n",
      "G loss: 1.860052466392517\n",
      "E loss:  0.9364239573478699\n",
      "G loss: 1.8546006679534912\n",
      "E loss:  0.9442687034606934\n",
      "G loss: 1.9763212203979492\n",
      "Training Model  ...\n",
      "E loss:  0.8196948170661926\n",
      "G loss: 1.974267840385437\n",
      "E loss:  0.8080907464027405\n",
      "G loss: 1.9792981147766113\n",
      "E loss:  0.7932051420211792\n",
      "G loss: 1.9487526416778564\n",
      "E loss:  0.7840822339057922\n",
      "G loss: 1.9879344701766968\n",
      "E loss:  0.776077389717102\n",
      "G loss: 2.047417640686035\n",
      "Training Model  ...\n",
      "E loss:  1.005363941192627\n",
      "G loss: 1.9114023447036743\n",
      "E loss:  0.9837561845779419\n",
      "G loss: 1.8917254209518433\n",
      "E loss:  0.9860759377479553\n",
      "G loss: 1.968285322189331\n",
      "E loss:  0.9905981421470642\n",
      "G loss: 1.8354263305664062\n",
      "E loss:  0.9792361259460449\n",
      "G loss: 1.8966548442840576\n",
      "Training Model  ...\n",
      "E loss:  0.8241302967071533\n",
      "G loss: 1.8960400819778442\n",
      "E loss:  0.8228592276573181\n",
      "G loss: 1.9830776453018188\n",
      "E loss:  0.8227816820144653\n",
      "G loss: 1.8925342559814453\n",
      "E loss:  0.820368230342865\n",
      "G loss: 1.9372789859771729\n",
      "E loss:  0.8284059762954712\n",
      "G loss: 1.8805224895477295\n",
      "Training Model  ...\n",
      "E loss:  0.8523383140563965\n",
      "G loss: 1.925601601600647\n",
      "E loss:  0.8517688512802124\n",
      "G loss: 1.9002916812896729\n",
      "E loss:  0.8453444242477417\n",
      "G loss: 1.8855869770050049\n",
      "E loss:  0.8385465145111084\n",
      "G loss: 1.708606481552124\n",
      "E loss:  0.830072283744812\n",
      "G loss: 1.7255570888519287\n",
      "Training Model  ...\n",
      "E loss:  0.8492076396942139\n",
      "G loss: 1.6866207122802734\n",
      "E loss:  0.8533253073692322\n",
      "G loss: 1.7717163562774658\n",
      "E loss:  0.8461898565292358\n",
      "G loss: 1.7840697765350342\n",
      "E loss:  0.8406307101249695\n",
      "G loss: 1.685036063194275\n",
      "E loss:  0.8514500856399536\n",
      "G loss: 1.7914410829544067\n",
      "Training Model  ...\n",
      "E loss:  0.8420665264129639\n",
      "G loss: 1.9081486463546753\n",
      "E loss:  0.831596851348877\n",
      "G loss: 1.9175915718078613\n",
      "E loss:  0.8144472241401672\n",
      "G loss: 1.9136106967926025\n",
      "E loss:  0.8220975995063782\n",
      "G loss: 1.8860142230987549\n",
      "E loss:  0.8107008934020996\n",
      "G loss: 1.8507722616195679\n",
      "Training Model  ...\n",
      "E loss:  0.8656544089317322\n",
      "G loss: 1.8844752311706543\n",
      "E loss:  0.8745096325874329\n",
      "G loss: 1.8788762092590332\n",
      "E loss:  0.8673774600028992\n",
      "G loss: 1.8779929876327515\n",
      "E loss:  0.8596595525741577\n",
      "G loss: 1.8589640855789185\n",
      "E loss:  0.8475517630577087\n",
      "G loss: 1.9254436492919922\n",
      "Training Model  ...\n",
      "E loss:  0.8389492034912109\n",
      "G loss: 1.8654357194900513\n",
      "E loss:  0.8095583915710449\n",
      "G loss: 1.8782970905303955\n",
      "E loss:  0.8081641793251038\n",
      "G loss: 1.9304795265197754\n",
      "E loss:  0.849612295627594\n",
      "G loss: 1.9911028146743774\n",
      "E loss:  0.8274564743041992\n",
      "G loss: 1.8643035888671875\n",
      "Training Model  ...\n",
      "E loss:  0.8793947696685791\n",
      "G loss: 1.8410753011703491\n",
      "E loss:  0.8634949326515198\n",
      "G loss: 1.8539445400238037\n",
      "E loss:  0.863451361656189\n",
      "G loss: 1.8080838918685913\n",
      "E loss:  0.8587294816970825\n",
      "G loss: 1.854734182357788\n",
      "E loss:  0.8770390748977661\n",
      "G loss: 1.8299822807312012\n",
      "Training Model  ...\n",
      "E loss:  0.9323102235794067\n",
      "G loss: 1.8551230430603027\n",
      "E loss:  0.9061753153800964\n",
      "G loss: 1.8734495639801025\n",
      "E loss:  0.9191280007362366\n",
      "G loss: 1.8489269018173218\n",
      "E loss:  0.9236237406730652\n",
      "G loss: 1.8277968168258667\n",
      "E loss:  0.9271186590194702\n",
      "G loss: 1.894973874092102\n",
      "Training Model  ...\n",
      "E loss:  0.8574163913726807\n",
      "G loss: 1.814480185508728\n",
      "E loss:  0.8417020440101624\n",
      "G loss: 1.9259029626846313\n",
      "E loss:  0.8454945087432861\n",
      "G loss: 1.8870707750320435\n",
      "E loss:  0.8367106914520264\n",
      "G loss: 1.800377607345581\n",
      "E loss:  0.8372864723205566\n",
      "G loss: 1.7975399494171143\n",
      "Training Model  ...\n",
      "E loss:  0.8724058270454407\n",
      "G loss: 1.8256272077560425\n",
      "E loss:  0.8702998757362366\n",
      "G loss: 1.8657077550888062\n",
      "E loss:  0.890602171421051\n",
      "G loss: 1.9015142917633057\n",
      "E loss:  0.8889299631118774\n",
      "G loss: 1.8758071660995483\n",
      "E loss:  0.894173264503479\n",
      "G loss: 1.9122062921524048\n",
      "Training Model  ...\n",
      "E loss:  0.8191579580307007\n",
      "G loss: 1.8734102249145508\n",
      "E loss:  0.8124262690544128\n",
      "G loss: 1.8556301593780518\n",
      "E loss:  0.8222007155418396\n",
      "G loss: 1.9069461822509766\n",
      "E loss:  0.81654953956604\n",
      "G loss: 1.7878620624542236\n",
      "E loss:  0.8199185132980347\n",
      "G loss: 1.8682537078857422\n",
      "Training Model  ...\n",
      "E loss:  0.945822536945343\n",
      "G loss: 1.8989367485046387\n",
      "E loss:  0.9762979745864868\n",
      "G loss: 1.9947831630706787\n",
      "E loss:  0.9535884857177734\n",
      "G loss: 1.7917625904083252\n",
      "E loss:  0.9564989805221558\n",
      "G loss: 1.919870376586914\n",
      "E loss:  0.9592666625976562\n",
      "G loss: 1.7966387271881104\n",
      "Training Model  ...\n",
      "E loss:  0.7580282688140869\n",
      "G loss: 1.9118789434432983\n",
      "E loss:  0.7625057697296143\n",
      "G loss: 1.9392540454864502\n",
      "E loss:  0.7320668697357178\n",
      "G loss: 1.9302631616592407\n",
      "E loss:  0.752855122089386\n",
      "G loss: 1.9363864660263062\n",
      "E loss:  0.7464728355407715\n",
      "G loss: 1.9879753589630127\n",
      "Training Model  ...\n",
      "E loss:  0.9189324975013733\n",
      "G loss: 1.9797853231430054\n",
      "E loss:  0.8947522640228271\n",
      "G loss: 1.8605725765228271\n",
      "E loss:  0.8766090273857117\n",
      "G loss: 1.8040552139282227\n",
      "E loss:  0.8824465274810791\n",
      "G loss: 1.6873815059661865\n",
      "E loss:  0.8842087388038635\n",
      "G loss: 1.6301767826080322\n",
      "Training Model  ...\n",
      "E loss:  0.7477788329124451\n",
      "G loss: 1.658006191253662\n",
      "E loss:  0.7528464794158936\n",
      "G loss: 1.7096781730651855\n",
      "E loss:  0.7447710037231445\n",
      "G loss: 1.715622901916504\n",
      "E loss:  0.7529648542404175\n",
      "G loss: 1.7520053386688232\n",
      "E loss:  0.7447205781936646\n",
      "G loss: 1.869635820388794\n",
      "Training Model  ...\n",
      "E loss:  0.7998335361480713\n",
      "G loss: 1.8111789226531982\n",
      "E loss:  0.811866819858551\n",
      "G loss: 1.884004831314087\n",
      "E loss:  0.8407371044158936\n",
      "G loss: 1.8683688640594482\n",
      "E loss:  0.8452959656715393\n",
      "G loss: 1.7811459302902222\n",
      "E loss:  0.833565890789032\n",
      "G loss: 1.7271649837493896\n",
      "Training Model  ...\n",
      "E loss:  0.8447458744049072\n",
      "G loss: 1.76488196849823\n",
      "E loss:  0.8504205942153931\n",
      "G loss: 1.8034892082214355\n",
      "E loss:  0.8706867694854736\n",
      "G loss: 1.894337773323059\n",
      "E loss:  0.8716464638710022\n",
      "G loss: 1.8791823387145996\n",
      "E loss:  0.8718201518058777\n",
      "G loss: 1.906146764755249\n",
      "Training Model  ...\n",
      "E loss:  0.8262363076210022\n",
      "G loss: 2.0000040531158447\n",
      "E loss:  0.8276587724685669\n",
      "G loss: 1.9398326873779297\n",
      "E loss:  0.833450973033905\n",
      "G loss: 1.870987892150879\n",
      "E loss:  0.8210681080818176\n",
      "G loss: 1.7621532678604126\n",
      "E loss:  0.8305425643920898\n",
      "G loss: 1.718687891960144\n",
      "Training Model  ...\n",
      "E loss:  0.7849218845367432\n",
      "G loss: 1.7542613744735718\n",
      "E loss:  0.7499611377716064\n",
      "G loss: 1.7119438648223877\n",
      "E loss:  0.7475823163986206\n",
      "G loss: 1.7331135272979736\n",
      "E loss:  0.7382736206054688\n",
      "G loss: 1.751780390739441\n",
      "E loss:  0.7318288683891296\n",
      "G loss: 1.6394286155700684\n",
      "Training Model  ...\n",
      "E loss:  0.9188991785049438\n",
      "G loss: 1.6491057872772217\n",
      "E loss:  0.892521321773529\n",
      "G loss: 1.7628815174102783\n",
      "E loss:  0.8973763585090637\n",
      "G loss: 1.8302319049835205\n",
      "E loss:  0.9035278558731079\n",
      "G loss: 1.8115804195404053\n",
      "E loss:  0.885837972164154\n",
      "G loss: 1.8002173900604248\n",
      "Training Model  ...\n",
      "E loss:  0.9097833633422852\n",
      "G loss: 1.8153393268585205\n",
      "E loss:  0.8950860500335693\n",
      "G loss: 1.8687041997909546\n",
      "E loss:  0.8910459876060486\n",
      "G loss: 1.7869057655334473\n",
      "E loss:  0.9001404047012329\n",
      "G loss: 1.7928509712219238\n",
      "E loss:  0.904996395111084\n",
      "G loss: 1.6610761880874634\n",
      "Training Model  ...\n",
      "E loss:  0.8009362816810608\n",
      "G loss: 1.6497611999511719\n",
      "E loss:  0.820876955986023\n",
      "G loss: 1.7210334539413452\n",
      "E loss:  0.8318036198616028\n",
      "G loss: 1.7014331817626953\n",
      "E loss:  0.8179376125335693\n",
      "G loss: 1.874836802482605\n",
      "E loss:  0.8224055767059326\n",
      "G loss: 1.8671562671661377\n",
      "Training Model  ...\n",
      "E loss:  0.7961195111274719\n",
      "G loss: 1.9225916862487793\n",
      "E loss:  0.7949727177619934\n",
      "G loss: 1.834938883781433\n",
      "E loss:  0.7912540435791016\n",
      "G loss: 1.7625854015350342\n",
      "E loss:  0.7690185308456421\n",
      "G loss: 1.7697296142578125\n",
      "E loss:  0.7738481760025024\n",
      "G loss: 1.8265132904052734\n",
      "Training Model  ...\n",
      "E loss:  0.8678063154220581\n",
      "G loss: 1.6719188690185547\n",
      "E loss:  0.8826320767402649\n",
      "G loss: 1.7178469896316528\n",
      "E loss:  0.8745893239974976\n",
      "G loss: 1.824406623840332\n",
      "E loss:  0.9015578031539917\n",
      "G loss: 1.6710121631622314\n",
      "E loss:  0.9030191898345947\n",
      "G loss: 1.7675743103027344\n",
      "Training Model  ...\n",
      "E loss:  0.7141541242599487\n",
      "G loss: 1.7304987907409668\n",
      "E loss:  0.707234799861908\n",
      "G loss: 1.7238603830337524\n",
      "E loss:  0.7146626710891724\n",
      "G loss: 1.8125841617584229\n",
      "E loss:  0.7235848307609558\n",
      "G loss: 1.8320415019989014\n",
      "E loss:  0.7333953976631165\n",
      "G loss: 1.843437910079956\n",
      "Training Model  ...\n",
      "E loss:  0.9296362996101379\n",
      "G loss: 1.7877357006072998\n",
      "E loss:  0.9274593591690063\n",
      "G loss: 1.7973939180374146\n",
      "E loss:  0.9049972891807556\n",
      "G loss: 1.7522152662277222\n",
      "E loss:  0.9036943316459656\n",
      "G loss: 1.8230491876602173\n",
      "E loss:  0.9137892723083496\n",
      "G loss: 1.8668384552001953\n",
      "Training Model  ...\n",
      "E loss:  0.8145924210548401\n",
      "G loss: 1.8705365657806396\n",
      "E loss:  0.830220639705658\n",
      "G loss: 1.8533046245574951\n",
      "E loss:  0.8134193420410156\n",
      "G loss: 1.8058174848556519\n",
      "E loss:  0.8156858086585999\n",
      "G loss: 1.785339593887329\n",
      "E loss:  0.8162212371826172\n",
      "G loss: 1.759124994277954\n",
      "Training Model  ...\n",
      "E loss:  0.8298606872558594\n",
      "G loss: 1.7214515209197998\n",
      "E loss:  0.8414505124092102\n",
      "G loss: 1.727973461151123\n",
      "E loss:  0.8360536098480225\n",
      "G loss: 1.822869062423706\n",
      "E loss:  0.8278844356536865\n",
      "G loss: 1.8392301797866821\n",
      "E loss:  0.827677845954895\n",
      "G loss: 1.839759349822998\n",
      "Training Model  ...\n",
      "E loss:  0.81296306848526\n",
      "G loss: 1.7520334720611572\n",
      "E loss:  0.8028795719146729\n",
      "G loss: 1.6355035305023193\n",
      "E loss:  0.8157823085784912\n",
      "G loss: 1.6651313304901123\n",
      "E loss:  0.8255401849746704\n",
      "G loss: 1.541549563407898\n",
      "E loss:  0.819139838218689\n",
      "G loss: 1.5489635467529297\n",
      "Training Model  ...\n",
      "E loss:  0.7957967519760132\n",
      "G loss: 1.5723683834075928\n",
      "E loss:  0.7876893877983093\n",
      "G loss: 1.591482162475586\n",
      "E loss:  0.7846705317497253\n",
      "G loss: 1.5665353536605835\n",
      "E loss:  0.7597159743309021\n",
      "G loss: 1.6603633165359497\n",
      "E loss:  0.7669791579246521\n",
      "G loss: 1.6843812465667725\n",
      "Training Model  ...\n",
      "E loss:  0.8204374313354492\n",
      "G loss: 1.6112470626831055\n",
      "E loss:  0.7937913537025452\n",
      "G loss: 1.709983468055725\n",
      "E loss:  0.8111098408699036\n",
      "G loss: 1.6279020309448242\n",
      "E loss:  0.8104965686798096\n",
      "G loss: 1.6011271476745605\n",
      "E loss:  0.8027204871177673\n",
      "G loss: 1.6711634397506714\n",
      "Training Model  ...\n",
      "E loss:  0.8723782300949097\n",
      "G loss: 1.7091399431228638\n",
      "E loss:  0.8748151659965515\n",
      "G loss: 1.7602328062057495\n",
      "E loss:  0.861809492111206\n",
      "G loss: 1.7677465677261353\n",
      "E loss:  0.8638713359832764\n",
      "G loss: 1.8273251056671143\n",
      "E loss:  0.866745114326477\n",
      "G loss: 1.7744088172912598\n",
      "Training Model  ...\n",
      "E loss:  0.7389916181564331\n",
      "G loss: 1.8320252895355225\n",
      "E loss:  0.7281519174575806\n",
      "G loss: 1.7506977319717407\n",
      "E loss:  0.729904055595398\n",
      "G loss: 1.768484354019165\n",
      "E loss:  0.7130892276763916\n",
      "G loss: 1.7487913370132446\n",
      "E loss:  0.7145549654960632\n",
      "G loss: 1.7210478782653809\n",
      "Training Model  ...\n",
      "E loss:  0.8417935967445374\n",
      "G loss: 1.6998369693756104\n",
      "E loss:  0.8449774384498596\n",
      "G loss: 1.6332201957702637\n",
      "E loss:  0.8476277589797974\n",
      "G loss: 1.597513198852539\n",
      "E loss:  0.8274745941162109\n",
      "G loss: 1.610572099685669\n",
      "E loss:  0.8069567680358887\n",
      "G loss: 1.485517144203186\n",
      "Training Model  ...\n",
      "E loss:  0.8766402006149292\n",
      "G loss: 1.569158673286438\n",
      "E loss:  0.8658861517906189\n",
      "G loss: 1.5690714120864868\n",
      "E loss:  0.8586258888244629\n",
      "G loss: 1.6729590892791748\n",
      "E loss:  0.8531484007835388\n",
      "G loss: 1.758887529373169\n",
      "E loss:  0.8469113707542419\n",
      "G loss: 1.840131163597107\n",
      "Training Model  ...\n",
      "E loss:  0.863438606262207\n",
      "G loss: 1.838765263557434\n",
      "E loss:  0.8615543246269226\n",
      "G loss: 1.784949541091919\n",
      "E loss:  0.8562800884246826\n",
      "G loss: 1.742246389389038\n",
      "E loss:  0.8475586175918579\n",
      "G loss: 1.699894666671753\n",
      "E loss:  0.8325105905532837\n",
      "G loss: 1.75727379322052\n",
      "Training Model  ...\n",
      "E loss:  0.937427818775177\n",
      "G loss: 1.7118695974349976\n",
      "E loss:  0.9391593933105469\n",
      "G loss: 1.7469927072525024\n",
      "E loss:  0.9211851358413696\n",
      "G loss: 1.779937505722046\n",
      "E loss:  0.9294285774230957\n",
      "G loss: 1.7580602169036865\n",
      "E loss:  0.9263778924942017\n",
      "G loss: 1.8115746974945068\n",
      "Training Model  ...\n",
      "E loss:  0.8394410014152527\n",
      "G loss: 1.8116004467010498\n",
      "E loss:  0.8291056156158447\n",
      "G loss: 1.7484462261199951\n",
      "E loss:  0.8282284736633301\n",
      "G loss: 1.699285864830017\n",
      "E loss:  0.8420957326889038\n",
      "G loss: 1.635579228401184\n",
      "E loss:  0.838011622428894\n",
      "G loss: 1.6096370220184326\n",
      "Training Model  ...\n",
      "E loss:  0.8042615056037903\n",
      "G loss: 1.5982649326324463\n",
      "E loss:  0.7929918169975281\n",
      "G loss: 1.701141595840454\n",
      "E loss:  0.78842693567276\n",
      "G loss: 1.7441409826278687\n",
      "E loss:  0.7845933437347412\n",
      "G loss: 1.7307981252670288\n",
      "E loss:  0.7876732349395752\n",
      "G loss: 1.7871248722076416\n",
      "Training Model  ...\n",
      "E loss:  0.8694307208061218\n",
      "G loss: 1.8258552551269531\n",
      "E loss:  0.9025113582611084\n",
      "G loss: 1.7428051233291626\n",
      "E loss:  0.9009090662002563\n",
      "G loss: 1.808618426322937\n",
      "E loss:  0.9075596332550049\n",
      "G loss: 1.696049451828003\n",
      "E loss:  0.8972716927528381\n",
      "G loss: 1.629019021987915\n",
      "Training Model  ...\n",
      "E loss:  0.9137356281280518\n",
      "G loss: 1.637235164642334\n",
      "E loss:  0.9166678190231323\n",
      "G loss: 1.6451959609985352\n",
      "E loss:  0.922804057598114\n",
      "G loss: 1.6444025039672852\n",
      "E loss:  0.9167588949203491\n",
      "G loss: 1.5732601881027222\n",
      "E loss:  0.9019023180007935\n",
      "G loss: 1.5876845121383667\n",
      "Training Model  ...\n",
      "E loss:  0.7949211597442627\n",
      "G loss: 1.5752931833267212\n",
      "E loss:  0.7852307558059692\n",
      "G loss: 1.6423344612121582\n",
      "E loss:  0.7850281596183777\n",
      "G loss: 1.5818324089050293\n",
      "E loss:  0.7935608625411987\n",
      "G loss: 1.5481834411621094\n",
      "E loss:  0.7957319617271423\n",
      "G loss: 1.609261393547058\n",
      "Training Model  ...\n",
      "E loss:  0.872498631477356\n",
      "G loss: 1.6821393966674805\n",
      "E loss:  0.8718074560165405\n",
      "G loss: 1.6450742483139038\n",
      "E loss:  0.8681252598762512\n",
      "G loss: 1.6953428983688354\n",
      "E loss:  0.8784384727478027\n",
      "G loss: 1.726444959640503\n",
      "E loss:  0.8490157127380371\n",
      "G loss: 1.7618135213851929\n",
      "Training Model  ...\n",
      "E loss:  0.8682457804679871\n",
      "G loss: 1.7528808116912842\n",
      "E loss:  0.8696147799491882\n",
      "G loss: 1.7273151874542236\n",
      "E loss:  0.8627392053604126\n",
      "G loss: 1.6589540243148804\n",
      "E loss:  0.8747135996818542\n",
      "G loss: 1.6890075206756592\n",
      "E loss:  0.8670408129692078\n",
      "G loss: 1.65017569065094\n",
      "Training Model  ...\n",
      "E loss:  0.8366794586181641\n",
      "G loss: 1.6359469890594482\n",
      "E loss:  0.8355970978736877\n",
      "G loss: 1.631925106048584\n",
      "E loss:  0.8204221725463867\n",
      "G loss: 1.624637246131897\n",
      "E loss:  0.8244599103927612\n",
      "G loss: 1.67869234085083\n",
      "E loss:  0.8170511722564697\n",
      "G loss: 1.7097455263137817\n",
      "Training Model  ...\n",
      "E loss:  0.8376591205596924\n",
      "G loss: 1.6846948862075806\n",
      "E loss:  0.8209202289581299\n",
      "G loss: 1.7158600091934204\n",
      "E loss:  0.8296076655387878\n",
      "G loss: 1.6997039318084717\n",
      "E loss:  0.8220453262329102\n",
      "G loss: 1.6839509010314941\n",
      "E loss:  0.8249584436416626\n",
      "G loss: 1.7030144929885864\n",
      "Training Model  ...\n",
      "E loss:  0.8826031684875488\n",
      "G loss: 1.8300724029541016\n",
      "E loss:  0.8908023238182068\n",
      "G loss: 1.6335220336914062\n",
      "E loss:  0.8871182203292847\n",
      "G loss: 1.6256862878799438\n",
      "E loss:  0.9115710258483887\n",
      "G loss: 1.5707199573516846\n",
      "E loss:  0.9045116901397705\n",
      "G loss: 1.5410189628601074\n",
      "Training Model  ...\n",
      "E loss:  0.7994253635406494\n",
      "G loss: 1.5336577892303467\n",
      "E loss:  0.7949093580245972\n",
      "G loss: 1.5458531379699707\n",
      "E loss:  0.7879875302314758\n",
      "G loss: 1.563247561454773\n",
      "E loss:  0.7913317084312439\n",
      "G loss: 1.6353833675384521\n",
      "E loss:  0.7969938516616821\n",
      "G loss: 1.624472737312317\n",
      "Training Model  ...\n",
      "E loss:  0.7667258381843567\n",
      "G loss: 1.6575186252593994\n",
      "E loss:  0.7560158967971802\n",
      "G loss: 1.6246323585510254\n",
      "E loss:  0.7502800822257996\n",
      "G loss: 1.610759973526001\n",
      "E loss:  0.7538535594940186\n",
      "G loss: 1.6129721403121948\n",
      "E loss:  0.7547308802604675\n",
      "G loss: 1.4894993305206299\n",
      "Training Model  ...\n",
      "E loss:  0.8555999398231506\n",
      "G loss: 1.5905184745788574\n",
      "E loss:  0.8586475253105164\n",
      "G loss: 1.6473708152770996\n",
      "E loss:  0.8577852845191956\n",
      "G loss: 1.5850275754928589\n",
      "E loss:  0.8489224910736084\n",
      "G loss: 1.5601783990859985\n",
      "E loss:  0.8582361340522766\n",
      "G loss: 1.5537230968475342\n",
      "Training Model  ...\n",
      "E loss:  0.8071008324623108\n",
      "G loss: 1.6031370162963867\n",
      "E loss:  0.7998954653739929\n",
      "G loss: 1.5765035152435303\n",
      "E loss:  0.8072084784507751\n",
      "G loss: 1.578763723373413\n",
      "E loss:  0.8074163794517517\n",
      "G loss: 1.61307954788208\n",
      "E loss:  0.8271268606185913\n",
      "G loss: 1.5737863779067993\n",
      "Training Model  ...\n",
      "E loss:  0.7900357842445374\n",
      "G loss: 1.664820671081543\n",
      "E loss:  0.7862374186515808\n",
      "G loss: 1.7284066677093506\n",
      "E loss:  0.7787196636199951\n",
      "G loss: 1.7126126289367676\n",
      "E loss:  0.7788273096084595\n",
      "G loss: 1.722007393836975\n",
      "E loss:  0.7799725532531738\n",
      "G loss: 1.7797414064407349\n",
      "Training Model  ...\n",
      "E loss:  0.8870142698287964\n",
      "G loss: 1.8004792928695679\n",
      "E loss:  0.8653350472450256\n",
      "G loss: 1.7860504388809204\n",
      "E loss:  0.8859989643096924\n",
      "G loss: 1.7984157800674438\n",
      "E loss:  0.879546582698822\n",
      "G loss: 1.7769856452941895\n",
      "E loss:  0.8757580518722534\n",
      "G loss: 1.843640685081482\n",
      "Training Model  ...\n",
      "E loss:  0.8093709945678711\n",
      "G loss: 1.742294430732727\n",
      "E loss:  0.8087742328643799\n",
      "G loss: 1.7953500747680664\n",
      "E loss:  0.7891396284103394\n",
      "G loss: 1.6827484369277954\n",
      "E loss:  0.798540472984314\n",
      "G loss: 1.6305058002471924\n",
      "E loss:  0.7820713520050049\n",
      "G loss: 1.68300199508667\n",
      "Training Model  ...\n",
      "E loss:  0.8334208130836487\n",
      "G loss: 1.6300150156021118\n",
      "E loss:  0.8260899782180786\n",
      "G loss: 1.6570576429367065\n",
      "E loss:  0.8211760520935059\n",
      "G loss: 1.5681617259979248\n",
      "E loss:  0.832363486289978\n",
      "G loss: 1.526487112045288\n",
      "E loss:  0.8168671727180481\n",
      "G loss: 1.571597933769226\n",
      "Training Model  ...\n",
      "E loss:  0.7958294749259949\n",
      "G loss: 1.5583148002624512\n",
      "E loss:  0.7845423221588135\n",
      "G loss: 1.5081216096878052\n",
      "E loss:  0.7843049764633179\n",
      "G loss: 1.5600335597991943\n",
      "E loss:  0.7870713472366333\n",
      "G loss: 1.560177206993103\n",
      "E loss:  0.7826194763183594\n",
      "G loss: 1.6326159238815308\n",
      "Training Model  ...\n",
      "E loss:  0.8233416080474854\n",
      "G loss: 1.593267560005188\n",
      "E loss:  0.8422729969024658\n",
      "G loss: 1.5979396104812622\n",
      "E loss:  0.8421375155448914\n",
      "G loss: 1.6391631364822388\n",
      "E loss:  0.8524755835533142\n",
      "G loss: 1.6428096294403076\n",
      "E loss:  0.8507763743400574\n",
      "G loss: 1.600054383277893\n",
      "Training Model  ...\n",
      "E loss:  0.942023754119873\n",
      "G loss: 1.6344914436340332\n",
      "E loss:  0.9447925090789795\n",
      "G loss: 1.7059730291366577\n",
      "E loss:  0.9375216960906982\n",
      "G loss: 1.6394909620285034\n",
      "E loss:  0.9576817750930786\n",
      "G loss: 1.5918364524841309\n",
      "E loss:  0.9631189107894897\n",
      "G loss: 1.5813764333724976\n",
      "Training Model  ...\n",
      "E loss:  0.8694733381271362\n",
      "G loss: 1.6164745092391968\n",
      "E loss:  0.8458740711212158\n",
      "G loss: 1.6065161228179932\n",
      "E loss:  0.8392987251281738\n",
      "G loss: 1.681559681892395\n",
      "E loss:  0.8149791359901428\n",
      "G loss: 1.7969515323638916\n",
      "E loss:  0.8193996548652649\n",
      "G loss: 1.810367226600647\n",
      "Training Model  ...\n",
      "E loss:  0.7701699733734131\n",
      "G loss: 1.7585861682891846\n",
      "E loss:  0.772942304611206\n",
      "G loss: 1.7155115604400635\n",
      "E loss:  0.7735174298286438\n",
      "G loss: 1.7996485233306885\n",
      "E loss:  0.7893786430358887\n",
      "G loss: 1.7199870347976685\n",
      "E loss:  0.7910305857658386\n",
      "G loss: 1.7088576555252075\n",
      "Training Model  ...\n",
      "E loss:  0.8300871253013611\n",
      "G loss: 1.7035647630691528\n",
      "E loss:  0.8123489618301392\n",
      "G loss: 1.6502976417541504\n",
      "E loss:  0.8117934465408325\n",
      "G loss: 1.6080735921859741\n",
      "E loss:  0.8054731488227844\n",
      "G loss: 1.560803771018982\n",
      "E loss:  0.8138909935951233\n",
      "G loss: 1.4976146221160889\n",
      "Training Model  ...\n",
      "E loss:  0.8601070642471313\n",
      "G loss: 1.5158997774124146\n",
      "E loss:  0.8472737669944763\n",
      "G loss: 1.6042898893356323\n",
      "E loss:  0.8336774110794067\n",
      "G loss: 1.6032569408416748\n",
      "E loss:  0.8494285345077515\n",
      "G loss: 1.7223186492919922\n",
      "E loss:  0.8604812026023865\n",
      "G loss: 1.720900058746338\n",
      "Training Model  ...\n",
      "E loss:  0.7871426343917847\n",
      "G loss: 1.714393138885498\n",
      "E loss:  0.7890918850898743\n",
      "G loss: 1.67080819606781\n",
      "E loss:  0.790905237197876\n",
      "G loss: 1.5808719396591187\n",
      "E loss:  0.7757527828216553\n",
      "G loss: 1.5567681789398193\n",
      "E loss:  0.7936294078826904\n",
      "G loss: 1.4667423963546753\n",
      "Training Model  ...\n",
      "E loss:  0.797457754611969\n",
      "G loss: 1.4360339641571045\n",
      "E loss:  0.7949907183647156\n",
      "G loss: 1.5026434659957886\n",
      "E loss:  0.7967345714569092\n",
      "G loss: 1.5032402276992798\n",
      "E loss:  0.7909194827079773\n",
      "G loss: 1.5085169076919556\n",
      "E loss:  0.7881954312324524\n",
      "G loss: 1.4587117433547974\n",
      "Training Model  ...\n",
      "E loss:  0.8032996654510498\n",
      "G loss: 1.4571330547332764\n",
      "E loss:  0.8117521405220032\n",
      "G loss: 1.52969229221344\n",
      "E loss:  0.8227783441543579\n",
      "G loss: 1.5544533729553223\n",
      "E loss:  0.8295060992240906\n",
      "G loss: 1.503835678100586\n",
      "E loss:  0.8218489289283752\n",
      "G loss: 1.5283286571502686\n",
      "Training Model  ...\n",
      "E loss:  0.858320415019989\n",
      "G loss: 1.4986268281936646\n",
      "E loss:  0.8552016019821167\n",
      "G loss: 1.5375343561172485\n",
      "E loss:  0.858493983745575\n",
      "G loss: 1.564176321029663\n",
      "E loss:  0.8689733147621155\n",
      "G loss: 1.6787374019622803\n",
      "E loss:  0.8554421067237854\n",
      "G loss: 1.620391607284546\n",
      "Training Model  ...\n",
      "E loss:  0.7904712557792664\n",
      "G loss: 1.6269359588623047\n",
      "E loss:  0.7914433479309082\n",
      "G loss: 1.6528708934783936\n",
      "E loss:  0.7859599590301514\n",
      "G loss: 1.6183826923370361\n",
      "E loss:  0.7775536775588989\n",
      "G loss: 1.504288911819458\n",
      "E loss:  0.7900722026824951\n",
      "G loss: 1.5360089540481567\n",
      "Training Model  ...\n",
      "E loss:  0.8793343305587769\n",
      "G loss: 1.5649206638336182\n",
      "E loss:  0.8687355518341064\n",
      "G loss: 1.4625301361083984\n",
      "E loss:  0.863722026348114\n",
      "G loss: 1.491105318069458\n",
      "E loss:  0.8502357602119446\n",
      "G loss: 1.5398684740066528\n",
      "E loss:  0.8644471168518066\n",
      "G loss: 1.4890484809875488\n",
      "Training Model  ...\n",
      "E loss:  0.868097186088562\n",
      "G loss: 1.5014818906784058\n",
      "E loss:  0.8888490200042725\n",
      "G loss: 1.5345076322555542\n",
      "E loss:  0.8818693161010742\n",
      "G loss: 1.5419681072235107\n",
      "E loss:  0.8850456476211548\n",
      "G loss: 1.4972474575042725\n",
      "E loss:  0.8927223682403564\n",
      "G loss: 1.555004596710205\n",
      "Training Model  ...\n",
      "E loss:  0.828892707824707\n",
      "G loss: 1.5321158170700073\n",
      "E loss:  0.8371558785438538\n",
      "G loss: 1.508061408996582\n",
      "E loss:  0.8231570720672607\n",
      "G loss: 1.5727858543395996\n",
      "E loss:  0.8170632123947144\n",
      "G loss: 1.6114485263824463\n",
      "E loss:  0.8239386677742004\n",
      "G loss: 1.6133558750152588\n",
      "Training Model  ...\n",
      "E loss:  0.8684626817703247\n",
      "G loss: 1.572810411453247\n",
      "E loss:  0.8684912323951721\n",
      "G loss: 1.5910897254943848\n",
      "E loss:  0.8861261606216431\n",
      "G loss: 1.5865840911865234\n",
      "E loss:  0.8842073082923889\n",
      "G loss: 1.504518985748291\n",
      "E loss:  0.8683362603187561\n",
      "G loss: 1.497720718383789\n",
      "Training Model  ...\n",
      "E loss:  0.7117462158203125\n",
      "G loss: 1.4521584510803223\n",
      "E loss:  0.7197903990745544\n",
      "G loss: 1.4873442649841309\n",
      "E loss:  0.7171743512153625\n",
      "G loss: 1.4765700101852417\n",
      "E loss:  0.7056183815002441\n",
      "G loss: 1.5236561298370361\n",
      "E loss:  0.7176892161369324\n",
      "G loss: 1.510154128074646\n",
      "Training Model  ...\n",
      "E loss:  0.8139252662658691\n",
      "G loss: 1.5247257947921753\n",
      "E loss:  0.807429850101471\n",
      "G loss: 1.5600979328155518\n",
      "E loss:  0.8100375533103943\n",
      "G loss: 1.4864388704299927\n",
      "E loss:  0.8102943301200867\n",
      "G loss: 1.494220495223999\n",
      "E loss:  0.8038344383239746\n",
      "G loss: 1.414989709854126\n",
      "Training Model  ...\n",
      "E loss:  0.7991759181022644\n",
      "G loss: 1.5058151483535767\n",
      "E loss:  0.7929558753967285\n",
      "G loss: 1.4647407531738281\n",
      "E loss:  0.8016276359558105\n",
      "G loss: 1.5260883569717407\n",
      "E loss:  0.7913861274719238\n",
      "G loss: 1.5024067163467407\n",
      "E loss:  0.7898330688476562\n",
      "G loss: 1.5870920419692993\n",
      "Training Model  ...\n",
      "E loss:  0.8319308161735535\n",
      "G loss: 1.6042628288269043\n",
      "E loss:  0.803076982498169\n",
      "G loss: 1.5397932529449463\n",
      "E loss:  0.8067072629928589\n",
      "G loss: 1.5080204010009766\n",
      "E loss:  0.8108639121055603\n",
      "G loss: 1.4622831344604492\n",
      "E loss:  0.8279227614402771\n",
      "G loss: 1.4365370273590088\n",
      "Training Model  ...\n",
      "E loss:  0.8248443603515625\n",
      "G loss: 1.3879262208938599\n",
      "E loss:  0.8231180310249329\n",
      "G loss: 1.468963861465454\n",
      "E loss:  0.8386151194572449\n",
      "G loss: 1.4129507541656494\n",
      "E loss:  0.8386329412460327\n",
      "G loss: 1.5714064836502075\n",
      "E loss:  0.8177918195724487\n",
      "G loss: 1.5360171794891357\n",
      "Training Model  ...\n",
      "E loss:  0.8526637554168701\n",
      "G loss: 1.5227899551391602\n",
      "E loss:  0.8593494296073914\n",
      "G loss: 1.52846360206604\n",
      "E loss:  0.8745208382606506\n",
      "G loss: 1.5108966827392578\n",
      "E loss:  0.8501927852630615\n",
      "G loss: 1.5306607484817505\n",
      "E loss:  0.8436540365219116\n",
      "G loss: 1.485294222831726\n",
      "Training Model  ...\n",
      "E loss:  0.7936499118804932\n",
      "G loss: 1.5113600492477417\n",
      "E loss:  0.7879879474639893\n",
      "G loss: 1.4398757219314575\n",
      "E loss:  0.7818113565444946\n",
      "G loss: 1.4382590055465698\n",
      "E loss:  0.7675704956054688\n",
      "G loss: 1.4351171255111694\n",
      "E loss:  0.7804745435714722\n",
      "G loss: 1.3683499097824097\n",
      "Training Model  ...\n",
      "E loss:  0.8003782033920288\n",
      "G loss: 1.4126580953598022\n",
      "E loss:  0.7960404753684998\n",
      "G loss: 1.4128211736679077\n",
      "E loss:  0.8123085498809814\n",
      "G loss: 1.4941656589508057\n",
      "E loss:  0.8036720752716064\n",
      "G loss: 1.4521949291229248\n",
      "E loss:  0.8138813376426697\n",
      "G loss: 1.477736473083496\n",
      "Training Model  ...\n",
      "E loss:  0.7718663811683655\n",
      "G loss: 1.4426652193069458\n",
      "E loss:  0.7859941720962524\n",
      "G loss: 1.4983292818069458\n",
      "E loss:  0.7761640548706055\n",
      "G loss: 1.5434409379959106\n",
      "E loss:  0.7858341932296753\n",
      "G loss: 1.4726841449737549\n",
      "E loss:  0.799380362033844\n",
      "G loss: 1.5095880031585693\n",
      "Training Model  ...\n",
      "E loss:  0.7946224212646484\n",
      "G loss: 1.4483438730239868\n",
      "E loss:  0.7851747870445251\n",
      "G loss: 1.5349175930023193\n",
      "E loss:  0.7738954424858093\n",
      "G loss: 1.7041518688201904\n",
      "E loss:  0.7891228795051575\n",
      "G loss: 1.7762060165405273\n",
      "E loss:  0.7914110422134399\n",
      "G loss: 1.8171024322509766\n",
      "Training Model  ...\n",
      "E loss:  0.881010115146637\n",
      "G loss: 1.710856556892395\n",
      "E loss:  0.8656575679779053\n",
      "G loss: 1.6479452848434448\n",
      "E loss:  0.8259229063987732\n",
      "G loss: 1.5076570510864258\n",
      "E loss:  0.7984817028045654\n",
      "G loss: 1.3738240003585815\n",
      "E loss:  0.7800744771957397\n",
      "G loss: 1.202234148979187\n",
      "Training Model  ...\n",
      "E loss:  0.8175851702690125\n",
      "G loss: 1.2609944343566895\n",
      "E loss:  0.8371315002441406\n",
      "G loss: 1.274168610572815\n",
      "E loss:  0.8295239806175232\n",
      "G loss: 1.3544809818267822\n",
      "E loss:  0.8181222677230835\n",
      "G loss: 1.4483767747879028\n",
      "E loss:  0.8178601264953613\n",
      "G loss: 1.5021708011627197\n",
      "Training Model  ...\n",
      "E loss:  0.7846523523330688\n",
      "G loss: 1.4638675451278687\n",
      "E loss:  0.7734721899032593\n",
      "G loss: 1.4239826202392578\n",
      "E loss:  0.7725276350975037\n",
      "G loss: 1.5232340097427368\n",
      "E loss:  0.782880425453186\n",
      "G loss: 1.4137729406356812\n",
      "E loss:  0.7927426099777222\n",
      "G loss: 1.4129555225372314\n",
      "Training Model  ...\n",
      "E loss:  0.8205000758171082\n",
      "G loss: 1.455916404724121\n",
      "E loss:  0.8031301498413086\n",
      "G loss: 1.3891518115997314\n",
      "E loss:  0.7932051420211792\n",
      "G loss: 1.5100339651107788\n",
      "E loss:  0.8020888566970825\n",
      "G loss: 1.494844913482666\n",
      "E loss:  0.801769495010376\n",
      "G loss: 1.4576810598373413\n",
      "Training Model  ...\n",
      "E loss:  0.7402912974357605\n",
      "G loss: 1.5155540704727173\n",
      "E loss:  0.7383838295936584\n",
      "G loss: 1.4353270530700684\n",
      "E loss:  0.72686368227005\n",
      "G loss: 1.3977206945419312\n",
      "E loss:  0.7475388050079346\n",
      "G loss: 1.3212631940841675\n",
      "E loss:  0.746847927570343\n",
      "G loss: 1.3020585775375366\n",
      "Training Model  ...\n",
      "E loss:  0.8121349215507507\n",
      "G loss: 1.3340251445770264\n",
      "E loss:  0.8187752366065979\n",
      "G loss: 1.3148938417434692\n",
      "E loss:  0.8244214057922363\n",
      "G loss: 1.4600106477737427\n",
      "E loss:  0.8207162618637085\n",
      "G loss: 1.446480631828308\n",
      "E loss:  0.821200966835022\n",
      "G loss: 1.383474349975586\n",
      "Training Model  ...\n",
      "E loss:  0.8196158409118652\n",
      "G loss: 1.4492275714874268\n",
      "E loss:  0.7939105033874512\n",
      "G loss: 1.5644681453704834\n",
      "E loss:  0.8039793372154236\n",
      "G loss: 1.4992072582244873\n",
      "E loss:  0.8091492056846619\n",
      "G loss: 1.5087660551071167\n",
      "E loss:  0.8030420541763306\n",
      "G loss: 1.5369977951049805\n",
      "Training Model  ...\n",
      "E loss:  0.7975082397460938\n",
      "G loss: 1.5353572368621826\n",
      "E loss:  0.7908563017845154\n",
      "G loss: 1.5070844888687134\n",
      "E loss:  0.7808230519294739\n",
      "G loss: 1.5101912021636963\n",
      "E loss:  0.7911328673362732\n",
      "G loss: 1.5089291334152222\n",
      "E loss:  0.7886846661567688\n",
      "G loss: 1.455426573753357\n",
      "Training Model  ...\n",
      "E loss:  0.8665574193000793\n",
      "G loss: 1.5047661066055298\n",
      "E loss:  0.8774175643920898\n",
      "G loss: 1.3737658262252808\n",
      "E loss:  0.8712061643600464\n",
      "G loss: 1.3881810903549194\n",
      "E loss:  0.8757529854774475\n",
      "G loss: 1.2569962739944458\n",
      "E loss:  0.8502463102340698\n",
      "G loss: 1.1911072731018066\n",
      "Training Model  ...\n",
      "E loss:  0.8329593539237976\n",
      "G loss: 1.2339712381362915\n",
      "E loss:  0.827987551689148\n",
      "G loss: 1.2357937097549438\n",
      "E loss:  0.8348632454872131\n",
      "G loss: 1.2233068943023682\n",
      "E loss:  0.8413140177726746\n",
      "G loss: 1.3009376525878906\n",
      "E loss:  0.8294252157211304\n",
      "G loss: 1.4233452081680298\n",
      "Training Model  ...\n",
      "E loss:  0.764136791229248\n",
      "G loss: 1.4085054397583008\n",
      "E loss:  0.771319568157196\n",
      "G loss: 1.3793323040008545\n",
      "E loss:  0.7547114491462708\n",
      "G loss: 1.3382593393325806\n",
      "E loss:  0.7645372748374939\n",
      "G loss: 1.326387882232666\n",
      "E loss:  0.7552676200866699\n",
      "G loss: 1.4124025106430054\n",
      "Training Model  ...\n",
      "E loss:  0.7943316698074341\n",
      "G loss: 1.3647524118423462\n",
      "E loss:  0.7954386472702026\n",
      "G loss: 1.3777145147323608\n",
      "E loss:  0.8045741319656372\n",
      "G loss: 1.4122849702835083\n",
      "E loss:  0.7907087802886963\n",
      "G loss: 1.3522586822509766\n",
      "E loss:  0.8111838102340698\n",
      "G loss: 1.4357281923294067\n",
      "Training Model  ...\n",
      "E loss:  0.7072167992591858\n",
      "G loss: 1.4082331657409668\n",
      "E loss:  0.6917132139205933\n",
      "G loss: 1.3839528560638428\n",
      "E loss:  0.7043800354003906\n",
      "G loss: 1.4157609939575195\n",
      "E loss:  0.6977134943008423\n",
      "G loss: 1.4448796510696411\n",
      "E loss:  0.6964371204376221\n",
      "G loss: 1.413466453552246\n",
      "Training Model  ...\n",
      "E loss:  0.7638711929321289\n",
      "G loss: 1.4593002796173096\n",
      "E loss:  0.7822538018226624\n",
      "G loss: 1.3483818769454956\n",
      "E loss:  0.7960188388824463\n",
      "G loss: 1.3226494789123535\n",
      "E loss:  0.803631603717804\n",
      "G loss: 1.3154942989349365\n",
      "E loss:  0.8102684020996094\n",
      "G loss: 1.2914249897003174\n",
      "Training Model  ...\n",
      "E loss:  0.8320854902267456\n",
      "G loss: 1.3105041980743408\n",
      "E loss:  0.8142763376235962\n",
      "G loss: 1.3264038562774658\n",
      "E loss:  0.8095449805259705\n",
      "G loss: 1.4079868793487549\n",
      "E loss:  0.7778341770172119\n",
      "G loss: 1.3414087295532227\n",
      "E loss:  0.7834088802337646\n",
      "G loss: 1.454123616218567\n",
      "Training Model  ...\n",
      "E loss:  0.8261795043945312\n",
      "G loss: 1.4579460620880127\n",
      "E loss:  0.8370461463928223\n",
      "G loss: 1.4266700744628906\n",
      "E loss:  0.8427828550338745\n",
      "G loss: 1.474379301071167\n",
      "E loss:  0.8377697467803955\n",
      "G loss: 1.4247636795043945\n",
      "E loss:  0.8319146633148193\n",
      "G loss: 1.402479887008667\n",
      "Training Model  ...\n",
      "E loss:  0.7316602468490601\n",
      "G loss: 1.3425564765930176\n",
      "E loss:  0.7136955857276917\n",
      "G loss: 1.4458365440368652\n",
      "E loss:  0.7147425413131714\n",
      "G loss: 1.4563243389129639\n",
      "E loss:  0.7094489932060242\n",
      "G loss: 1.4164341688156128\n",
      "E loss:  0.7092972993850708\n",
      "G loss: 1.4550176858901978\n",
      "Training Model  ...\n",
      "E loss:  0.6856602430343628\n",
      "G loss: 1.3833385705947876\n",
      "E loss:  0.6658825874328613\n",
      "G loss: 1.3356122970581055\n",
      "E loss:  0.6670966148376465\n",
      "G loss: 1.3036481142044067\n",
      "E loss:  0.6552614569664001\n",
      "G loss: 1.2682374715805054\n",
      "E loss:  0.6774783134460449\n",
      "G loss: 1.2163991928100586\n",
      "Training Model  ...\n",
      "E loss:  0.870542585849762\n",
      "G loss: 1.2504663467407227\n",
      "E loss:  0.8902576565742493\n",
      "G loss: 1.2376713752746582\n",
      "E loss:  0.8937907814979553\n",
      "G loss: 1.3569015264511108\n",
      "E loss:  0.876041829586029\n",
      "G loss: 1.3624259233474731\n",
      "E loss:  0.8705965280532837\n",
      "G loss: 1.4125615358352661\n",
      "Training Model  ...\n",
      "E loss:  0.7862720489501953\n",
      "G loss: 1.5182232856750488\n",
      "E loss:  0.8016514778137207\n",
      "G loss: 1.4460991621017456\n",
      "E loss:  0.8020105957984924\n",
      "G loss: 1.510209321975708\n",
      "E loss:  0.7954441905021667\n",
      "G loss: 1.5079623460769653\n",
      "E loss:  0.7913486957550049\n",
      "G loss: 1.5521738529205322\n",
      "Training Model  ...\n",
      "E loss:  0.6644355058670044\n",
      "G loss: 1.5246533155441284\n",
      "E loss:  0.6757878065109253\n",
      "G loss: 1.5023659467697144\n",
      "E loss:  0.6861585974693298\n",
      "G loss: 1.4828016757965088\n",
      "E loss:  0.6828784346580505\n",
      "G loss: 1.3737561702728271\n",
      "E loss:  0.6870567798614502\n",
      "G loss: 1.3642802238464355\n",
      "Training Model  ...\n",
      "E loss:  0.7917137742042542\n",
      "G loss: 1.3789957761764526\n",
      "E loss:  0.7699182033538818\n",
      "G loss: 1.3487513065338135\n",
      "E loss:  0.7874594926834106\n",
      "G loss: 1.3696706295013428\n",
      "E loss:  0.7794395089149475\n",
      "G loss: 1.3355945348739624\n",
      "E loss:  0.7609606385231018\n",
      "G loss: 1.4059594869613647\n",
      "Training Model  ...\n",
      "E loss:  0.7886337637901306\n",
      "G loss: 1.3612570762634277\n",
      "E loss:  0.7870301604270935\n",
      "G loss: 1.3497413396835327\n",
      "E loss:  0.7758263945579529\n",
      "G loss: 1.3516992330551147\n",
      "E loss:  0.7887348532676697\n",
      "G loss: 1.33083176612854\n",
      "E loss:  0.7860918045043945\n",
      "G loss: 1.3448971509933472\n",
      "Training Model  ...\n",
      "E loss:  0.7279249429702759\n",
      "G loss: 1.2965213060379028\n",
      "E loss:  0.7343357801437378\n",
      "G loss: 1.3594778776168823\n",
      "E loss:  0.7407000064849854\n",
      "G loss: 1.3853994607925415\n",
      "E loss:  0.731853187084198\n",
      "G loss: 1.391474723815918\n",
      "E loss:  0.7336056232452393\n",
      "G loss: 1.4089515209197998\n",
      "Training Model  ...\n",
      "E loss:  0.8545010089874268\n",
      "G loss: 1.4039115905761719\n",
      "E loss:  0.8478847146034241\n",
      "G loss: 1.3350285291671753\n",
      "E loss:  0.8573999404907227\n",
      "G loss: 1.304909586906433\n",
      "E loss:  0.8494247198104858\n",
      "G loss: 1.3053011894226074\n",
      "E loss:  0.8348146677017212\n",
      "G loss: 1.2797980308532715\n",
      "Training Model  ...\n",
      "E loss:  0.8376388549804688\n",
      "G loss: 1.2290593385696411\n",
      "E loss:  0.8499824404716492\n",
      "G loss: 1.276989221572876\n",
      "E loss:  0.8240823745727539\n",
      "G loss: 1.3079079389572144\n",
      "E loss:  0.8254969120025635\n",
      "G loss: 1.3113244771957397\n",
      "E loss:  0.8502731919288635\n",
      "G loss: 1.382718801498413\n",
      "Training Model  ...\n",
      "E loss:  0.7447155117988586\n",
      "G loss: 1.2896480560302734\n",
      "E loss:  0.7406108379364014\n",
      "G loss: 1.3511326313018799\n",
      "E loss:  0.7416746616363525\n",
      "G loss: 1.3556299209594727\n",
      "E loss:  0.7264443039894104\n",
      "G loss: 1.3584473133087158\n",
      "E loss:  0.7274088859558105\n",
      "G loss: 1.3104270696640015\n",
      "Training Model  ...\n",
      "E loss:  0.720308244228363\n",
      "G loss: 1.310880422592163\n",
      "E loss:  0.7292949557304382\n",
      "G loss: 1.3496677875518799\n",
      "E loss:  0.7348177433013916\n",
      "G loss: 1.4104969501495361\n",
      "E loss:  0.7435635328292847\n",
      "G loss: 1.3717560768127441\n",
      "E loss:  0.759490966796875\n",
      "G loss: 1.4721565246582031\n",
      "Training Model  ...\n",
      "E loss:  0.8023412823677063\n",
      "G loss: 1.4285329580307007\n",
      "E loss:  0.8064290881156921\n",
      "G loss: 1.4368913173675537\n",
      "E loss:  0.8028008341789246\n",
      "G loss: 1.4665617942810059\n",
      "E loss:  0.8098627328872681\n",
      "G loss: 1.4628831148147583\n",
      "E loss:  0.8032644987106323\n",
      "G loss: 1.437027096748352\n",
      "Training Model  ...\n",
      "E loss:  0.8357162475585938\n",
      "G loss: 1.4079610109329224\n",
      "E loss:  0.8161559104919434\n",
      "G loss: 1.3860664367675781\n",
      "E loss:  0.8006346225738525\n",
      "G loss: 1.332004427909851\n",
      "E loss:  0.7889394760131836\n",
      "G loss: 1.2686705589294434\n",
      "E loss:  0.7990898489952087\n",
      "G loss: 1.2739067077636719\n",
      "Training Model  ...\n",
      "E loss:  0.8207488059997559\n",
      "G loss: 1.2547295093536377\n",
      "E loss:  0.8207335472106934\n",
      "G loss: 1.2984522581100464\n",
      "E loss:  0.8111371397972107\n",
      "G loss: 1.3386640548706055\n",
      "E loss:  0.7774340510368347\n",
      "G loss: 1.4233978986740112\n",
      "E loss:  0.7734265327453613\n",
      "G loss: 1.3312344551086426\n",
      "Training Model  ...\n",
      "E loss:  0.7101647853851318\n",
      "G loss: 1.4318046569824219\n",
      "E loss:  0.7158368825912476\n",
      "G loss: 1.3884046077728271\n",
      "E loss:  0.7164687514305115\n",
      "G loss: 1.4338237047195435\n",
      "E loss:  0.7280049324035645\n",
      "G loss: 1.3707845211029053\n",
      "E loss:  0.7441079616546631\n",
      "G loss: 1.3572614192962646\n",
      "Training Model  ...\n",
      "E loss:  0.812362790107727\n",
      "G loss: 1.394040822982788\n",
      "E loss:  0.8099827766418457\n",
      "G loss: 1.4076331853866577\n",
      "E loss:  0.7927502393722534\n",
      "G loss: 1.3446799516677856\n",
      "E loss:  0.7911791801452637\n",
      "G loss: 1.3316377401351929\n",
      "E loss:  0.7989710569381714\n",
      "G loss: 1.3036115169525146\n",
      "Training Model  ...\n",
      "E loss:  0.7800829410552979\n",
      "G loss: 1.3165754079818726\n",
      "E loss:  0.7831411361694336\n",
      "G loss: 1.2651835680007935\n",
      "E loss:  0.7797102928161621\n",
      "G loss: 1.3218096494674683\n",
      "E loss:  0.7782527208328247\n",
      "G loss: 1.254363775253296\n",
      "E loss:  0.787742555141449\n",
      "G loss: 1.2903779745101929\n",
      "Training Model  ...\n",
      "E loss:  0.6972948312759399\n",
      "G loss: 1.2494759559631348\n",
      "E loss:  0.7002512812614441\n",
      "G loss: 1.3318397998809814\n",
      "E loss:  0.6809430122375488\n",
      "G loss: 1.3484952449798584\n",
      "E loss:  0.6838838458061218\n",
      "G loss: 1.367117166519165\n",
      "E loss:  0.671950101852417\n",
      "G loss: 1.3606140613555908\n",
      "Training Model  ...\n",
      "E loss:  0.7994270324707031\n",
      "G loss: 1.3230730295181274\n",
      "E loss:  0.7847086191177368\n",
      "G loss: 1.3714300394058228\n",
      "E loss:  0.790641725063324\n",
      "G loss: 1.3679510354995728\n",
      "E loss:  0.7808084487915039\n",
      "G loss: 1.3582205772399902\n",
      "E loss:  0.7787489891052246\n",
      "G loss: 1.31219482421875\n",
      "Training Model  ...\n",
      "E loss:  0.8644306659698486\n",
      "G loss: 1.31121027469635\n",
      "E loss:  0.8716275691986084\n",
      "G loss: 1.3152289390563965\n",
      "E loss:  0.872620701789856\n",
      "G loss: 1.3119826316833496\n",
      "E loss:  0.855211079120636\n",
      "G loss: 1.3810172080993652\n",
      "E loss:  0.8520533442497253\n",
      "G loss: 1.4461299180984497\n",
      "Training Model  ...\n",
      "E loss:  0.8829761743545532\n",
      "G loss: 1.3401212692260742\n",
      "E loss:  0.8741397261619568\n",
      "G loss: 1.3694484233856201\n",
      "E loss:  0.8784022331237793\n",
      "G loss: 1.4532015323638916\n",
      "E loss:  0.8661473989486694\n",
      "G loss: 1.4942384958267212\n",
      "E loss:  0.8763551115989685\n",
      "G loss: 1.456114411354065\n",
      "Training Model  ...\n",
      "E loss:  0.7877624034881592\n",
      "G loss: 1.4740720987319946\n",
      "E loss:  0.7816175818443298\n",
      "G loss: 1.4291956424713135\n",
      "E loss:  0.778404712677002\n",
      "G loss: 1.411273717880249\n",
      "E loss:  0.7924271821975708\n",
      "G loss: 1.290642261505127\n",
      "E loss:  0.7963343858718872\n",
      "G loss: 1.3150612115859985\n",
      "Training Model  ...\n",
      "E loss:  0.8661338090896606\n",
      "G loss: 1.2244641780853271\n",
      "E loss:  0.8574714660644531\n",
      "G loss: 1.299950361251831\n",
      "E loss:  0.8449828624725342\n",
      "G loss: 1.3348454236984253\n",
      "E loss:  0.8391187191009521\n",
      "G loss: 1.319716453552246\n",
      "E loss:  0.8122820258140564\n",
      "G loss: 1.3601512908935547\n",
      "Training Model  ...\n",
      "E loss:  0.7073706388473511\n",
      "G loss: 1.3371634483337402\n",
      "E loss:  0.7225045561790466\n",
      "G loss: 1.2926647663116455\n",
      "E loss:  0.7213099598884583\n",
      "G loss: 1.2740288972854614\n",
      "E loss:  0.7334019541740417\n",
      "G loss: 1.2589974403381348\n",
      "E loss:  0.7367167472839355\n",
      "G loss: 1.313486933708191\n",
      "Training Model  ...\n",
      "E loss:  0.8161517977714539\n",
      "G loss: 1.1641687154769897\n",
      "E loss:  0.8102620840072632\n",
      "G loss: 1.2193392515182495\n",
      "E loss:  0.8055551052093506\n",
      "G loss: 1.2621968984603882\n",
      "E loss:  0.8217495679855347\n",
      "G loss: 1.2710413932800293\n",
      "E loss:  0.8201331496238708\n",
      "G loss: 1.3801853656768799\n",
      "Training Model  ...\n",
      "E loss:  0.824635922908783\n",
      "G loss: 1.3124666213989258\n",
      "E loss:  0.8390798568725586\n",
      "G loss: 1.342801570892334\n",
      "E loss:  0.8408432006835938\n",
      "G loss: 1.314225673675537\n",
      "E loss:  0.8421115279197693\n",
      "G loss: 1.3654762506484985\n",
      "E loss:  0.8444257974624634\n",
      "G loss: 1.2907681465148926\n",
      "Training Model  ...\n",
      "E loss:  0.7229578495025635\n",
      "G loss: 1.3516297340393066\n",
      "E loss:  0.736027717590332\n",
      "G loss: 1.3034591674804688\n",
      "E loss:  0.7195199728012085\n",
      "G loss: 1.2834621667861938\n",
      "E loss:  0.7249466180801392\n",
      "G loss: 1.2783674001693726\n",
      "E loss:  0.7536423206329346\n",
      "G loss: 1.2561140060424805\n",
      "Training Model  ...\n",
      "E loss:  0.7817132472991943\n",
      "G loss: 1.3164794445037842\n",
      "E loss:  0.7735139727592468\n",
      "G loss: 1.232527256011963\n",
      "E loss:  0.7605304718017578\n",
      "G loss: 1.268994688987732\n",
      "E loss:  0.7502038478851318\n",
      "G loss: 1.230389952659607\n",
      "E loss:  0.7720816135406494\n",
      "G loss: 1.2591863870620728\n",
      "Training Model  ...\n",
      "E loss:  0.8403223156929016\n",
      "G loss: 1.2432888746261597\n",
      "E loss:  0.8462794423103333\n",
      "G loss: 1.2685703039169312\n",
      "E loss:  0.8235717415809631\n",
      "G loss: 1.3238916397094727\n",
      "E loss:  0.8140037059783936\n",
      "G loss: 1.2839465141296387\n",
      "E loss:  0.8123980164527893\n",
      "G loss: 1.3256146907806396\n",
      "Training Model  ...\n",
      "E loss:  0.765508234500885\n",
      "G loss: 1.3331810235977173\n",
      "E loss:  0.7563182711601257\n",
      "G loss: 1.3157365322113037\n",
      "E loss:  0.7546314001083374\n",
      "G loss: 1.337714433670044\n",
      "E loss:  0.7444329261779785\n",
      "G loss: 1.2860000133514404\n",
      "E loss:  0.7398924827575684\n",
      "G loss: 1.3218387365341187\n",
      "Training Model  ...\n",
      "E loss:  0.6976168751716614\n",
      "G loss: 1.2647889852523804\n",
      "E loss:  0.7052394151687622\n",
      "G loss: 1.2933194637298584\n",
      "E loss:  0.7169075608253479\n",
      "G loss: 1.3234740495681763\n",
      "E loss:  0.7132537364959717\n",
      "G loss: 1.3108201026916504\n",
      "E loss:  0.7221247553825378\n",
      "G loss: 1.286805510520935\n",
      "Training Model  ...\n",
      "E loss:  0.7753632664680481\n",
      "G loss: 1.229453444480896\n",
      "E loss:  0.7797201871871948\n",
      "G loss: 1.335092544555664\n",
      "E loss:  0.779664158821106\n",
      "G loss: 1.266345500946045\n",
      "E loss:  0.784760057926178\n",
      "G loss: 1.2491477727890015\n",
      "E loss:  0.7927018404006958\n",
      "G loss: 1.1473355293273926\n",
      "Training Model  ...\n",
      "E loss:  0.818418025970459\n",
      "G loss: 1.21864914894104\n",
      "E loss:  0.8032068014144897\n",
      "G loss: 1.2140088081359863\n",
      "E loss:  0.8087656497955322\n",
      "G loss: 1.17426598072052\n",
      "E loss:  0.7948459386825562\n",
      "G loss: 1.2502448558807373\n",
      "E loss:  0.7787496447563171\n",
      "G loss: 1.2907685041427612\n",
      "Training Model  ...\n",
      "E loss:  0.8803921937942505\n",
      "G loss: 1.250827670097351\n",
      "E loss:  0.8797112703323364\n",
      "G loss: 1.2714990377426147\n",
      "E loss:  0.8751857280731201\n",
      "G loss: 1.3064597845077515\n",
      "E loss:  0.8750520944595337\n",
      "G loss: 1.3741278648376465\n",
      "E loss:  0.873278021812439\n",
      "G loss: 1.4472161531448364\n",
      "Training Model  ...\n",
      "E loss:  0.7336888909339905\n",
      "G loss: 1.3950402736663818\n",
      "E loss:  0.7369714975357056\n",
      "G loss: 1.34700345993042\n",
      "E loss:  0.743999719619751\n",
      "G loss: 1.2757432460784912\n",
      "E loss:  0.7609032988548279\n",
      "G loss: 1.2098431587219238\n",
      "E loss:  0.7642773985862732\n",
      "G loss: 1.1406172513961792\n",
      "Training Model  ...\n",
      "E loss:  0.8094763159751892\n",
      "G loss: 1.1745598316192627\n",
      "E loss:  0.8060007691383362\n",
      "G loss: 1.2031232118606567\n",
      "E loss:  0.8083115816116333\n",
      "G loss: 1.270759105682373\n",
      "E loss:  0.7985358238220215\n",
      "G loss: 1.278253436088562\n",
      "E loss:  0.7918123006820679\n",
      "G loss: 1.3691269159317017\n",
      "Training Model  ...\n",
      "E loss:  0.7874407172203064\n",
      "G loss: 1.3595435619354248\n",
      "E loss:  0.7845838069915771\n",
      "G loss: 1.2905727624893188\n",
      "E loss:  0.7666745781898499\n",
      "G loss: 1.2624449729919434\n",
      "E loss:  0.7599615454673767\n",
      "G loss: 1.2602031230926514\n",
      "E loss:  0.7511529326438904\n",
      "G loss: 1.1645532846450806\n",
      "Training Model  ...\n",
      "E loss:  0.7585930824279785\n",
      "G loss: 1.1766972541809082\n",
      "E loss:  0.7443594336509705\n",
      "G loss: 1.2001421451568604\n",
      "E loss:  0.7518651485443115\n",
      "G loss: 1.1347696781158447\n",
      "E loss:  0.7506662607192993\n",
      "G loss: 1.1374220848083496\n",
      "E loss:  0.759642481803894\n",
      "G loss: 1.1651926040649414\n",
      "Training Model  ...\n",
      "E loss:  0.7776967287063599\n",
      "G loss: 1.202858567237854\n",
      "E loss:  0.7644033432006836\n",
      "G loss: 1.1934514045715332\n",
      "E loss:  0.7811601758003235\n",
      "G loss: 1.2858012914657593\n",
      "E loss:  0.7797603011131287\n",
      "G loss: 1.3398091793060303\n",
      "E loss:  0.7758212089538574\n",
      "G loss: 1.437251329421997\n",
      "Training Model  ...\n",
      "E loss:  0.6805875301361084\n",
      "G loss: 1.4812003374099731\n",
      "E loss:  0.6764581203460693\n",
      "G loss: 1.3893722295761108\n",
      "E loss:  0.6842173337936401\n",
      "G loss: 1.3581520318984985\n",
      "E loss:  0.6891672015190125\n",
      "G loss: 1.28848397731781\n",
      "E loss:  0.696399986743927\n",
      "G loss: 1.330991268157959\n",
      "Training Model  ...\n",
      "E loss:  0.7429121136665344\n",
      "G loss: 1.3042445182800293\n",
      "E loss:  0.7107609510421753\n",
      "G loss: 1.2729791402816772\n",
      "E loss:  0.7041979432106018\n",
      "G loss: 1.2403782606124878\n",
      "E loss:  0.7090967893600464\n",
      "G loss: 1.1727526187896729\n",
      "E loss:  0.710347056388855\n",
      "G loss: 1.2269363403320312\n",
      "Training Model  ...\n",
      "E loss:  0.7669166326522827\n",
      "G loss: 1.146317720413208\n",
      "E loss:  0.778950035572052\n",
      "G loss: 1.1766583919525146\n",
      "E loss:  0.7759723663330078\n",
      "G loss: 1.1883823871612549\n",
      "E loss:  0.7762146592140198\n",
      "G loss: 1.2826529741287231\n",
      "E loss:  0.7831666469573975\n",
      "G loss: 1.2316224575042725\n",
      "Training Model  ...\n",
      "E loss:  0.7877116203308105\n",
      "G loss: 1.2114388942718506\n",
      "E loss:  0.7912285327911377\n",
      "G loss: 1.167566180229187\n",
      "E loss:  0.7852383255958557\n",
      "G loss: 1.2496142387390137\n",
      "E loss:  0.7894768714904785\n",
      "G loss: 1.2796090841293335\n",
      "E loss:  0.7995588183403015\n",
      "G loss: 1.278447151184082\n",
      "Training Model  ...\n",
      "E loss:  0.8091515302658081\n",
      "G loss: 1.2338604927062988\n",
      "E loss:  0.8083702325820923\n",
      "G loss: 1.2883251905441284\n",
      "E loss:  0.8018902540206909\n",
      "G loss: 1.2462940216064453\n",
      "E loss:  0.7937140464782715\n",
      "G loss: 1.3145670890808105\n",
      "E loss:  0.785826563835144\n",
      "G loss: 1.3785187005996704\n",
      "Training Model  ...\n",
      "E loss:  0.7050617933273315\n",
      "G loss: 1.3728305101394653\n",
      "E loss:  0.7068514823913574\n",
      "G loss: 1.330068826675415\n",
      "E loss:  0.7152184247970581\n",
      "G loss: 1.2575244903564453\n",
      "E loss:  0.713630735874176\n",
      "G loss: 1.2348392009735107\n",
      "E loss:  0.7130753397941589\n",
      "G loss: 1.1417114734649658\n",
      "Training Model  ...\n",
      "E loss:  0.7343530058860779\n",
      "G loss: 1.189795970916748\n",
      "E loss:  0.7369643449783325\n",
      "G loss: 1.1731815338134766\n",
      "E loss:  0.7310658693313599\n",
      "G loss: 1.1155216693878174\n",
      "E loss:  0.7126250863075256\n",
      "G loss: 1.1970458030700684\n",
      "E loss:  0.7120314836502075\n",
      "G loss: 1.1610792875289917\n",
      "Training Model  ...\n",
      "E loss:  0.7767301201820374\n",
      "G loss: 1.1632835865020752\n",
      "E loss:  0.7865189909934998\n",
      "G loss: 1.2522642612457275\n",
      "E loss:  0.7868009209632874\n",
      "G loss: 1.2726203203201294\n",
      "E loss:  0.7974126935005188\n",
      "G loss: 1.209146499633789\n",
      "E loss:  0.7918481230735779\n",
      "G loss: 1.3621703386306763\n",
      "Training Model  ...\n",
      "E loss:  0.7448579668998718\n",
      "G loss: 1.2836838960647583\n",
      "E loss:  0.7326177358627319\n",
      "G loss: 1.2323848009109497\n",
      "E loss:  0.7224116921424866\n",
      "G loss: 1.220633625984192\n",
      "E loss:  0.697083592414856\n",
      "G loss: 1.093194842338562\n",
      "E loss:  0.688635528087616\n",
      "G loss: 1.0591185092926025\n",
      "Training Model  ...\n",
      "E loss:  0.784902036190033\n",
      "G loss: 1.0882729291915894\n",
      "E loss:  0.768756628036499\n",
      "G loss: 1.1207367181777954\n",
      "E loss:  0.7716107964515686\n",
      "G loss: 1.118086338043213\n",
      "E loss:  0.7470941543579102\n",
      "G loss: 1.197914719581604\n",
      "E loss:  0.7414328455924988\n",
      "G loss: 1.2334249019622803\n",
      "Training Model  ...\n",
      "E loss:  0.7061938047409058\n",
      "G loss: 1.1597591638565063\n",
      "E loss:  0.7095280289649963\n",
      "G loss: 1.1576882600784302\n",
      "E loss:  0.7207247018814087\n",
      "G loss: 1.1602715253829956\n",
      "E loss:  0.7368913888931274\n",
      "G loss: 1.1894357204437256\n",
      "E loss:  0.7362624406814575\n",
      "G loss: 1.188798189163208\n",
      "Training Model  ...\n",
      "E loss:  0.8128109574317932\n",
      "G loss: 1.2390594482421875\n",
      "E loss:  0.8075848817825317\n",
      "G loss: 1.2419884204864502\n",
      "E loss:  0.7889953851699829\n",
      "G loss: 1.388292908668518\n",
      "E loss:  0.7901911735534668\n",
      "G loss: 1.4363365173339844\n",
      "E loss:  0.8021572828292847\n",
      "G loss: 1.5255972146987915\n",
      "Training Model  ...\n",
      "E loss:  0.8482449650764465\n",
      "G loss: 1.4954534769058228\n",
      "E loss:  0.8361175060272217\n",
      "G loss: 1.3768571615219116\n",
      "E loss:  0.8274263739585876\n",
      "G loss: 1.236248254776001\n",
      "E loss:  0.8182821273803711\n",
      "G loss: 1.1118985414505005\n",
      "E loss:  0.7999220490455627\n",
      "G loss: 1.0934829711914062\n",
      "Training Model  ...\n",
      "E loss:  0.7515725493431091\n",
      "G loss: 1.0336790084838867\n",
      "E loss:  0.7422444224357605\n",
      "G loss: 1.1304922103881836\n",
      "E loss:  0.7547585368156433\n",
      "G loss: 1.1508935689926147\n",
      "E loss:  0.740035891532898\n",
      "G loss: 1.3431432247161865\n",
      "E loss:  0.7321567535400391\n",
      "G loss: 1.4071720838546753\n",
      "Training Model  ...\n",
      "E loss:  0.6761700510978699\n",
      "G loss: 1.3159730434417725\n",
      "E loss:  0.6726325750350952\n",
      "G loss: 1.3017927408218384\n",
      "E loss:  0.6801834106445312\n",
      "G loss: 1.156302809715271\n",
      "E loss:  0.6801379323005676\n",
      "G loss: 1.1586016416549683\n",
      "E loss:  0.6725488901138306\n",
      "G loss: 1.0601344108581543\n",
      "Training Model  ...\n",
      "E loss:  0.8178562521934509\n",
      "G loss: 1.121460199356079\n",
      "E loss:  0.8259247541427612\n",
      "G loss: 1.1038053035736084\n",
      "E loss:  0.8327082991600037\n",
      "G loss: 1.02957284450531\n",
      "E loss:  0.8165237307548523\n",
      "G loss: 1.0691490173339844\n",
      "E loss:  0.7999159097671509\n",
      "G loss: 1.0984253883361816\n",
      "Training Model  ...\n",
      "E loss:  0.8118641376495361\n",
      "G loss: 1.1249163150787354\n",
      "E loss:  0.8030377626419067\n",
      "G loss: 1.1246803998947144\n",
      "E loss:  0.7948652505874634\n",
      "G loss: 1.1504100561141968\n",
      "E loss:  0.786548376083374\n",
      "G loss: 1.1942278146743774\n",
      "E loss:  0.786482036113739\n",
      "G loss: 1.2084811925888062\n",
      "Training Model  ...\n",
      "E loss:  0.8151915073394775\n",
      "G loss: 1.2547885179519653\n",
      "E loss:  0.8093457818031311\n",
      "G loss: 1.2294979095458984\n",
      "E loss:  0.8139939308166504\n",
      "G loss: 1.203566074371338\n",
      "E loss:  0.8066396713256836\n",
      "G loss: 1.1537203788757324\n",
      "E loss:  0.8083047270774841\n",
      "G loss: 1.246755599975586\n",
      "Training Model  ...\n",
      "E loss:  0.8159939050674438\n",
      "G loss: 1.215097427368164\n",
      "E loss:  0.8191214799880981\n",
      "G loss: 1.2143285274505615\n",
      "E loss:  0.810529351234436\n",
      "G loss: 1.229034662246704\n",
      "E loss:  0.7687104940414429\n",
      "G loss: 1.238075613975525\n",
      "E loss:  0.7661895751953125\n",
      "G loss: 1.2518644332885742\n",
      "Training Model  ...\n",
      "E loss:  0.809487521648407\n",
      "G loss: 1.256493330001831\n",
      "E loss:  0.8092992901802063\n",
      "G loss: 1.1602773666381836\n",
      "E loss:  0.8077429533004761\n",
      "G loss: 1.176254153251648\n",
      "E loss:  0.8123925924301147\n",
      "G loss: 1.1623432636260986\n",
      "E loss:  0.7994543313980103\n",
      "G loss: 1.1441173553466797\n",
      "Training Model  ...\n",
      "E loss:  0.6801941394805908\n",
      "G loss: 1.1138122081756592\n",
      "E loss:  0.6728692054748535\n",
      "G loss: 1.1149497032165527\n",
      "E loss:  0.6872930526733398\n",
      "G loss: 1.1292169094085693\n",
      "E loss:  0.6959851384162903\n",
      "G loss: 1.0580356121063232\n",
      "E loss:  0.7002068758010864\n",
      "G loss: 1.1923620700836182\n",
      "Training Model  ...\n",
      "E loss:  0.796219527721405\n",
      "G loss: 1.2003518342971802\n",
      "E loss:  0.7957660555839539\n",
      "G loss: 1.1625237464904785\n",
      "E loss:  0.7936804294586182\n",
      "G loss: 1.1553022861480713\n",
      "E loss:  0.7878158092498779\n",
      "G loss: 1.1132980585098267\n",
      "E loss:  0.7705491185188293\n",
      "G loss: 1.2297614812850952\n",
      "Training Model  ...\n",
      "E loss:  0.7894929051399231\n",
      "G loss: 1.169574499130249\n",
      "E loss:  0.7924901247024536\n",
      "G loss: 1.1645041704177856\n",
      "E loss:  0.777353823184967\n",
      "G loss: 1.156516671180725\n",
      "E loss:  0.7725235223770142\n",
      "G loss: 1.1461114883422852\n",
      "E loss:  0.7866623401641846\n",
      "G loss: 1.1219764947891235\n",
      "Training Model  ...\n",
      "E loss:  0.7594693303108215\n",
      "G loss: 1.142618179321289\n",
      "E loss:  0.7587746381759644\n",
      "G loss: 1.0954434871673584\n",
      "E loss:  0.7629323601722717\n",
      "G loss: 1.107197880744934\n",
      "E loss:  0.7469140291213989\n",
      "G loss: 1.1325565576553345\n",
      "E loss:  0.7448837161064148\n",
      "G loss: 1.1357377767562866\n",
      "Training Model  ...\n",
      "E loss:  0.8024780750274658\n",
      "G loss: 1.1120048761367798\n",
      "E loss:  0.8082516193389893\n",
      "G loss: 1.1083050966262817\n",
      "E loss:  0.8065547347068787\n",
      "G loss: 1.1284141540527344\n",
      "E loss:  0.7994367480278015\n",
      "G loss: 1.0848634243011475\n",
      "E loss:  0.8060358166694641\n",
      "G loss: 1.1290152072906494\n",
      "Training Model  ...\n",
      "E loss:  0.738114595413208\n",
      "G loss: 1.1414902210235596\n",
      "E loss:  0.7385278344154358\n",
      "G loss: 1.1887938976287842\n",
      "E loss:  0.7402870655059814\n",
      "G loss: 1.2188891172409058\n",
      "E loss:  0.737442135810852\n",
      "G loss: 1.1567389965057373\n",
      "E loss:  0.7376406192779541\n",
      "G loss: 1.1796956062316895\n",
      "Training Model  ...\n",
      "E loss:  0.7490891218185425\n",
      "G loss: 1.2084662914276123\n",
      "E loss:  0.7382757663726807\n",
      "G loss: 1.2136616706848145\n",
      "E loss:  0.7410519123077393\n",
      "G loss: 1.2246067523956299\n",
      "E loss:  0.7361752390861511\n",
      "G loss: 1.1955115795135498\n",
      "E loss:  0.7455383539199829\n",
      "G loss: 1.1094329357147217\n",
      "Training Model  ...\n",
      "E loss:  0.7816135287284851\n",
      "G loss: 1.1288844347000122\n",
      "E loss:  0.7624411582946777\n",
      "G loss: 1.1029566526412964\n",
      "E loss:  0.7757071256637573\n",
      "G loss: 1.167339563369751\n",
      "E loss:  0.7769584655761719\n",
      "G loss: 1.1197847127914429\n",
      "E loss:  0.7698301076889038\n",
      "G loss: 1.092024803161621\n",
      "Training Model  ...\n",
      "E loss:  0.8520825505256653\n",
      "G loss: 1.0961220264434814\n",
      "E loss:  0.8526930809020996\n",
      "G loss: 1.1499086618423462\n",
      "E loss:  0.824676513671875\n",
      "G loss: 1.1087068319320679\n",
      "E loss:  0.8340916633605957\n",
      "G loss: 1.1497111320495605\n",
      "E loss:  0.8391947150230408\n",
      "G loss: 1.126169204711914\n",
      "Training Model  ...\n",
      "E loss:  0.7314207553863525\n",
      "G loss: 1.125149130821228\n",
      "E loss:  0.7285729646682739\n",
      "G loss: 1.2266690731048584\n",
      "E loss:  0.7517441511154175\n",
      "G loss: 1.3105669021606445\n",
      "E loss:  0.749600887298584\n",
      "G loss: 1.3290518522262573\n",
      "E loss:  0.7543479204177856\n",
      "G loss: 1.3549522161483765\n",
      "Training Model  ...\n",
      "E loss:  0.7303094863891602\n",
      "G loss: 1.3190205097198486\n",
      "E loss:  0.7293409705162048\n",
      "G loss: 1.2710012197494507\n",
      "E loss:  0.7360792756080627\n",
      "G loss: 1.293876051902771\n",
      "E loss:  0.7426268458366394\n",
      "G loss: 1.3083387613296509\n",
      "E loss:  0.7441614866256714\n",
      "G loss: 1.2381678819656372\n",
      "Training Model  ...\n",
      "E loss:  0.8054494261741638\n",
      "G loss: 1.2717418670654297\n",
      "E loss:  0.812641978263855\n",
      "G loss: 1.3132309913635254\n",
      "E loss:  0.8166484832763672\n",
      "G loss: 1.2453529834747314\n",
      "E loss:  0.8135952353477478\n",
      "G loss: 1.241591215133667\n",
      "E loss:  0.8088252544403076\n",
      "G loss: 1.25397527217865\n",
      "Training Model  ...\n",
      "E loss:  0.8412641882896423\n",
      "G loss: 1.2311203479766846\n",
      "E loss:  0.8560556769371033\n",
      "G loss: 1.1844055652618408\n",
      "E loss:  0.8647027015686035\n",
      "G loss: 1.1518405675888062\n",
      "E loss:  0.8667399883270264\n",
      "G loss: 1.044476866722107\n",
      "E loss:  0.8784574270248413\n",
      "G loss: 0.9850854873657227\n",
      "Training Model  ...\n",
      "E loss:  0.9152730703353882\n",
      "G loss: 0.9524588584899902\n",
      "E loss:  0.8973373174667358\n",
      "G loss: 0.9140773415565491\n",
      "E loss:  0.8949589729309082\n",
      "G loss: 1.024372935295105\n",
      "E loss:  0.9151908159255981\n",
      "G loss: 1.0045287609100342\n",
      "E loss:  0.9017465114593506\n",
      "G loss: 1.0178059339523315\n",
      "Training Model  ...\n",
      "E loss:  0.8080062866210938\n",
      "G loss: 1.0320744514465332\n",
      "E loss:  0.8026885986328125\n",
      "G loss: 1.0733799934387207\n",
      "E loss:  0.8199273347854614\n",
      "G loss: 1.0823566913604736\n",
      "E loss:  0.8061991930007935\n",
      "G loss: 1.170440912246704\n",
      "E loss:  0.7994657754898071\n",
      "G loss: 1.1278291940689087\n",
      "Training Model  ...\n",
      "E loss:  0.7291204929351807\n",
      "G loss: 1.106068730354309\n",
      "E loss:  0.7065691351890564\n",
      "G loss: 1.131792664527893\n",
      "E loss:  0.7018712162971497\n",
      "G loss: 1.1384953260421753\n",
      "E loss:  0.6974976658821106\n",
      "G loss: 1.1150331497192383\n",
      "E loss:  0.6863740682601929\n",
      "G loss: 1.1598176956176758\n",
      "Training Model  ...\n",
      "E loss:  0.6743236184120178\n",
      "G loss: 1.1087920665740967\n",
      "E loss:  0.6716346740722656\n",
      "G loss: 1.1077152490615845\n",
      "E loss:  0.6699644327163696\n",
      "G loss: 1.0936598777770996\n",
      "E loss:  0.6690400838851929\n",
      "G loss: 1.0863033533096313\n",
      "E loss:  0.6805007457733154\n",
      "G loss: 1.0560013055801392\n",
      "Training Model  ...\n",
      "E loss:  0.6758471727371216\n",
      "G loss: 1.1050372123718262\n",
      "E loss:  0.675599217414856\n",
      "G loss: 1.0805853605270386\n",
      "E loss:  0.690448522567749\n",
      "G loss: 1.0999491214752197\n",
      "E loss:  0.6948630213737488\n",
      "G loss: 1.034775972366333\n",
      "E loss:  0.6856271624565125\n",
      "G loss: 1.0594382286071777\n",
      "Training Model  ...\n",
      "E loss:  0.7951573729515076\n",
      "G loss: 1.0619561672210693\n",
      "E loss:  0.7883458137512207\n",
      "G loss: 1.0706532001495361\n",
      "E loss:  0.7935526371002197\n",
      "G loss: 1.146384596824646\n",
      "E loss:  0.7631396651268005\n",
      "G loss: 1.139418601989746\n",
      "E loss:  0.7672867774963379\n",
      "G loss: 1.0783320665359497\n",
      "Training Model  ...\n",
      "E loss:  0.8111675977706909\n",
      "G loss: 1.1278407573699951\n",
      "E loss:  0.8144407868385315\n",
      "G loss: 1.1345540285110474\n",
      "E loss:  0.8112239241600037\n",
      "G loss: 1.167189121246338\n",
      "E loss:  0.8036653399467468\n",
      "G loss: 1.1355971097946167\n",
      "E loss:  0.7983517646789551\n",
      "G loss: 1.14566171169281\n",
      "Training Model  ...\n",
      "E loss:  0.7382907867431641\n",
      "G loss: 1.128161072731018\n",
      "E loss:  0.7483314275741577\n",
      "G loss: 1.1773176193237305\n",
      "E loss:  0.7528924942016602\n",
      "G loss: 1.1132752895355225\n",
      "E loss:  0.736714243888855\n",
      "G loss: 1.147937536239624\n",
      "E loss:  0.7337496876716614\n",
      "G loss: 1.1082388162612915\n",
      "Training Model  ...\n",
      "E loss:  0.7564237117767334\n",
      "G loss: 1.1056239604949951\n",
      "E loss:  0.7535082101821899\n",
      "G loss: 1.1921237707138062\n",
      "E loss:  0.7523870468139648\n",
      "G loss: 1.145302176475525\n",
      "E loss:  0.7489013671875\n",
      "G loss: 1.1473733186721802\n",
      "E loss:  0.7452250123023987\n",
      "G loss: 1.126800537109375\n",
      "Training Model  ...\n",
      "E loss:  0.7893945574760437\n",
      "G loss: 1.1090164184570312\n",
      "E loss:  0.8154876232147217\n",
      "G loss: 1.1180673837661743\n",
      "E loss:  0.8077431917190552\n",
      "G loss: 1.0266435146331787\n",
      "E loss:  0.7989625930786133\n",
      "G loss: 1.014912486076355\n",
      "E loss:  0.7847187519073486\n",
      "G loss: 0.9824298620223999\n",
      "Training Model  ...\n",
      "E loss:  0.7547934055328369\n",
      "G loss: 0.9813854098320007\n",
      "E loss:  0.7350893616676331\n",
      "G loss: 0.9933092594146729\n",
      "E loss:  0.7412911057472229\n",
      "G loss: 1.0758496522903442\n",
      "E loss:  0.722633957862854\n",
      "G loss: 1.1007808446884155\n",
      "E loss:  0.7063430547714233\n",
      "G loss: 1.1739988327026367\n",
      "Training Model  ...\n",
      "E loss:  0.720589280128479\n",
      "G loss: 1.1470736265182495\n",
      "E loss:  0.721846342086792\n",
      "G loss: 1.1073986291885376\n",
      "E loss:  0.7109125256538391\n",
      "G loss: 1.135110855102539\n",
      "E loss:  0.706722617149353\n",
      "G loss: 1.0906505584716797\n",
      "E loss:  0.707649827003479\n",
      "G loss: 1.1069107055664062\n",
      "Training Model  ...\n",
      "E loss:  0.7676208019256592\n",
      "G loss: 0.9814187288284302\n",
      "E loss:  0.7757136225700378\n",
      "G loss: 1.026688575744629\n",
      "E loss:  0.777047872543335\n",
      "G loss: 0.9993548393249512\n",
      "E loss:  0.7856342196464539\n",
      "G loss: 1.0000598430633545\n",
      "E loss:  0.7948376536369324\n",
      "G loss: 1.0111780166625977\n",
      "Training Model  ...\n",
      "E loss:  0.8037499189376831\n",
      "G loss: 0.9989567995071411\n",
      "E loss:  0.798679530620575\n",
      "G loss: 0.9773540496826172\n",
      "E loss:  0.8157966732978821\n",
      "G loss: 1.0467442274093628\n",
      "E loss:  0.823239266872406\n",
      "G loss: 1.070643663406372\n",
      "E loss:  0.8062229752540588\n",
      "G loss: 1.0764951705932617\n",
      "Training Model  ...\n",
      "E loss:  0.706773579120636\n",
      "G loss: 1.0418219566345215\n",
      "E loss:  0.7096356153488159\n",
      "G loss: 1.1052008867263794\n",
      "E loss:  0.7208335399627686\n",
      "G loss: 1.1287490129470825\n",
      "E loss:  0.7279548645019531\n",
      "G loss: 1.0435658693313599\n",
      "E loss:  0.7342540621757507\n",
      "G loss: 1.1039236783981323\n",
      "Training Model  ...\n",
      "E loss:  0.8018854856491089\n",
      "G loss: 1.0660076141357422\n",
      "E loss:  0.7994962930679321\n",
      "G loss: 1.0534234046936035\n",
      "E loss:  0.7836815118789673\n",
      "G loss: 1.0941200256347656\n",
      "E loss:  0.783045768737793\n",
      "G loss: 1.1216286420822144\n",
      "E loss:  0.778109610080719\n",
      "G loss: 1.1425695419311523\n",
      "Training Model  ...\n",
      "E loss:  0.7156259417533875\n",
      "G loss: 1.1371506452560425\n",
      "E loss:  0.7173707485198975\n",
      "G loss: 1.0724607706069946\n",
      "E loss:  0.705428421497345\n",
      "G loss: 1.0943635702133179\n",
      "E loss:  0.7137530446052551\n",
      "G loss: 1.0663588047027588\n",
      "E loss:  0.7249796986579895\n",
      "G loss: 1.1139165163040161\n",
      "Training Model  ...\n",
      "E loss:  0.7087557911872864\n",
      "G loss: 1.1192072629928589\n",
      "E loss:  0.7125146389007568\n",
      "G loss: 1.0747523307800293\n",
      "E loss:  0.7159203886985779\n",
      "G loss: 1.0772219896316528\n",
      "E loss:  0.7032917141914368\n",
      "G loss: 1.0043529272079468\n",
      "E loss:  0.7073701620101929\n",
      "G loss: 1.027215600013733\n",
      "Training Model  ...\n",
      "E loss:  0.8207311034202576\n",
      "G loss: 1.057777762413025\n",
      "E loss:  0.8190184235572815\n",
      "G loss: 1.041121244430542\n",
      "E loss:  0.8196709156036377\n",
      "G loss: 1.018659234046936\n",
      "E loss:  0.8370532989501953\n",
      "G loss: 0.9934993386268616\n",
      "E loss:  0.8290517330169678\n",
      "G loss: 0.9242472052574158\n",
      "Training Model  ...\n",
      "E loss:  0.7174369692802429\n",
      "G loss: 0.9415689706802368\n",
      "E loss:  0.7237510681152344\n",
      "G loss: 0.9767376780509949\n",
      "E loss:  0.7385936379432678\n",
      "G loss: 1.0019985437393188\n",
      "E loss:  0.7571861147880554\n",
      "G loss: 1.0023303031921387\n",
      "E loss:  0.7542845606803894\n",
      "G loss: 1.1007537841796875\n",
      "Training Model  ...\n",
      "E loss:  0.7059276103973389\n",
      "G loss: 1.0243024826049805\n",
      "E loss:  0.7152819633483887\n",
      "G loss: 1.0172393321990967\n",
      "E loss:  0.7145492434501648\n",
      "G loss: 1.0137356519699097\n",
      "E loss:  0.7056216597557068\n",
      "G loss: 0.9523922801017761\n",
      "E loss:  0.6879194974899292\n",
      "G loss: 0.9383236169815063\n",
      "Training Model  ...\n",
      "E loss:  0.6455958485603333\n",
      "G loss: 0.8871258497238159\n",
      "E loss:  0.6413030624389648\n",
      "G loss: 0.9243760704994202\n",
      "E loss:  0.6469288468360901\n",
      "G loss: 0.8880556225776672\n",
      "E loss:  0.6410917043685913\n",
      "G loss: 0.9470848441123962\n",
      "E loss:  0.6407433748245239\n",
      "G loss: 1.0063387155532837\n",
      "Training Model  ...\n",
      "E loss:  0.6912566423416138\n",
      "G loss: 0.9271878004074097\n",
      "E loss:  0.6942809224128723\n",
      "G loss: 0.9981642961502075\n",
      "E loss:  0.6836608052253723\n",
      "G loss: 0.9803012609481812\n",
      "E loss:  0.7114887237548828\n",
      "G loss: 1.0242716073989868\n",
      "E loss:  0.6976455450057983\n",
      "G loss: 1.0259861946105957\n",
      "Training Model  ...\n",
      "E loss:  0.7075390815734863\n",
      "G loss: 0.993850827217102\n",
      "E loss:  0.7085099816322327\n",
      "G loss: 1.0252944231033325\n",
      "E loss:  0.7110418677330017\n",
      "G loss: 1.0598217248916626\n",
      "E loss:  0.7011637687683105\n",
      "G loss: 1.0110538005828857\n",
      "E loss:  0.6906606554985046\n",
      "G loss: 1.1251107454299927\n",
      "Training Model  ...\n",
      "E loss:  0.7178225517272949\n",
      "G loss: 1.0994129180908203\n",
      "E loss:  0.7106927633285522\n",
      "G loss: 1.0994093418121338\n",
      "E loss:  0.7156355381011963\n",
      "G loss: 1.0108115673065186\n",
      "E loss:  0.7142146825790405\n",
      "G loss: 0.9416048526763916\n",
      "E loss:  0.7132635712623596\n",
      "G loss: 1.0016858577728271\n",
      "Training Model  ...\n",
      "E loss:  0.8507590293884277\n",
      "G loss: 0.9770590662956238\n",
      "E loss:  0.834017276763916\n",
      "G loss: 0.969399094581604\n",
      "E loss:  0.833161473274231\n",
      "G loss: 0.9934070110321045\n",
      "E loss:  0.8321406245231628\n",
      "G loss: 0.9827530384063721\n",
      "E loss:  0.8387632966041565\n",
      "G loss: 0.9464712738990784\n",
      "Training Model  ...\n",
      "E loss:  0.7075803875923157\n",
      "G loss: 1.005863070487976\n",
      "E loss:  0.7097421884536743\n",
      "G loss: 0.9896723031997681\n",
      "E loss:  0.7099114656448364\n",
      "G loss: 1.0071146488189697\n",
      "E loss:  0.6999737024307251\n",
      "G loss: 0.9885671138763428\n",
      "E loss:  0.6916377544403076\n",
      "G loss: 1.0593862533569336\n",
      "Training Model  ...\n",
      "E loss:  0.7220861911773682\n",
      "G loss: 0.9558449387550354\n",
      "E loss:  0.7193017601966858\n",
      "G loss: 0.9937108755111694\n",
      "E loss:  0.7229029536247253\n",
      "G loss: 0.9833233952522278\n",
      "E loss:  0.7230139970779419\n",
      "G loss: 1.029813289642334\n",
      "E loss:  0.715740978717804\n",
      "G loss: 0.9910159111022949\n",
      "Training Model  ...\n",
      "E loss:  0.7721457481384277\n",
      "G loss: 1.0621362924575806\n",
      "E loss:  0.7722510099411011\n",
      "G loss: 1.066004753112793\n",
      "E loss:  0.7709017395973206\n",
      "G loss: 1.077000379562378\n",
      "E loss:  0.7518775463104248\n",
      "G loss: 1.0477443933486938\n",
      "E loss:  0.7509013414382935\n",
      "G loss: 1.082029104232788\n",
      "Training Model  ...\n",
      "E loss:  0.8318748474121094\n",
      "G loss: 1.0939029455184937\n",
      "E loss:  0.82286536693573\n",
      "G loss: 1.1077349185943604\n",
      "E loss:  0.8239681124687195\n",
      "G loss: 1.0463132858276367\n",
      "E loss:  0.8138975501060486\n",
      "G loss: 0.971032977104187\n",
      "E loss:  0.8038074374198914\n",
      "G loss: 0.9985395073890686\n",
      "Training Model  ...\n",
      "E loss:  0.7514932155609131\n",
      "G loss: 0.9796701669692993\n",
      "E loss:  0.7489896416664124\n",
      "G loss: 0.9634220600128174\n",
      "E loss:  0.7375673651695251\n",
      "G loss: 0.9641903638839722\n",
      "E loss:  0.7322992086410522\n",
      "G loss: 0.9438309073448181\n",
      "E loss:  0.7372612357139587\n",
      "G loss: 0.897476077079773\n",
      "Training Model  ...\n",
      "E loss:  0.8335868120193481\n",
      "G loss: 0.9163716435432434\n",
      "E loss:  0.8066973686218262\n",
      "G loss: 0.9459446668624878\n",
      "E loss:  0.7940056324005127\n",
      "G loss: 0.9987321496009827\n",
      "E loss:  0.7879657745361328\n",
      "G loss: 1.1481578350067139\n",
      "E loss:  0.8013502359390259\n",
      "G loss: 1.1912221908569336\n",
      "Training Model  ...\n",
      "E loss:  0.6736329793930054\n",
      "G loss: 1.0911662578582764\n",
      "E loss:  0.6838402152061462\n",
      "G loss: 1.0538761615753174\n",
      "E loss:  0.6835471987724304\n",
      "G loss: 0.9892316460609436\n",
      "E loss:  0.6602824926376343\n",
      "G loss: 0.945379912853241\n",
      "E loss:  0.6766168475151062\n",
      "G loss: 0.882965624332428\n",
      "Training Model  ...\n",
      "E loss:  0.7097077965736389\n",
      "G loss: 0.8980922698974609\n",
      "E loss:  0.6972548365592957\n",
      "G loss: 0.9156240820884705\n",
      "E loss:  0.705102801322937\n",
      "G loss: 0.9258476495742798\n",
      "E loss:  0.691908061504364\n",
      "G loss: 0.947935938835144\n",
      "E loss:  0.6763274669647217\n",
      "G loss: 1.011009931564331\n",
      "Training Model  ...\n",
      "E loss:  0.6769943237304688\n",
      "G loss: 0.9152928590774536\n",
      "E loss:  0.6956402063369751\n",
      "G loss: 0.9174087643623352\n",
      "E loss:  0.6988513469696045\n",
      "G loss: 1.0112316608428955\n",
      "E loss:  0.699390172958374\n",
      "G loss: 0.95196932554245\n",
      "E loss:  0.7012574672698975\n",
      "G loss: 0.9243032932281494\n",
      "Training Model  ...\n",
      "E loss:  0.7557362914085388\n",
      "G loss: 0.935564398765564\n",
      "E loss:  0.7658050060272217\n",
      "G loss: 0.9752033352851868\n",
      "E loss:  0.7635682821273804\n",
      "G loss: 1.0383213758468628\n",
      "E loss:  0.76784348487854\n",
      "G loss: 1.0391792058944702\n",
      "E loss:  0.7611386179924011\n",
      "G loss: 1.108721137046814\n",
      "Training Model  ...\n",
      "E loss:  0.7018018960952759\n",
      "G loss: 1.0258129835128784\n",
      "E loss:  0.6991702914237976\n",
      "G loss: 1.056016206741333\n",
      "E loss:  0.7092852592468262\n",
      "G loss: 0.986004650592804\n",
      "E loss:  0.7101984024047852\n",
      "G loss: 0.9770539999008179\n",
      "E loss:  0.7198690176010132\n",
      "G loss: 0.9032289385795593\n",
      "Training Model  ...\n",
      "E loss:  0.8753699660301208\n",
      "G loss: 0.9292092323303223\n",
      "E loss:  0.8837301135063171\n",
      "G loss: 0.9821953177452087\n",
      "E loss:  0.8762903809547424\n",
      "G loss: 0.9541816115379333\n",
      "E loss:  0.8504894971847534\n",
      "G loss: 0.9923141002655029\n",
      "E loss:  0.8575865030288696\n",
      "G loss: 0.9951916337013245\n",
      "Training Model  ...\n",
      "E loss:  0.7173405885696411\n",
      "G loss: 1.0199995040893555\n",
      "E loss:  0.7150283455848694\n",
      "G loss: 0.9679865837097168\n",
      "E loss:  0.7105940580368042\n",
      "G loss: 0.9699026942253113\n",
      "E loss:  0.7120824456214905\n",
      "G loss: 1.0013331174850464\n",
      "E loss:  0.7154910564422607\n",
      "G loss: 0.947515606880188\n",
      "Training Model  ...\n",
      "E loss:  0.8155292868614197\n",
      "G loss: 0.9702017903327942\n",
      "E loss:  0.8246999382972717\n",
      "G loss: 1.0100111961364746\n",
      "E loss:  0.8184570074081421\n",
      "G loss: 1.0092276334762573\n",
      "E loss:  0.8354761004447937\n",
      "G loss: 1.0380265712738037\n",
      "E loss:  0.8518400192260742\n",
      "G loss: 1.0410130023956299\n",
      "Training Model  ...\n",
      "E loss:  0.6557663083076477\n",
      "G loss: 1.0481009483337402\n",
      "E loss:  0.6677271723747253\n",
      "G loss: 0.9803339838981628\n",
      "E loss:  0.6785844564437866\n",
      "G loss: 0.941936731338501\n",
      "E loss:  0.6876517534255981\n",
      "G loss: 0.9980300664901733\n",
      "E loss:  0.6807771325111389\n",
      "G loss: 0.9158965945243835\n",
      "Training Model  ...\n",
      "E loss:  0.6940093040466309\n",
      "G loss: 0.9570629000663757\n",
      "E loss:  0.6949896216392517\n",
      "G loss: 0.9341811537742615\n",
      "E loss:  0.6910410523414612\n",
      "G loss: 0.9599130749702454\n",
      "E loss:  0.6883793473243713\n",
      "G loss: 0.9395806789398193\n",
      "E loss:  0.6903743147850037\n",
      "G loss: 1.0048450231552124\n",
      "Training Model  ...\n",
      "E loss:  0.7692118883132935\n",
      "G loss: 0.9605680704116821\n",
      "E loss:  0.7569943070411682\n",
      "G loss: 0.9128717184066772\n",
      "E loss:  0.7408855557441711\n",
      "G loss: 0.8107780814170837\n",
      "E loss:  0.7431385517120361\n",
      "G loss: 0.7639405727386475\n",
      "E loss:  0.7402235865592957\n",
      "G loss: 0.7457163333892822\n",
      "Training Model  ...\n",
      "E loss:  0.7907099723815918\n",
      "G loss: 0.7082546353340149\n",
      "E loss:  0.780356228351593\n",
      "G loss: 0.7362861633300781\n",
      "E loss:  0.765871524810791\n",
      "G loss: 0.7678690552711487\n",
      "E loss:  0.7672160863876343\n",
      "G loss: 0.8420456647872925\n",
      "E loss:  0.7882934212684631\n",
      "G loss: 0.8979262113571167\n",
      "Training Model  ...\n",
      "E loss:  0.7546368837356567\n",
      "G loss: 0.8929202556610107\n",
      "E loss:  0.7471145987510681\n",
      "G loss: 0.9291755557060242\n",
      "E loss:  0.7376918196678162\n",
      "G loss: 0.8831090331077576\n",
      "E loss:  0.737231433391571\n",
      "G loss: 0.8849197030067444\n",
      "E loss:  0.7424502968788147\n",
      "G loss: 0.8500959873199463\n",
      "Training Model  ...\n",
      "E loss:  0.7625697255134583\n",
      "G loss: 0.8636944890022278\n",
      "E loss:  0.740504264831543\n",
      "G loss: 0.9057154655456543\n",
      "E loss:  0.7522425651550293\n",
      "G loss: 0.8547623753547668\n",
      "E loss:  0.7324003577232361\n",
      "G loss: 0.9634863138198853\n",
      "E loss:  0.7438594698905945\n",
      "G loss: 0.8872972726821899\n",
      "Training Model  ...\n",
      "E loss:  0.6969499588012695\n",
      "G loss: 0.8997187614440918\n",
      "E loss:  0.6928707361221313\n",
      "G loss: 0.9523642063140869\n",
      "E loss:  0.6932178139686584\n",
      "G loss: 0.9251302480697632\n",
      "E loss:  0.6985371112823486\n",
      "G loss: 0.9189964532852173\n",
      "E loss:  0.7078619003295898\n",
      "G loss: 0.9838042259216309\n",
      "Training Model  ...\n",
      "E loss:  0.757823646068573\n",
      "G loss: 0.9439719319343567\n",
      "E loss:  0.7511429786682129\n",
      "G loss: 0.9404147863388062\n",
      "E loss:  0.7752312421798706\n",
      "G loss: 1.0128675699234009\n",
      "E loss:  0.7719054818153381\n",
      "G loss: 0.9823644161224365\n",
      "E loss:  0.7711041569709778\n",
      "G loss: 1.098960041999817\n",
      "Training Model  ...\n",
      "E loss:  0.6523179411888123\n",
      "G loss: 1.068049669265747\n",
      "E loss:  0.6512523889541626\n",
      "G loss: 1.09720778465271\n",
      "E loss:  0.6581554412841797\n",
      "G loss: 1.0572330951690674\n",
      "E loss:  0.6545367240905762\n",
      "G loss: 1.0816068649291992\n",
      "E loss:  0.6505225300788879\n",
      "G loss: 1.0903239250183105\n",
      "Training Model  ...\n",
      "E loss:  0.7149915099143982\n",
      "G loss: 1.0147292613983154\n",
      "E loss:  0.7043161392211914\n",
      "G loss: 1.055892825126648\n",
      "E loss:  0.7127574682235718\n",
      "G loss: 1.0636658668518066\n",
      "E loss:  0.6857553720474243\n",
      "G loss: 1.0130215883255005\n",
      "E loss:  0.6803750991821289\n",
      "G loss: 1.0019906759262085\n",
      "Training Model  ...\n",
      "E loss:  0.6805968284606934\n",
      "G loss: 0.9455026388168335\n",
      "E loss:  0.6792998313903809\n",
      "G loss: 0.9497330188751221\n",
      "E loss:  0.6876002550125122\n",
      "G loss: 0.9478041529655457\n",
      "E loss:  0.7019813656806946\n",
      "G loss: 0.8974040746688843\n",
      "E loss:  0.7113885283470154\n",
      "G loss: 0.8395494222640991\n",
      "Training Model  ...\n",
      "E loss:  0.7718920707702637\n",
      "G loss: 0.8835548162460327\n",
      "E loss:  0.7589094638824463\n",
      "G loss: 0.8896819353103638\n",
      "E loss:  0.7862406969070435\n",
      "G loss: 0.908327043056488\n",
      "E loss:  0.7881584763526917\n",
      "G loss: 0.9559695720672607\n",
      "E loss:  0.7803166508674622\n",
      "G loss: 0.9242696166038513\n",
      "Training Model  ...\n",
      "E loss:  0.7009373307228088\n",
      "G loss: 1.005623698234558\n",
      "E loss:  0.7136207818984985\n",
      "G loss: 0.8890568017959595\n",
      "E loss:  0.7129356861114502\n",
      "G loss: 0.8723624348640442\n",
      "E loss:  0.7062489986419678\n",
      "G loss: 0.8368071913719177\n",
      "E loss:  0.7043322324752808\n",
      "G loss: 0.7795249819755554\n",
      "Training Model  ...\n",
      "E loss:  0.7324487566947937\n",
      "G loss: 0.8360946178436279\n",
      "E loss:  0.7321032881736755\n",
      "G loss: 0.8457210659980774\n",
      "E loss:  0.7156258225440979\n",
      "G loss: 0.842301070690155\n",
      "E loss:  0.7047690153121948\n",
      "G loss: 0.9398171305656433\n",
      "E loss:  0.6950487494468689\n",
      "G loss: 0.9006324410438538\n",
      "Training Model  ...\n",
      "E loss:  0.7520777583122253\n",
      "G loss: 0.9838221073150635\n",
      "E loss:  0.7431905269622803\n",
      "G loss: 0.9250858426094055\n",
      "E loss:  0.7426264882087708\n",
      "G loss: 0.924335777759552\n",
      "E loss:  0.7298420667648315\n",
      "G loss: 0.9053341150283813\n",
      "E loss:  0.727596640586853\n",
      "G loss: 0.8751251697540283\n",
      "Training Model  ...\n",
      "E loss:  0.7136932015419006\n",
      "G loss: 0.925909161567688\n",
      "E loss:  0.7046955823898315\n",
      "G loss: 0.9013397693634033\n",
      "E loss:  0.691659152507782\n",
      "G loss: 0.9179136157035828\n",
      "E loss:  0.6816598176956177\n",
      "G loss: 0.8699537515640259\n",
      "E loss:  0.6955135464668274\n",
      "G loss: 0.9111623167991638\n",
      "Training Model  ...\n",
      "E loss:  0.7420868873596191\n",
      "G loss: 0.9371285438537598\n",
      "E loss:  0.7301347255706787\n",
      "G loss: 0.9424780011177063\n",
      "E loss:  0.7356476187705994\n",
      "G loss: 0.9081485867500305\n",
      "E loss:  0.736815333366394\n",
      "G loss: 0.9693412780761719\n",
      "E loss:  0.731881320476532\n",
      "G loss: 0.9692237377166748\n",
      "Training Model  ...\n",
      "E loss:  0.6967921257019043\n",
      "G loss: 0.9392930865287781\n",
      "E loss:  0.6929483413696289\n",
      "G loss: 0.9200035929679871\n",
      "E loss:  0.6906500458717346\n",
      "G loss: 0.9083942174911499\n",
      "E loss:  0.686450183391571\n",
      "G loss: 0.8310149908065796\n",
      "E loss:  0.6716939210891724\n",
      "G loss: 0.8387200832366943\n",
      "Training Model  ...\n",
      "E loss:  0.7681882381439209\n",
      "G loss: 0.8290559649467468\n",
      "E loss:  0.7731307744979858\n",
      "G loss: 0.8848894238471985\n",
      "E loss:  0.7624419331550598\n",
      "G loss: 0.8598270416259766\n",
      "E loss:  0.7684832215309143\n",
      "G loss: 0.9247836470603943\n",
      "E loss:  0.7672654986381531\n",
      "G loss: 0.9486473798751831\n",
      "Training Model  ...\n",
      "E loss:  0.7351455092430115\n",
      "G loss: 0.92475426197052\n",
      "E loss:  0.7419443726539612\n",
      "G loss: 0.9020214080810547\n",
      "E loss:  0.7310779094696045\n",
      "G loss: 0.856141984462738\n",
      "E loss:  0.7396737933158875\n",
      "G loss: 0.8800613880157471\n",
      "E loss:  0.7373805046081543\n",
      "G loss: 0.8161035180091858\n",
      "Training Model  ...\n",
      "E loss:  0.6945769786834717\n",
      "G loss: 0.8611739277839661\n",
      "E loss:  0.6992062330245972\n",
      "G loss: 0.8622167110443115\n",
      "E loss:  0.715110182762146\n",
      "G loss: 0.9109246134757996\n",
      "E loss:  0.6777989268302917\n",
      "G loss: 0.9398755431175232\n",
      "E loss:  0.6891473531723022\n",
      "G loss: 0.8730952143669128\n",
      "Training Model  ...\n",
      "E loss:  0.7259868383407593\n",
      "G loss: 0.9007986783981323\n",
      "E loss:  0.7331872582435608\n",
      "G loss: 0.870461642742157\n",
      "E loss:  0.7426491975784302\n",
      "G loss: 0.9302866458892822\n",
      "E loss:  0.7458155155181885\n",
      "G loss: 0.9689170122146606\n",
      "E loss:  0.7535684108734131\n",
      "G loss: 1.0388797521591187\n",
      "Training Model  ...\n",
      "E loss:  0.7129614353179932\n",
      "G loss: 1.0359264612197876\n",
      "E loss:  0.6965245008468628\n",
      "G loss: 0.9735075831413269\n",
      "E loss:  0.6913481950759888\n",
      "G loss: 0.8989590406417847\n",
      "E loss:  0.6938184499740601\n",
      "G loss: 0.7806114554405212\n",
      "E loss:  0.6858845949172974\n",
      "G loss: 0.7711484432220459\n",
      "Training Model  ...\n",
      "E loss:  0.7201680541038513\n",
      "G loss: 0.8137630224227905\n",
      "E loss:  0.7018677592277527\n",
      "G loss: 0.9024425148963928\n",
      "E loss:  0.6833328008651733\n",
      "G loss: 0.8801085352897644\n",
      "E loss:  0.6767915487289429\n",
      "G loss: 0.9578849673271179\n",
      "E loss:  0.674654483795166\n",
      "G loss: 1.0040967464447021\n",
      "Training Model  ...\n",
      "E loss:  0.7280842661857605\n",
      "G loss: 1.0492849349975586\n",
      "E loss:  0.7249647974967957\n",
      "G loss: 1.0536096096038818\n",
      "E loss:  0.7204760909080505\n",
      "G loss: 0.9853249192237854\n",
      "E loss:  0.7248384952545166\n",
      "G loss: 0.9184802770614624\n",
      "E loss:  0.7167962193489075\n",
      "G loss: 0.850313663482666\n",
      "Training Model  ...\n",
      "E loss:  0.6408804655075073\n",
      "G loss: 0.8565326929092407\n",
      "E loss:  0.6598219871520996\n",
      "G loss: 0.9307606816291809\n",
      "E loss:  0.6502446532249451\n",
      "G loss: 0.9258882999420166\n",
      "E loss:  0.661025881767273\n",
      "G loss: 0.9158438444137573\n",
      "E loss:  0.6574190258979797\n",
      "G loss: 0.9125927686691284\n",
      "Training Model  ...\n",
      "E loss:  0.6970447301864624\n",
      "G loss: 0.8504918813705444\n",
      "E loss:  0.7029828429222107\n",
      "G loss: 0.9037775993347168\n",
      "E loss:  0.7187877893447876\n",
      "G loss: 0.9088433980941772\n",
      "E loss:  0.717747151851654\n",
      "G loss: 0.9459788203239441\n",
      "E loss:  0.6989575624465942\n",
      "G loss: 0.9288994073867798\n",
      "Training Model  ...\n",
      "E loss:  0.6984418034553528\n",
      "G loss: 0.9417256116867065\n",
      "E loss:  0.6886852383613586\n",
      "G loss: 0.917489767074585\n",
      "E loss:  0.6740889549255371\n",
      "G loss: 0.852607250213623\n",
      "E loss:  0.675045907497406\n",
      "G loss: 0.7882946729660034\n",
      "E loss:  0.6640042662620544\n",
      "G loss: 0.7614801526069641\n",
      "Training Model  ...\n",
      "E loss:  0.7206151485443115\n",
      "G loss: 0.8393291234970093\n",
      "E loss:  0.7100543975830078\n",
      "G loss: 0.8384627103805542\n",
      "E loss:  0.7134413719177246\n",
      "G loss: 0.8098680377006531\n",
      "E loss:  0.7316012382507324\n",
      "G loss: 0.8276241421699524\n",
      "E loss:  0.7372985482215881\n",
      "G loss: 0.8367493152618408\n",
      "Training Model  ...\n",
      "E loss:  0.7945652008056641\n",
      "G loss: 0.8459217548370361\n",
      "E loss:  0.7956467270851135\n",
      "G loss: 0.8554851412773132\n",
      "E loss:  0.7816969752311707\n",
      "G loss: 0.8594225645065308\n",
      "E loss:  0.7757636904716492\n",
      "G loss: 0.934440016746521\n",
      "E loss:  0.7757000923156738\n",
      "G loss: 0.8909509778022766\n",
      "Training Model  ...\n",
      "E loss:  0.7338046431541443\n",
      "G loss: 0.874657154083252\n",
      "E loss:  0.7563815116882324\n",
      "G loss: 0.9165692329406738\n",
      "E loss:  0.7348765730857849\n",
      "G loss: 0.8376988172531128\n",
      "E loss:  0.7408251166343689\n",
      "G loss: 0.8375073671340942\n",
      "E loss:  0.7270171046257019\n",
      "G loss: 0.8025848865509033\n",
      "Training Model  ...\n",
      "E loss:  0.7535980343818665\n",
      "G loss: 0.7749050855636597\n",
      "E loss:  0.7479580640792847\n",
      "G loss: 0.7617853283882141\n",
      "E loss:  0.733873724937439\n",
      "G loss: 0.7771446108818054\n",
      "E loss:  0.7290748357772827\n",
      "G loss: 0.7622215151786804\n",
      "E loss:  0.7332530617713928\n",
      "G loss: 0.7695916295051575\n",
      "Training Model  ...\n",
      "E loss:  0.7363989949226379\n",
      "G loss: 0.7847296595573425\n",
      "E loss:  0.7451971173286438\n",
      "G loss: 0.7906768321990967\n",
      "E loss:  0.7299281358718872\n",
      "G loss: 0.8196088671684265\n",
      "E loss:  0.717978835105896\n",
      "G loss: 0.8596395254135132\n",
      "E loss:  0.7026735544204712\n",
      "G loss: 0.8777230381965637\n",
      "Training Model  ...\n",
      "E loss:  0.6512576341629028\n",
      "G loss: 0.8456467390060425\n",
      "E loss:  0.6409501433372498\n",
      "G loss: 0.891784131526947\n",
      "E loss:  0.6470739841461182\n",
      "G loss: 0.8552327752113342\n",
      "E loss:  0.6550604104995728\n",
      "G loss: 0.8520987033843994\n",
      "E loss:  0.6582908630371094\n",
      "G loss: 0.8213258385658264\n",
      "Training Model  ...\n",
      "E loss:  0.7359641790390015\n",
      "G loss: 0.8177438974380493\n",
      "E loss:  0.7277843952178955\n",
      "G loss: 0.828401505947113\n",
      "E loss:  0.7416639924049377\n",
      "G loss: 0.8765214681625366\n",
      "E loss:  0.7521253228187561\n",
      "G loss: 0.8265392780303955\n",
      "E loss:  0.751266360282898\n",
      "G loss: 0.8396422863006592\n",
      "Training Model  ...\n",
      "E loss:  0.6961294412612915\n",
      "G loss: 0.8296232223510742\n",
      "E loss:  0.6868595480918884\n",
      "G loss: 0.8404188752174377\n",
      "E loss:  0.6746528744697571\n",
      "G loss: 0.8258206248283386\n",
      "E loss:  0.6743476390838623\n",
      "G loss: 0.820711076259613\n",
      "E loss:  0.6736668348312378\n",
      "G loss: 0.8547982573509216\n",
      "Training Model  ...\n",
      "E loss:  0.6980257034301758\n",
      "G loss: 0.8318058252334595\n",
      "E loss:  0.712943434715271\n",
      "G loss: 0.8092502951622009\n",
      "E loss:  0.7228128910064697\n",
      "G loss: 0.7585439682006836\n",
      "E loss:  0.7196825742721558\n",
      "G loss: 0.7658912539482117\n",
      "E loss:  0.7239270806312561\n",
      "G loss: 0.7368090152740479\n",
      "Training Model  ...\n",
      "E loss:  0.6958814859390259\n",
      "G loss: 0.7202370762825012\n",
      "E loss:  0.7147982716560364\n",
      "G loss: 0.7372669577598572\n",
      "E loss:  0.7059530019760132\n",
      "G loss: 0.7661792635917664\n",
      "E loss:  0.6970282793045044\n",
      "G loss: 0.8217076063156128\n",
      "E loss:  0.6870949864387512\n",
      "G loss: 0.8150274157524109\n",
      "Training Model  ...\n",
      "E loss:  0.7436921000480652\n",
      "G loss: 0.7837207913398743\n",
      "E loss:  0.7413896918296814\n",
      "G loss: 0.8381999731063843\n",
      "E loss:  0.7373452186584473\n",
      "G loss: 0.8581658601760864\n",
      "E loss:  0.7291487455368042\n",
      "G loss: 0.817171573638916\n",
      "E loss:  0.7203122973442078\n",
      "G loss: 0.8304216265678406\n",
      "Training Model  ...\n",
      "E loss:  0.7281825542449951\n",
      "G loss: 0.8687847256660461\n",
      "E loss:  0.7418876886367798\n",
      "G loss: 0.8633002042770386\n",
      "E loss:  0.7233964800834656\n",
      "G loss: 0.8784887194633484\n",
      "E loss:  0.7315929532051086\n",
      "G loss: 0.8572689294815063\n",
      "E loss:  0.7293314933776855\n",
      "G loss: 0.9101590514183044\n",
      "Training Model  ...\n",
      "E loss:  0.6665533781051636\n",
      "G loss: 0.9273970127105713\n",
      "E loss:  0.6574166417121887\n",
      "G loss: 0.9540865421295166\n",
      "E loss:  0.6532274484634399\n",
      "G loss: 0.9478659629821777\n",
      "E loss:  0.6606317758560181\n",
      "G loss: 0.9091770052909851\n",
      "E loss:  0.6636060476303101\n",
      "G loss: 0.8719337582588196\n",
      "Training Model  ...\n",
      "E loss:  0.8002226948738098\n",
      "G loss: 0.9030045866966248\n",
      "E loss:  0.8062190413475037\n",
      "G loss: 0.938450813293457\n",
      "E loss:  0.8164061903953552\n",
      "G loss: 0.8852130174636841\n",
      "E loss:  0.8248099684715271\n",
      "G loss: 0.7996065616607666\n",
      "E loss:  0.8137741684913635\n",
      "G loss: 0.871388852596283\n",
      "Training Model  ...\n",
      "E loss:  0.7135609984397888\n",
      "G loss: 0.8636265397071838\n",
      "E loss:  0.7024118304252625\n",
      "G loss: 0.8240821361541748\n",
      "E loss:  0.7204300761222839\n",
      "G loss: 0.8193287253379822\n",
      "E loss:  0.7263290286064148\n",
      "G loss: 0.7606574296951294\n",
      "E loss:  0.7420875430107117\n",
      "G loss: 0.6738512516021729\n",
      "Training Model  ...\n",
      "E loss:  0.6889352798461914\n",
      "G loss: 0.7097749710083008\n",
      "E loss:  0.6950790882110596\n",
      "G loss: 0.7250998616218567\n",
      "E loss:  0.7123570442199707\n",
      "G loss: 0.7690753936767578\n",
      "E loss:  0.7003563642501831\n",
      "G loss: 0.801236629486084\n",
      "E loss:  0.684754490852356\n",
      "G loss: 0.8229026794433594\n",
      "Training Model  ...\n",
      "E loss:  0.6959893107414246\n",
      "G loss: 0.7843016386032104\n",
      "E loss:  0.6953954696655273\n",
      "G loss: 0.7831751704216003\n",
      "E loss:  0.6891319155693054\n",
      "G loss: 0.7514616847038269\n",
      "E loss:  0.6918479800224304\n",
      "G loss: 0.7840961217880249\n",
      "E loss:  0.6813462972640991\n",
      "G loss: 0.750399112701416\n",
      "Training Model  ...\n",
      "E loss:  0.7203595042228699\n",
      "G loss: 0.7742695808410645\n",
      "E loss:  0.7330812215805054\n",
      "G loss: 0.8025858402252197\n",
      "E loss:  0.7252874970436096\n",
      "G loss: 0.8088823556900024\n",
      "E loss:  0.7288108468055725\n",
      "G loss: 0.8120181560516357\n",
      "E loss:  0.7186723947525024\n",
      "G loss: 0.8285408020019531\n",
      "Training Model  ...\n",
      "E loss:  0.7453715205192566\n",
      "G loss: 0.8107390999794006\n",
      "E loss:  0.7453951835632324\n",
      "G loss: 0.820803165435791\n",
      "E loss:  0.7390182614326477\n",
      "G loss: 0.8846020698547363\n",
      "E loss:  0.7375450134277344\n",
      "G loss: 0.8918750286102295\n",
      "E loss:  0.7338665127754211\n",
      "G loss: 0.8810263276100159\n",
      "Training Model  ...\n",
      "E loss:  0.7856122255325317\n",
      "G loss: 0.8981657028198242\n",
      "E loss:  0.7871502041816711\n",
      "G loss: 0.8228470683097839\n",
      "E loss:  0.7832518815994263\n",
      "G loss: 0.8809939622879028\n",
      "E loss:  0.7888767719268799\n",
      "G loss: 0.8838254809379578\n",
      "E loss:  0.8163473606109619\n",
      "G loss: 0.8698994517326355\n",
      "Training Model  ...\n",
      "E loss:  0.6623289585113525\n",
      "G loss: 0.8540987968444824\n",
      "E loss:  0.6650212407112122\n",
      "G loss: 0.8344907760620117\n",
      "E loss:  0.6504442095756531\n",
      "G loss: 0.7669943571090698\n",
      "E loss:  0.6633830070495605\n",
      "G loss: 0.7646504640579224\n",
      "E loss:  0.6603953838348389\n",
      "G loss: 0.7282544374465942\n",
      "Training Model  ...\n",
      "E loss:  0.7738653421401978\n",
      "G loss: 0.7915667295455933\n",
      "E loss:  0.767973780632019\n",
      "G loss: 0.8260711431503296\n",
      "E loss:  0.7570725679397583\n",
      "G loss: 0.8391011953353882\n",
      "E loss:  0.7544763684272766\n",
      "G loss: 0.9143180847167969\n",
      "E loss:  0.7705184817314148\n",
      "G loss: 1.0224108695983887\n",
      "Training Model  ...\n",
      "E loss:  0.7916679382324219\n",
      "G loss: 0.9264858365058899\n",
      "E loss:  0.7782993316650391\n",
      "G loss: 0.93492591381073\n",
      "E loss:  0.7885134220123291\n",
      "G loss: 0.8362564444541931\n",
      "E loss:  0.7816747426986694\n",
      "G loss: 0.7733102440834045\n",
      "E loss:  0.785973846912384\n",
      "G loss: 0.8090596199035645\n",
      "Training Model  ...\n",
      "E loss:  0.7000636458396912\n",
      "G loss: 0.7809846997261047\n",
      "E loss:  0.7169215083122253\n",
      "G loss: 0.7853468656539917\n",
      "E loss:  0.709324836730957\n",
      "G loss: 0.74513179063797\n",
      "E loss:  0.6773319840431213\n",
      "G loss: 0.7416130900382996\n",
      "E loss:  0.6736961603164673\n",
      "G loss: 0.7359212040901184\n",
      "Training Model  ...\n",
      "E loss:  0.785563588142395\n",
      "G loss: 0.7446655035018921\n",
      "E loss:  0.7882792949676514\n",
      "G loss: 0.7249500155448914\n",
      "E loss:  0.7753093838691711\n",
      "G loss: 0.7183007597923279\n",
      "E loss:  0.7841362357139587\n",
      "G loss: 0.7203022837638855\n",
      "E loss:  0.7806143760681152\n",
      "G loss: 0.7222496271133423\n",
      "Training Model  ...\n",
      "E loss:  0.7327548265457153\n",
      "G loss: 0.7436171770095825\n",
      "E loss:  0.7469712495803833\n",
      "G loss: 0.7897430062294006\n",
      "E loss:  0.7318021655082703\n",
      "G loss: 0.7831241488456726\n",
      "E loss:  0.7467202544212341\n",
      "G loss: 0.8347249031066895\n",
      "E loss:  0.73280268907547\n",
      "G loss: 0.858016312122345\n",
      "Training Model  ...\n",
      "E loss:  0.6705600619316101\n",
      "G loss: 0.8524917960166931\n",
      "E loss:  0.6626071929931641\n",
      "G loss: 0.8679125905036926\n",
      "E loss:  0.6694404482841492\n",
      "G loss: 0.8286038637161255\n",
      "E loss:  0.6592618823051453\n",
      "G loss: 0.7792727947235107\n",
      "E loss:  0.6577627658843994\n",
      "G loss: 0.7946277856826782\n",
      "Training Model  ...\n",
      "E loss:  0.7487741708755493\n",
      "G loss: 0.7977342009544373\n",
      "E loss:  0.749051570892334\n",
      "G loss: 0.8096328377723694\n",
      "E loss:  0.7462072372436523\n",
      "G loss: 0.7839369177818298\n",
      "E loss:  0.7511172890663147\n",
      "G loss: 0.7861114740371704\n",
      "E loss:  0.7425521612167358\n",
      "G loss: 0.7719841599464417\n",
      "Training Model  ...\n",
      "E loss:  0.8035511374473572\n",
      "G loss: 0.7701389789581299\n",
      "E loss:  0.7828943729400635\n",
      "G loss: 0.8476770520210266\n",
      "E loss:  0.7683310508728027\n",
      "G loss: 0.8620299100875854\n",
      "E loss:  0.7574002742767334\n",
      "G loss: 0.9112353324890137\n",
      "E loss:  0.7812961339950562\n",
      "G loss: 0.9391961693763733\n",
      "Training Model  ...\n",
      "E loss:  0.7290608882904053\n",
      "G loss: 0.8840242028236389\n",
      "E loss:  0.7190402746200562\n",
      "G loss: 0.8744995594024658\n",
      "E loss:  0.7174254059791565\n",
      "G loss: 0.844302773475647\n",
      "E loss:  0.7253828048706055\n",
      "G loss: 0.7579520344734192\n",
      "E loss:  0.7281026840209961\n",
      "G loss: 0.8216504454612732\n",
      "Training Model  ...\n",
      "E loss:  0.777397632598877\n",
      "G loss: 0.7998651266098022\n",
      "E loss:  0.761656641960144\n",
      "G loss: 0.7694084644317627\n",
      "E loss:  0.7688449621200562\n",
      "G loss: 0.7709351181983948\n",
      "E loss:  0.7766158580780029\n",
      "G loss: 0.7774596214294434\n",
      "E loss:  0.7938967347145081\n",
      "G loss: 0.8016625642776489\n",
      "Training Model  ...\n",
      "E loss:  0.7238147258758545\n",
      "G loss: 0.8187903165817261\n",
      "E loss:  0.7294548153877258\n",
      "G loss: 0.7894205451011658\n",
      "E loss:  0.7228096127510071\n",
      "G loss: 0.817981481552124\n",
      "E loss:  0.7146931290626526\n",
      "G loss: 0.8367159366607666\n",
      "E loss:  0.7083152532577515\n",
      "G loss: 0.9437528252601624\n",
      "Training Model  ...\n",
      "E loss:  0.6667616367340088\n",
      "G loss: 0.8700416684150696\n",
      "E loss:  0.673957347869873\n",
      "G loss: 0.8501142859458923\n",
      "E loss:  0.6740173697471619\n",
      "G loss: 0.8294265270233154\n",
      "E loss:  0.6787412762641907\n",
      "G loss: 0.8442819118499756\n",
      "E loss:  0.6802073121070862\n",
      "G loss: 0.8495677709579468\n",
      "Training Model  ...\n",
      "E loss:  0.6607168316841125\n",
      "G loss: 0.755151629447937\n",
      "E loss:  0.6753811836242676\n",
      "G loss: 0.7680559754371643\n",
      "E loss:  0.6703562140464783\n",
      "G loss: 0.7304098606109619\n",
      "E loss:  0.6545847654342651\n",
      "G loss: 0.7115666270256042\n",
      "E loss:  0.661030113697052\n",
      "G loss: 0.7054576873779297\n",
      "Training Model  ...\n",
      "E loss:  0.6880560517311096\n",
      "G loss: 0.7044422626495361\n",
      "E loss:  0.6619452834129333\n",
      "G loss: 0.7051249146461487\n",
      "E loss:  0.6600803732872009\n",
      "G loss: 0.7425370812416077\n",
      "E loss:  0.6625491380691528\n",
      "G loss: 0.7281254529953003\n",
      "E loss:  0.6594088673591614\n",
      "G loss: 0.7904442548751831\n",
      "Training Model  ...\n",
      "E loss:  0.6862664222717285\n",
      "G loss: 0.7492011785507202\n",
      "E loss:  0.698650598526001\n",
      "G loss: 0.8055989742279053\n",
      "E loss:  0.6987061500549316\n",
      "G loss: 0.7897793054580688\n",
      "E loss:  0.6987830996513367\n",
      "G loss: 0.8070615530014038\n",
      "E loss:  0.6981611847877502\n",
      "G loss: 0.8059883713722229\n",
      "Training Model  ...\n",
      "E loss:  0.7032970786094666\n",
      "G loss: 0.7868999242782593\n",
      "E loss:  0.6997098922729492\n",
      "G loss: 0.7454873919487\n",
      "E loss:  0.6878297924995422\n",
      "G loss: 0.788601815700531\n",
      "E loss:  0.6921793818473816\n",
      "G loss: 0.7589220404624939\n",
      "E loss:  0.6905263662338257\n",
      "G loss: 0.7495351433753967\n",
      "Training Model  ...\n",
      "E loss:  0.7140114307403564\n",
      "G loss: 0.7223522663116455\n",
      "E loss:  0.7208887934684753\n",
      "G loss: 0.732347309589386\n",
      "E loss:  0.7130716443061829\n",
      "G loss: 0.7488234043121338\n",
      "E loss:  0.6982169151306152\n",
      "G loss: 0.7384219169616699\n",
      "E loss:  0.7074760794639587\n",
      "G loss: 0.7646738290786743\n",
      "Training Model  ...\n",
      "E loss:  0.7497236728668213\n",
      "G loss: 0.7558267116546631\n",
      "E loss:  0.7461092472076416\n",
      "G loss: 0.7641086578369141\n",
      "E loss:  0.7500397562980652\n",
      "G loss: 0.7545855641365051\n",
      "E loss:  0.7310848832130432\n",
      "G loss: 0.7946599125862122\n",
      "E loss:  0.7256588935852051\n",
      "G loss: 0.850580632686615\n",
      "Training Model  ...\n",
      "E loss:  0.6649394035339355\n",
      "G loss: 0.8058075308799744\n",
      "E loss:  0.6524055004119873\n",
      "G loss: 0.8108579516410828\n",
      "E loss:  0.6460665464401245\n",
      "G loss: 0.8304811120033264\n",
      "E loss:  0.6484605073928833\n",
      "G loss: 0.7894339561462402\n",
      "E loss:  0.6403269171714783\n",
      "G loss: 0.743495523929596\n",
      "Training Model  ...\n",
      "E loss:  0.7691584825515747\n",
      "G loss: 0.7780725955963135\n",
      "E loss:  0.7621333003044128\n",
      "G loss: 0.8194525241851807\n",
      "E loss:  0.7606769800186157\n",
      "G loss: 0.802370548248291\n",
      "E loss:  0.7633628249168396\n",
      "G loss: 0.7384361028671265\n",
      "E loss:  0.7673494219779968\n",
      "G loss: 0.8125142455101013\n",
      "Training Model  ...\n",
      "E loss:  0.6847878098487854\n",
      "G loss: 0.7433637380599976\n",
      "E loss:  0.682613730430603\n",
      "G loss: 0.7270681858062744\n",
      "E loss:  0.6861227750778198\n",
      "G loss: 0.760335385799408\n",
      "E loss:  0.6867828369140625\n",
      "G loss: 0.6950530409812927\n",
      "E loss:  0.6948720216751099\n",
      "G loss: 0.6850555539131165\n",
      "Training Model  ...\n",
      "E loss:  0.7598759531974792\n",
      "G loss: 0.7471365928649902\n",
      "E loss:  0.7509488463401794\n",
      "G loss: 0.6990761756896973\n",
      "E loss:  0.7486898899078369\n",
      "G loss: 0.7351190447807312\n",
      "E loss:  0.7366957664489746\n",
      "G loss: 0.7272425889968872\n",
      "E loss:  0.7291349768638611\n",
      "G loss: 0.7817538976669312\n",
      "Training Model  ...\n",
      "E loss:  0.762638509273529\n",
      "G loss: 0.7820961475372314\n",
      "E loss:  0.769946277141571\n",
      "G loss: 0.7279355525970459\n",
      "E loss:  0.7860945463180542\n",
      "G loss: 0.7391135096549988\n",
      "E loss:  0.791114330291748\n",
      "G loss: 0.7097612023353577\n",
      "E loss:  0.79222571849823\n",
      "G loss: 0.6917370557785034\n",
      "Training Model  ...\n",
      "E loss:  0.6886813640594482\n",
      "G loss: 0.742651104927063\n",
      "E loss:  0.694286584854126\n",
      "G loss: 0.7514345645904541\n",
      "E loss:  0.6896907091140747\n",
      "G loss: 0.701736330986023\n",
      "E loss:  0.6915278434753418\n",
      "G loss: 0.7023360133171082\n",
      "E loss:  0.693259596824646\n",
      "G loss: 0.7045192122459412\n",
      "Training Model  ...\n",
      "E loss:  0.6966838836669922\n",
      "G loss: 0.7351280450820923\n",
      "E loss:  0.7015534043312073\n",
      "G loss: 0.709187388420105\n",
      "E loss:  0.7066238522529602\n",
      "G loss: 0.805168867111206\n",
      "E loss:  0.6969862580299377\n",
      "G loss: 0.7444262504577637\n",
      "E loss:  0.6974056959152222\n",
      "G loss: 0.7798374891281128\n",
      "Training Model  ...\n",
      "E loss:  0.7073715925216675\n",
      "G loss: 0.7153867483139038\n",
      "E loss:  0.7081322073936462\n",
      "G loss: 0.7570568323135376\n",
      "E loss:  0.7158502340316772\n",
      "G loss: 0.7758536338806152\n",
      "E loss:  0.7193306088447571\n",
      "G loss: 0.7370539307594299\n",
      "E loss:  0.736897349357605\n",
      "G loss: 0.7539234161376953\n",
      "Training Model  ...\n",
      "E loss:  0.6458773612976074\n",
      "G loss: 0.8055273294448853\n",
      "E loss:  0.6515983939170837\n",
      "G loss: 0.7231413722038269\n",
      "E loss:  0.6406316757202148\n",
      "G loss: 0.778082013130188\n",
      "E loss:  0.6659422516822815\n",
      "G loss: 0.7586240172386169\n",
      "E loss:  0.6757512092590332\n",
      "G loss: 0.7877172827720642\n",
      "Training Model  ...\n",
      "E loss:  0.749224066734314\n",
      "G loss: 0.809810996055603\n",
      "E loss:  0.7400177717208862\n",
      "G loss: 0.7603732943534851\n",
      "E loss:  0.7599155306816101\n",
      "G loss: 0.8270925283432007\n",
      "E loss:  0.7532621622085571\n",
      "G loss: 0.786872386932373\n",
      "E loss:  0.7733551263809204\n",
      "G loss: 0.7817146182060242\n",
      "Training Model  ...\n",
      "E loss:  0.7198722958564758\n",
      "G loss: 0.7631191611289978\n",
      "E loss:  0.7134658098220825\n",
      "G loss: 0.7513328790664673\n",
      "E loss:  0.695069432258606\n",
      "G loss: 0.7308463454246521\n",
      "E loss:  0.6881623268127441\n",
      "G loss: 0.705282986164093\n",
      "E loss:  0.6888440847396851\n",
      "G loss: 0.6800476908683777\n",
      "Training Model  ...\n",
      "E loss:  0.6947302222251892\n",
      "G loss: 0.7238712310791016\n",
      "E loss:  0.6916778087615967\n",
      "G loss: 0.6778392791748047\n",
      "E loss:  0.68702232837677\n",
      "G loss: 0.6935081481933594\n",
      "E loss:  0.6703516244888306\n",
      "G loss: 0.6248837113380432\n",
      "E loss:  0.6695387363433838\n",
      "G loss: 0.6324229836463928\n",
      "Training Model  ...\n",
      "E loss:  0.6124803423881531\n",
      "G loss: 0.680286169052124\n",
      "E loss:  0.6172176003456116\n",
      "G loss: 0.6577264070510864\n",
      "E loss:  0.6112788915634155\n",
      "G loss: 0.6446442604064941\n",
      "E loss:  0.614425539970398\n",
      "G loss: 0.6118714809417725\n",
      "E loss:  0.610000729560852\n",
      "G loss: 0.6041647791862488\n",
      "Training Model  ...\n",
      "E loss:  0.7083093523979187\n",
      "G loss: 0.6665880680084229\n",
      "E loss:  0.6996573805809021\n",
      "G loss: 0.6770261526107788\n",
      "E loss:  0.7073280811309814\n",
      "G loss: 0.6883811950683594\n",
      "E loss:  0.6875699162483215\n",
      "G loss: 0.7168950438499451\n",
      "E loss:  0.6815855503082275\n",
      "G loss: 0.6917330622673035\n",
      "Training Model  ...\n",
      "E loss:  0.7037516236305237\n",
      "G loss: 0.731146514415741\n",
      "E loss:  0.7081823348999023\n",
      "G loss: 0.7274279594421387\n",
      "E loss:  0.7091116309165955\n",
      "G loss: 0.7839828133583069\n",
      "E loss:  0.7267180681228638\n",
      "G loss: 0.767046332359314\n",
      "E loss:  0.7418029308319092\n",
      "G loss: 0.7634286880493164\n",
      "Training Model  ...\n",
      "E loss:  0.7243595719337463\n",
      "G loss: 0.8323609828948975\n",
      "E loss:  0.7288824319839478\n",
      "G loss: 0.7355591654777527\n",
      "E loss:  0.7357816100120544\n",
      "G loss: 0.7919042110443115\n",
      "E loss:  0.7118105888366699\n",
      "G loss: 0.7634138464927673\n",
      "E loss:  0.7080923318862915\n",
      "G loss: 0.730736494064331\n",
      "Training Model  ...\n",
      "E loss:  0.7408349514007568\n",
      "G loss: 0.7166712284088135\n",
      "E loss:  0.743209183216095\n",
      "G loss: 0.7109914422035217\n",
      "E loss:  0.7534743547439575\n",
      "G loss: 0.6858046650886536\n",
      "E loss:  0.75252366065979\n",
      "G loss: 0.729546844959259\n",
      "E loss:  0.7594448328018188\n",
      "G loss: 0.7033638954162598\n",
      "Training Model  ...\n",
      "E loss:  0.7280306220054626\n",
      "G loss: 0.7621638774871826\n",
      "E loss:  0.7316004037857056\n",
      "G loss: 0.6871102452278137\n",
      "E loss:  0.7301204204559326\n",
      "G loss: 0.7177385091781616\n",
      "E loss:  0.7289678454399109\n",
      "G loss: 0.7474187016487122\n",
      "E loss:  0.7342485189437866\n",
      "G loss: 0.7158442139625549\n",
      "Training Model  ...\n",
      "E loss:  0.6792486310005188\n",
      "G loss: 0.6967805624008179\n",
      "E loss:  0.6920617818832397\n",
      "G loss: 0.6933088302612305\n",
      "E loss:  0.7073914408683777\n",
      "G loss: 0.653414249420166\n",
      "E loss:  0.7167329788208008\n",
      "G loss: 0.6458911299705505\n",
      "E loss:  0.7186732888221741\n",
      "G loss: 0.6311336159706116\n",
      "Training Model  ...\n",
      "E loss:  0.6728346347808838\n",
      "G loss: 0.6535772681236267\n",
      "E loss:  0.6800062656402588\n",
      "G loss: 0.6514998078346252\n",
      "E loss:  0.671440839767456\n",
      "G loss: 0.7061490416526794\n",
      "E loss:  0.6654309034347534\n",
      "G loss: 0.6807554960250854\n",
      "E loss:  0.6670568585395813\n",
      "G loss: 0.7149936556816101\n",
      "Training Model  ...\n",
      "E loss:  0.7532031536102295\n",
      "G loss: 0.7310664057731628\n",
      "E loss:  0.7951676249504089\n",
      "G loss: 0.7310723066329956\n",
      "E loss:  0.7801917195320129\n",
      "G loss: 0.7335389852523804\n",
      "E loss:  0.7831128835678101\n",
      "G loss: 0.7209243774414062\n",
      "E loss:  0.7819277048110962\n",
      "G loss: 0.73128741979599\n",
      "Training Model  ...\n",
      "E loss:  0.7685240507125854\n",
      "G loss: 0.712547779083252\n",
      "E loss:  0.7623655200004578\n",
      "G loss: 0.7367850542068481\n",
      "E loss:  0.7631853222846985\n",
      "G loss: 0.684877872467041\n",
      "E loss:  0.765587329864502\n",
      "G loss: 0.7183012962341309\n",
      "E loss:  0.7638716697692871\n",
      "G loss: 0.6891125440597534\n",
      "Training Model  ...\n",
      "E loss:  0.6426575183868408\n",
      "G loss: 0.6928704977035522\n",
      "E loss:  0.648885190486908\n",
      "G loss: 0.7120763659477234\n",
      "E loss:  0.6497781872749329\n",
      "G loss: 0.6986216902732849\n",
      "E loss:  0.639075517654419\n",
      "G loss: 0.6816542744636536\n",
      "E loss:  0.6434003710746765\n",
      "G loss: 0.7048951387405396\n",
      "Training Model  ...\n",
      "E loss:  0.7335487604141235\n",
      "G loss: 0.6628933548927307\n",
      "E loss:  0.7305383682250977\n",
      "G loss: 0.678311288356781\n",
      "E loss:  0.7273056507110596\n",
      "G loss: 0.6562535166740417\n",
      "E loss:  0.7225149273872375\n",
      "G loss: 0.6744146347045898\n",
      "E loss:  0.7154139280319214\n",
      "G loss: 0.6527618169784546\n",
      "Training Model  ...\n",
      "E loss:  0.7438616752624512\n",
      "G loss: 0.6522002220153809\n",
      "E loss:  0.7353522777557373\n",
      "G loss: 0.7318087220191956\n",
      "E loss:  0.7447236776351929\n",
      "G loss: 0.6607344746589661\n",
      "E loss:  0.7550761103630066\n",
      "G loss: 0.642711877822876\n",
      "E loss:  0.7411676049232483\n",
      "G loss: 0.7065595984458923\n",
      "Training Model  ...\n",
      "E loss:  0.688696026802063\n",
      "G loss: 0.7028766870498657\n",
      "E loss:  0.6762053370475769\n",
      "G loss: 0.7217509746551514\n",
      "E loss:  0.669670820236206\n",
      "G loss: 0.7697491645812988\n",
      "E loss:  0.6858699321746826\n",
      "G loss: 0.7550704479217529\n",
      "E loss:  0.6792250871658325\n",
      "G loss: 0.7498577833175659\n",
      "Training Model  ...\n",
      "E loss:  0.790473461151123\n",
      "G loss: 0.7666149139404297\n",
      "E loss:  0.799947202205658\n",
      "G loss: 0.72743821144104\n",
      "E loss:  0.8022472262382507\n",
      "G loss: 0.6500484347343445\n",
      "E loss:  0.8002992272377014\n",
      "G loss: 0.6061893701553345\n",
      "E loss:  0.7913651466369629\n",
      "G loss: 0.5644075274467468\n",
      "Training Model  ...\n",
      "E loss:  0.6582598686218262\n",
      "G loss: 0.5730507969856262\n",
      "E loss:  0.6689980626106262\n",
      "G loss: 0.577424943447113\n",
      "E loss:  0.653749942779541\n",
      "G loss: 0.6379820704460144\n",
      "E loss:  0.6465387344360352\n",
      "G loss: 0.6779484748840332\n",
      "E loss:  0.6514759063720703\n",
      "G loss: 0.688403308391571\n",
      "Training Model  ...\n",
      "E loss:  0.6492704749107361\n",
      "G loss: 0.7100318670272827\n",
      "E loss:  0.6582980155944824\n",
      "G loss: 0.6742109656333923\n",
      "E loss:  0.6540600657463074\n",
      "G loss: 0.6698116660118103\n",
      "E loss:  0.6463298797607422\n",
      "G loss: 0.6380797028541565\n",
      "E loss:  0.6601695418357849\n",
      "G loss: 0.64679354429245\n",
      "Training Model  ...\n",
      "E loss:  0.7534410357475281\n",
      "G loss: 0.6066449284553528\n",
      "E loss:  0.7582525610923767\n",
      "G loss: 0.6256637573242188\n",
      "E loss:  0.7499833106994629\n",
      "G loss: 0.6365520358085632\n",
      "E loss:  0.7449035048484802\n",
      "G loss: 0.6279819011688232\n",
      "E loss:  0.7356029748916626\n",
      "G loss: 0.6550025343894958\n",
      "Training Model  ...\n",
      "E loss:  0.7376425862312317\n",
      "G loss: 0.6556728482246399\n",
      "E loss:  0.7257946133613586\n",
      "G loss: 0.6879155039787292\n",
      "E loss:  0.7440721988677979\n",
      "G loss: 0.7645047903060913\n",
      "E loss:  0.7538634538650513\n",
      "G loss: 0.7006077766418457\n",
      "E loss:  0.7581808567047119\n",
      "G loss: 0.7276906967163086\n",
      "Training Model  ...\n",
      "E loss:  0.65901118516922\n",
      "G loss: 0.7388572692871094\n",
      "E loss:  0.6655101776123047\n",
      "G loss: 0.717024028301239\n",
      "E loss:  0.6639258861541748\n",
      "G loss: 0.650973379611969\n",
      "E loss:  0.6612045168876648\n",
      "G loss: 0.573031485080719\n",
      "E loss:  0.6587197780609131\n",
      "G loss: 0.541117250919342\n",
      "Training Model  ...\n",
      "E loss:  0.6454911828041077\n",
      "G loss: 0.5248967409133911\n",
      "E loss:  0.6504212021827698\n",
      "G loss: 0.5833690762519836\n",
      "E loss:  0.6561231017112732\n",
      "G loss: 0.5939906239509583\n",
      "E loss:  0.646848201751709\n",
      "G loss: 0.6718495488166809\n",
      "E loss:  0.6427963972091675\n",
      "G loss: 0.6898511648178101\n",
      "Training Model  ...\n",
      "E loss:  0.683763861656189\n",
      "G loss: 0.7199670672416687\n",
      "E loss:  0.7001834511756897\n",
      "G loss: 0.7067691087722778\n",
      "E loss:  0.6949548125267029\n",
      "G loss: 0.7168278694152832\n",
      "E loss:  0.6803863048553467\n",
      "G loss: 0.729181170463562\n",
      "E loss:  0.6765281558036804\n",
      "G loss: 0.7207070589065552\n",
      "Training Model  ...\n",
      "E loss:  0.6639745235443115\n",
      "G loss: 0.7108061909675598\n",
      "E loss:  0.680838942527771\n",
      "G loss: 0.6965177655220032\n",
      "E loss:  0.6799127459526062\n",
      "G loss: 0.7354913949966431\n",
      "E loss:  0.668834388256073\n",
      "G loss: 0.6736242175102234\n",
      "E loss:  0.6678125858306885\n",
      "G loss: 0.6872722506523132\n",
      "Training Model  ...\n",
      "E loss:  0.7569518089294434\n",
      "G loss: 0.701227605342865\n",
      "E loss:  0.7203593254089355\n",
      "G loss: 0.6866042017936707\n",
      "E loss:  0.7258875370025635\n",
      "G loss: 0.7211941480636597\n",
      "E loss:  0.7374169826507568\n",
      "G loss: 0.653046190738678\n",
      "E loss:  0.740852952003479\n",
      "G loss: 0.6733326315879822\n",
      "Training Model  ...\n",
      "E loss:  0.7333540916442871\n",
      "G loss: 0.7593525052070618\n",
      "E loss:  0.736289381980896\n",
      "G loss: 0.6630842685699463\n",
      "E loss:  0.7279455661773682\n",
      "G loss: 0.6845322847366333\n",
      "E loss:  0.7365893125534058\n",
      "G loss: 0.7071738243103027\n",
      "E loss:  0.7420042753219604\n",
      "G loss: 0.6704058647155762\n",
      "Training Model  ...\n",
      "E loss:  0.704547107219696\n",
      "G loss: 0.7222115397453308\n",
      "E loss:  0.6835082769393921\n",
      "G loss: 0.6939672231674194\n",
      "E loss:  0.6771718263626099\n",
      "G loss: 0.6886913180351257\n",
      "E loss:  0.6885416507720947\n",
      "G loss: 0.7412735819816589\n",
      "E loss:  0.6895611882209778\n",
      "G loss: 0.7040171027183533\n",
      "Training Model  ...\n",
      "E loss:  0.693650484085083\n",
      "G loss: 0.7007254362106323\n",
      "E loss:  0.6936096549034119\n",
      "G loss: 0.704375684261322\n",
      "E loss:  0.6801301836967468\n",
      "G loss: 0.6419336199760437\n",
      "E loss:  0.6901601552963257\n",
      "G loss: 0.6296001672744751\n",
      "E loss:  0.7020460963249207\n",
      "G loss: 0.6199914216995239\n",
      "Training Model  ...\n",
      "E loss:  0.6336213946342468\n",
      "G loss: 0.6234737634658813\n",
      "E loss:  0.6400681138038635\n",
      "G loss: 0.6267730593681335\n",
      "E loss:  0.6346749663352966\n",
      "G loss: 0.610158383846283\n",
      "E loss:  0.6218958497047424\n",
      "G loss: 0.6294986009597778\n",
      "E loss:  0.6241081357002258\n",
      "G loss: 0.6568746566772461\n",
      "Training Model  ...\n",
      "E loss:  0.7491872310638428\n",
      "G loss: 0.6428014039993286\n",
      "E loss:  0.7326221466064453\n",
      "G loss: 0.6496723890304565\n",
      "E loss:  0.7251443862915039\n",
      "G loss: 0.6548529267311096\n",
      "E loss:  0.7283788919448853\n",
      "G loss: 0.6430211067199707\n",
      "E loss:  0.7295629978179932\n",
      "G loss: 0.7337520718574524\n",
      "Training Model  ...\n",
      "E loss:  0.6334936618804932\n",
      "G loss: 0.6779100894927979\n",
      "E loss:  0.6451637744903564\n",
      "G loss: 0.7240992784500122\n",
      "E loss:  0.6430991888046265\n",
      "G loss: 0.7445133924484253\n",
      "E loss:  0.6300287246704102\n",
      "G loss: 0.7501505017280579\n",
      "E loss:  0.6546880006790161\n",
      "G loss: 0.7960423231124878\n",
      "Training Model  ...\n",
      "E loss:  0.7403411269187927\n",
      "G loss: 0.73923259973526\n",
      "E loss:  0.7567276358604431\n",
      "G loss: 0.7335858941078186\n",
      "E loss:  0.7497747540473938\n",
      "G loss: 0.6822363138198853\n",
      "E loss:  0.7333641052246094\n",
      "G loss: 0.6561381220817566\n",
      "E loss:  0.7317224144935608\n",
      "G loss: 0.6440606117248535\n",
      "Training Model  ...\n",
      "E loss:  0.7516802549362183\n",
      "G loss: 0.6139112710952759\n",
      "E loss:  0.7434804439544678\n",
      "G loss: 0.6563959717750549\n",
      "E loss:  0.7322210073471069\n",
      "G loss: 0.7365161180496216\n",
      "E loss:  0.7187792062759399\n",
      "G loss: 0.7879464030265808\n",
      "E loss:  0.6989705562591553\n",
      "G loss: 0.8593156933784485\n",
      "Training Model  ...\n",
      "E loss:  0.662696123123169\n",
      "G loss: 0.856916069984436\n",
      "E loss:  0.6536875367164612\n",
      "G loss: 0.7584817409515381\n",
      "E loss:  0.6524666547775269\n",
      "G loss: 0.7033761739730835\n",
      "E loss:  0.6542815566062927\n",
      "G loss: 0.6538202166557312\n",
      "E loss:  0.6307546496391296\n",
      "G loss: 0.5762489438056946\n",
      "Training Model  ...\n",
      "E loss:  0.74135822057724\n",
      "G loss: 0.6135745644569397\n",
      "E loss:  0.7446802258491516\n",
      "G loss: 0.564897358417511\n",
      "E loss:  0.73337721824646\n",
      "G loss: 0.5905097723007202\n",
      "E loss:  0.7554647922515869\n",
      "G loss: 0.5703098177909851\n",
      "E loss:  0.7405060529708862\n",
      "G loss: 0.5433164834976196\n",
      "Training Model  ...\n",
      "E loss:  0.7075397968292236\n",
      "G loss: 0.6108527183532715\n",
      "E loss:  0.6993332505226135\n",
      "G loss: 0.5934320688247681\n",
      "E loss:  0.696304976940155\n",
      "G loss: 0.6358635425567627\n",
      "E loss:  0.6871545314788818\n",
      "G loss: 0.6928331255912781\n",
      "E loss:  0.6854137778282166\n",
      "G loss: 0.6886722445487976\n",
      "Training Model  ...\n",
      "E loss:  0.6909599304199219\n",
      "G loss: 0.6946704387664795\n",
      "E loss:  0.681597888469696\n",
      "G loss: 0.6809839010238647\n",
      "E loss:  0.6829713582992554\n",
      "G loss: 0.6342707276344299\n",
      "E loss:  0.6801137328147888\n",
      "G loss: 0.6729486584663391\n",
      "E loss:  0.6778655052185059\n",
      "G loss: 0.6609799861907959\n",
      "Training Model  ...\n",
      "E loss:  0.6813496947288513\n",
      "G loss: 0.6732277274131775\n",
      "E loss:  0.6706824898719788\n",
      "G loss: 0.5959396362304688\n",
      "E loss:  0.6736708879470825\n",
      "G loss: 0.6586909294128418\n",
      "E loss:  0.6664601564407349\n",
      "G loss: 0.6667917966842651\n",
      "E loss:  0.6602546572685242\n",
      "G loss: 0.653813898563385\n",
      "Training Model  ...\n",
      "E loss:  0.697761058807373\n",
      "G loss: 0.6659844517707825\n",
      "E loss:  0.7062289714813232\n",
      "G loss: 0.6456651091575623\n",
      "E loss:  0.7301322221755981\n",
      "G loss: 0.647286593914032\n",
      "E loss:  0.7194089293479919\n",
      "G loss: 0.6629847884178162\n",
      "E loss:  0.733590841293335\n",
      "G loss: 0.5941447615623474\n",
      "Training Model  ...\n",
      "E loss:  0.6864694952964783\n",
      "G loss: 0.6377099752426147\n",
      "E loss:  0.6756988763809204\n",
      "G loss: 0.6735405921936035\n",
      "E loss:  0.6865062117576599\n",
      "G loss: 0.6486140489578247\n",
      "E loss:  0.7097668051719666\n",
      "G loss: 0.6242608428001404\n",
      "E loss:  0.7070186138153076\n",
      "G loss: 0.6080785393714905\n",
      "Training Model  ...\n",
      "E loss:  0.7032068371772766\n",
      "G loss: 0.6338961124420166\n",
      "E loss:  0.7264004945755005\n",
      "G loss: 0.6461780071258545\n",
      "E loss:  0.7252349853515625\n",
      "G loss: 0.6387246251106262\n",
      "E loss:  0.7124799489974976\n",
      "G loss: 0.6188796162605286\n",
      "E loss:  0.7220484614372253\n",
      "G loss: 0.6557143926620483\n",
      "Training Model  ...\n",
      "E loss:  0.6168931126594543\n",
      "G loss: 0.6602261066436768\n",
      "E loss:  0.6297588348388672\n",
      "G loss: 0.6670880317687988\n",
      "E loss:  0.6334825158119202\n",
      "G loss: 0.6306928396224976\n",
      "E loss:  0.6493123769760132\n",
      "G loss: 0.6426858305931091\n",
      "E loss:  0.6431463360786438\n",
      "G loss: 0.6599471569061279\n",
      "Training Model  ...\n",
      "E loss:  0.6610311269760132\n",
      "G loss: 0.6526100635528564\n",
      "E loss:  0.655119776725769\n",
      "G loss: 0.6412159204483032\n",
      "E loss:  0.6492811441421509\n",
      "G loss: 0.6235694289207458\n",
      "E loss:  0.6593483686447144\n",
      "G loss: 0.6052809357643127\n",
      "E loss:  0.6466846466064453\n",
      "G loss: 0.6145504713058472\n",
      "Training Model  ...\n",
      "E loss:  0.6162539124488831\n",
      "G loss: 0.6362730860710144\n",
      "E loss:  0.6159106492996216\n",
      "G loss: 0.5703215003013611\n",
      "E loss:  0.6242918968200684\n",
      "G loss: 0.573007345199585\n",
      "E loss:  0.6231611967086792\n",
      "G loss: 0.5644999146461487\n",
      "E loss:  0.6275572776794434\n",
      "G loss: 0.5982893705368042\n",
      "Training Model  ...\n",
      "E loss:  0.700038492679596\n",
      "G loss: 0.5915699601173401\n",
      "E loss:  0.7016047239303589\n",
      "G loss: 0.5428754091262817\n",
      "E loss:  0.6976361870765686\n",
      "G loss: 0.5737425684928894\n",
      "E loss:  0.6955209970474243\n",
      "G loss: 0.5648349523544312\n",
      "E loss:  0.6958199143409729\n",
      "G loss: 0.5790231227874756\n",
      "Training Model  ...\n",
      "E loss:  0.7299379706382751\n",
      "G loss: 0.6000484228134155\n",
      "E loss:  0.7347041368484497\n",
      "G loss: 0.5937432050704956\n",
      "E loss:  0.7392478585243225\n",
      "G loss: 0.6315805912017822\n",
      "E loss:  0.7506461143493652\n",
      "G loss: 0.6339793801307678\n",
      "E loss:  0.7500768899917603\n",
      "G loss: 0.6585888862609863\n",
      "Training Model  ...\n",
      "E loss:  0.630834698677063\n",
      "G loss: 0.6599999666213989\n",
      "E loss:  0.6312254071235657\n",
      "G loss: 0.6675419807434082\n",
      "E loss:  0.6332678198814392\n",
      "G loss: 0.6503351330757141\n",
      "E loss:  0.619303822517395\n",
      "G loss: 0.607032299041748\n",
      "E loss:  0.6396687030792236\n",
      "G loss: 0.5719996690750122\n",
      "Training Model  ...\n",
      "E loss:  0.7320647239685059\n",
      "G loss: 0.5956411361694336\n",
      "E loss:  0.7306233048439026\n",
      "G loss: 0.59731525182724\n",
      "E loss:  0.7282043695449829\n",
      "G loss: 0.6312961578369141\n",
      "E loss:  0.7277638912200928\n",
      "G loss: 0.5912847518920898\n",
      "E loss:  0.7305210828781128\n",
      "G loss: 0.559097945690155\n",
      "Training Model  ...\n",
      "E loss:  0.6182861328125\n",
      "G loss: 0.6298307776451111\n",
      "E loss:  0.6136355400085449\n",
      "G loss: 0.633693516254425\n",
      "E loss:  0.6197293400764465\n",
      "G loss: 0.5932652950286865\n",
      "E loss:  0.6232428550720215\n",
      "G loss: 0.5474769473075867\n",
      "E loss:  0.6260874271392822\n",
      "G loss: 0.6092914342880249\n",
      "Training Model  ...\n",
      "E loss:  0.72624272108078\n",
      "G loss: 0.6036326885223389\n",
      "E loss:  0.7120187282562256\n",
      "G loss: 0.5710896849632263\n",
      "E loss:  0.7106482982635498\n",
      "G loss: 0.6011390686035156\n",
      "E loss:  0.6999303698539734\n",
      "G loss: 0.6413705945014954\n",
      "E loss:  0.692744255065918\n",
      "G loss: 0.6632811427116394\n",
      "Training Model  ...\n",
      "E loss:  0.7309406399726868\n",
      "G loss: 0.7129747867584229\n",
      "E loss:  0.7160636186599731\n",
      "G loss: 0.6558429598808289\n",
      "E loss:  0.7151051759719849\n",
      "G loss: 0.602764368057251\n",
      "E loss:  0.7043143510818481\n",
      "G loss: 0.5956820845603943\n",
      "E loss:  0.7097415328025818\n",
      "G loss: 0.5849502682685852\n",
      "Training Model  ...\n",
      "E loss:  0.6833484172821045\n",
      "G loss: 0.5802558064460754\n",
      "E loss:  0.6768147349357605\n",
      "G loss: 0.594214141368866\n",
      "E loss:  0.6989049315452576\n",
      "G loss: 0.6057634353637695\n",
      "E loss:  0.6967070698738098\n",
      "G loss: 0.5861592292785645\n",
      "E loss:  0.6735187768936157\n",
      "G loss: 0.6440946459770203\n",
      "Training Model  ...\n",
      "E loss:  0.6825758218765259\n",
      "G loss: 0.6257290840148926\n",
      "E loss:  0.6722984313964844\n",
      "G loss: 0.6046195030212402\n",
      "E loss:  0.6756535768508911\n",
      "G loss: 0.5566025972366333\n",
      "E loss:  0.6579443216323853\n",
      "G loss: 0.5949001312255859\n",
      "E loss:  0.6752064228057861\n",
      "G loss: 0.5447664856910706\n",
      "Training Model  ...\n",
      "E loss:  0.7037085890769958\n",
      "G loss: 0.5298510193824768\n",
      "E loss:  0.7130386233329773\n",
      "G loss: 0.5113770961761475\n",
      "E loss:  0.7196603417396545\n",
      "G loss: 0.4966456890106201\n",
      "E loss:  0.7074642777442932\n",
      "G loss: 0.4616159200668335\n",
      "E loss:  0.7253230810165405\n",
      "G loss: 0.4935325086116791\n",
      "Training Model  ...\n",
      "E loss:  0.706292986869812\n",
      "G loss: 0.4782564043998718\n",
      "E loss:  0.6878609657287598\n",
      "G loss: 0.5569566488265991\n",
      "E loss:  0.6747727394104004\n",
      "G loss: 0.5742147564888\n",
      "E loss:  0.6830316185951233\n",
      "G loss: 0.6104714870452881\n",
      "E loss:  0.6904788017272949\n",
      "G loss: 0.7036535739898682\n",
      "Training Model  ...\n",
      "E loss:  0.6804933547973633\n",
      "G loss: 0.660635769367218\n",
      "E loss:  0.6790633797645569\n",
      "G loss: 0.6336719989776611\n",
      "E loss:  0.6631892919540405\n",
      "G loss: 0.5564161539077759\n",
      "E loss:  0.6674818396568298\n",
      "G loss: 0.5078445672988892\n",
      "E loss:  0.6582564115524292\n",
      "G loss: 0.5331231951713562\n",
      "Training Model  ...\n",
      "E loss:  0.6237459778785706\n",
      "G loss: 0.5011255741119385\n",
      "E loss:  0.6156808137893677\n",
      "G loss: 0.5511019229888916\n",
      "E loss:  0.619434118270874\n",
      "G loss: 0.557953953742981\n",
      "E loss:  0.6101857423782349\n",
      "G loss: 0.5706115365028381\n",
      "E loss:  0.5960733890533447\n",
      "G loss: 0.5673734545707703\n",
      "Training Model  ...\n",
      "E loss:  0.7344148755073547\n",
      "G loss: 0.5909352898597717\n",
      "E loss:  0.7444466352462769\n",
      "G loss: 0.6004852056503296\n",
      "E loss:  0.7395294308662415\n",
      "G loss: 0.584740161895752\n",
      "E loss:  0.7366955876350403\n",
      "G loss: 0.572081446647644\n",
      "E loss:  0.7470815777778625\n",
      "G loss: 0.5370916128158569\n",
      "Training Model  ...\n",
      "E loss:  0.7004246115684509\n",
      "G loss: 0.571441113948822\n",
      "E loss:  0.6963605880737305\n",
      "G loss: 0.5974589586257935\n",
      "E loss:  0.7030436396598816\n",
      "G loss: 0.5841768980026245\n",
      "E loss:  0.7078237533569336\n",
      "G loss: 0.5252127647399902\n",
      "E loss:  0.7043275237083435\n",
      "G loss: 0.6261541247367859\n",
      "Training Model  ...\n",
      "E loss:  0.6622762084007263\n",
      "G loss: 0.5602970123291016\n",
      "E loss:  0.6426821947097778\n",
      "G loss: 0.5668306946754456\n",
      "E loss:  0.6486697196960449\n",
      "G loss: 0.5697128772735596\n",
      "E loss:  0.6484014987945557\n",
      "G loss: 0.539011538028717\n",
      "E loss:  0.6400127410888672\n",
      "G loss: 0.572969377040863\n",
      "Training Model  ...\n",
      "E loss:  0.7108751535415649\n",
      "G loss: 0.5442025065422058\n",
      "E loss:  0.7050747275352478\n",
      "G loss: 0.5393171906471252\n",
      "E loss:  0.6884639263153076\n",
      "G loss: 0.5571362972259521\n",
      "E loss:  0.6901530623435974\n",
      "G loss: 0.5803747177124023\n",
      "E loss:  0.688571035861969\n",
      "G loss: 0.5813772678375244\n",
      "Training Model  ...\n",
      "E loss:  0.6896244883537292\n",
      "G loss: 0.5503692626953125\n",
      "E loss:  0.6839168071746826\n",
      "G loss: 0.5300598740577698\n",
      "E loss:  0.7150765061378479\n",
      "G loss: 0.531283438205719\n",
      "E loss:  0.7134765386581421\n",
      "G loss: 0.5003479719161987\n",
      "E loss:  0.7168611884117126\n",
      "G loss: 0.5573300719261169\n",
      "Training Model  ...\n",
      "E loss:  0.6661497354507446\n",
      "G loss: 0.5782930254936218\n",
      "E loss:  0.6655821800231934\n",
      "G loss: 0.6014386415481567\n",
      "E loss:  0.670627236366272\n",
      "G loss: 0.6367177367210388\n",
      "E loss:  0.6664345264434814\n",
      "G loss: 0.6567701101303101\n",
      "E loss:  0.6681656241416931\n",
      "G loss: 0.6949228644371033\n",
      "Training Model  ...\n",
      "E loss:  0.7366328239440918\n",
      "G loss: 0.690491795539856\n",
      "E loss:  0.7219606041908264\n",
      "G loss: 0.6293925046920776\n",
      "E loss:  0.7295209765434265\n",
      "G loss: 0.5787400007247925\n",
      "E loss:  0.7305696606636047\n",
      "G loss: 0.5143768787384033\n",
      "E loss:  0.736876368522644\n",
      "G loss: 0.5268920660018921\n",
      "Training Model  ...\n",
      "E loss:  0.6015868782997131\n",
      "G loss: 0.5168050527572632\n",
      "E loss:  0.6125859022140503\n",
      "G loss: 0.49974650144577026\n",
      "E loss:  0.6069991588592529\n",
      "G loss: 0.5309011340141296\n",
      "E loss:  0.6019762754440308\n",
      "G loss: 0.5490226745605469\n",
      "E loss:  0.6176793575286865\n",
      "G loss: 0.5809991955757141\n",
      "Training Model  ...\n",
      "E loss:  0.6529419422149658\n",
      "G loss: 0.5644015073776245\n",
      "E loss:  0.6498438119888306\n",
      "G loss: 0.6151232123374939\n",
      "E loss:  0.6569281816482544\n",
      "G loss: 0.6670854091644287\n",
      "E loss:  0.6770376563072205\n",
      "G loss: 0.678091287612915\n",
      "E loss:  0.674092173576355\n",
      "G loss: 0.6739057898521423\n",
      "Training Model  ...\n",
      "E loss:  0.7510509490966797\n",
      "G loss: 0.7037968635559082\n",
      "E loss:  0.7567284107208252\n",
      "G loss: 0.6777200102806091\n",
      "E loss:  0.7428834438323975\n",
      "G loss: 0.6048783659934998\n",
      "E loss:  0.7489576935768127\n",
      "G loss: 0.5828902721405029\n",
      "E loss:  0.7612447142601013\n",
      "G loss: 0.5775114297866821\n",
      "Training Model  ...\n",
      "E loss:  0.680320143699646\n",
      "G loss: 0.5493209362030029\n",
      "E loss:  0.6778610944747925\n",
      "G loss: 0.5234786868095398\n",
      "E loss:  0.6825754642486572\n",
      "G loss: 0.5545535087585449\n",
      "E loss:  0.7048938274383545\n",
      "G loss: 0.5277779698371887\n",
      "E loss:  0.6994110941886902\n",
      "G loss: 0.5033684372901917\n",
      "Training Model  ...\n",
      "E loss:  0.7815424203872681\n",
      "G loss: 0.5407063961029053\n",
      "E loss:  0.7665640115737915\n",
      "G loss: 0.5387701988220215\n",
      "E loss:  0.7594080567359924\n",
      "G loss: 0.5326455235481262\n",
      "E loss:  0.7634997367858887\n",
      "G loss: 0.5878938436508179\n",
      "E loss:  0.7429778575897217\n",
      "G loss: 0.5745365619659424\n",
      "Training Model  ...\n",
      "E loss:  0.6529667377471924\n",
      "G loss: 0.5804301500320435\n",
      "E loss:  0.6725490689277649\n",
      "G loss: 0.6233534216880798\n",
      "E loss:  0.6386789083480835\n",
      "G loss: 0.5990738868713379\n",
      "E loss:  0.6499491930007935\n",
      "G loss: 0.6326520442962646\n",
      "E loss:  0.6399224400520325\n",
      "G loss: 0.5971997380256653\n",
      "Training Model  ...\n",
      "E loss:  0.7218589782714844\n",
      "G loss: 0.6112717986106873\n",
      "E loss:  0.7304192185401917\n",
      "G loss: 0.5487298369407654\n",
      "E loss:  0.7190382480621338\n",
      "G loss: 0.5432872772216797\n",
      "E loss:  0.7254850268363953\n",
      "G loss: 0.48307764530181885\n",
      "E loss:  0.7223986387252808\n",
      "G loss: 0.47023284435272217\n",
      "Training Model  ...\n",
      "E loss:  0.7510017156600952\n",
      "G loss: 0.5299586653709412\n",
      "E loss:  0.7337891459465027\n",
      "G loss: 0.474756121635437\n",
      "E loss:  0.7326603531837463\n",
      "G loss: 0.5873550772666931\n",
      "E loss:  0.7379239797592163\n",
      "G loss: 0.590026319026947\n",
      "E loss:  0.724636971950531\n",
      "G loss: 0.6522356271743774\n",
      "Training Model  ...\n",
      "E loss:  0.7341557741165161\n",
      "G loss: 0.6437000036239624\n",
      "E loss:  0.7340831160545349\n",
      "G loss: 0.5812066793441772\n",
      "E loss:  0.7409780621528625\n",
      "G loss: 0.5194336175918579\n",
      "E loss:  0.7249458432197571\n",
      "G loss: 0.5066388845443726\n",
      "E loss:  0.7301086187362671\n",
      "G loss: 0.45324525237083435\n",
      "Training Model  ...\n",
      "E loss:  0.6800153851509094\n",
      "G loss: 0.4983261525630951\n",
      "E loss:  0.6678866744041443\n",
      "G loss: 0.454741507768631\n",
      "E loss:  0.6755795478820801\n",
      "G loss: 0.4956517219543457\n",
      "E loss:  0.6661888957023621\n",
      "G loss: 0.5081238150596619\n",
      "E loss:  0.6590002775192261\n",
      "G loss: 0.5548587441444397\n",
      "Training Model  ...\n",
      "E loss:  0.6882869005203247\n",
      "G loss: 0.5330129265785217\n",
      "E loss:  0.698758065700531\n",
      "G loss: 0.5428882241249084\n",
      "E loss:  0.6943721175193787\n",
      "G loss: 0.522308886051178\n",
      "E loss:  0.6941299438476562\n",
      "G loss: 0.5303358435630798\n",
      "E loss:  0.6892812252044678\n",
      "G loss: 0.5608164072036743\n",
      "Training Model  ...\n",
      "E loss:  0.7179664969444275\n",
      "G loss: 0.5405691862106323\n",
      "E loss:  0.7012287378311157\n",
      "G loss: 0.5418667197227478\n",
      "E loss:  0.6957460045814514\n",
      "G loss: 0.5336586833000183\n",
      "E loss:  0.7032296657562256\n",
      "G loss: 0.5865842700004578\n",
      "E loss:  0.7139317989349365\n",
      "G loss: 0.5900726318359375\n",
      "Training Model  ...\n",
      "E loss:  0.7917687296867371\n",
      "G loss: 0.5757092833518982\n",
      "E loss:  0.7844855189323425\n",
      "G loss: 0.5726185441017151\n",
      "E loss:  0.7837200164794922\n",
      "G loss: 0.561322033405304\n",
      "E loss:  0.7880982160568237\n",
      "G loss: 0.511775016784668\n",
      "E loss:  0.7947991490364075\n",
      "G loss: 0.5256555676460266\n",
      "Training Model  ...\n",
      "E loss:  0.7684686779975891\n",
      "G loss: 0.5479289889335632\n",
      "E loss:  0.7697970271110535\n",
      "G loss: 0.5236865878105164\n",
      "E loss:  0.7583985328674316\n",
      "G loss: 0.5750656127929688\n",
      "E loss:  0.7646997570991516\n",
      "G loss: 0.586067259311676\n",
      "E loss:  0.7598053216934204\n",
      "G loss: 0.6075326800346375\n",
      "Training Model  ...\n",
      "E loss:  0.6677711606025696\n",
      "G loss: 0.6116234660148621\n",
      "E loss:  0.6394103765487671\n",
      "G loss: 0.5791023969650269\n",
      "E loss:  0.6617158651351929\n",
      "G loss: 0.5394047498703003\n",
      "E loss:  0.6491787433624268\n",
      "G loss: 0.5353583097457886\n",
      "E loss:  0.65177321434021\n",
      "G loss: 0.5473498702049255\n",
      "Training Model  ...\n",
      "E loss:  0.6492682099342346\n",
      "G loss: 0.5538329482078552\n",
      "E loss:  0.6553149819374084\n",
      "G loss: 0.5375282764434814\n",
      "E loss:  0.6355514526367188\n",
      "G loss: 0.5260681509971619\n",
      "E loss:  0.6407332420349121\n",
      "G loss: 0.5545795559883118\n",
      "E loss:  0.6440257430076599\n",
      "G loss: 0.5609327554702759\n",
      "Training Model  ...\n",
      "E loss:  0.6936322450637817\n",
      "G loss: 0.5623841285705566\n",
      "E loss:  0.6963881254196167\n",
      "G loss: 0.5465359091758728\n",
      "E loss:  0.6976414322853088\n",
      "G loss: 0.5671068429946899\n",
      "E loss:  0.713243842124939\n",
      "G loss: 0.5083117485046387\n",
      "E loss:  0.7184028029441833\n",
      "G loss: 0.503487229347229\n",
      "Training Model  ...\n",
      "E loss:  0.7195978164672852\n",
      "G loss: 0.5440641045570374\n",
      "E loss:  0.7364576458930969\n",
      "G loss: 0.5119203329086304\n",
      "E loss:  0.7463710904121399\n",
      "G loss: 0.5039491653442383\n",
      "E loss:  0.7482688426971436\n",
      "G loss: 0.5194364190101624\n",
      "E loss:  0.744788408279419\n",
      "G loss: 0.5073803663253784\n",
      "Training Model  ...\n",
      "E loss:  0.7000954151153564\n",
      "G loss: 0.5298941731452942\n",
      "E loss:  0.6844987869262695\n",
      "G loss: 0.5197127461433411\n",
      "E loss:  0.6835199594497681\n",
      "G loss: 0.5309815406799316\n",
      "E loss:  0.6928204298019409\n",
      "G loss: 0.5920889377593994\n",
      "E loss:  0.6858881711959839\n",
      "G loss: 0.5026262998580933\n",
      "Training Model  ...\n",
      "E loss:  0.6960256695747375\n",
      "G loss: 0.5586676597595215\n",
      "E loss:  0.7048762440681458\n",
      "G loss: 0.5341047644615173\n",
      "E loss:  0.6969404220581055\n",
      "G loss: 0.6295233964920044\n",
      "E loss:  0.6963287591934204\n",
      "G loss: 0.6094363331794739\n",
      "E loss:  0.7010373473167419\n",
      "G loss: 0.6205417513847351\n",
      "Training Model  ...\n",
      "E loss:  0.6787046194076538\n",
      "G loss: 0.5878250598907471\n",
      "E loss:  0.6657261252403259\n",
      "G loss: 0.5120595693588257\n",
      "E loss:  0.6742408275604248\n",
      "G loss: 0.4963212013244629\n",
      "E loss:  0.653710663318634\n",
      "G loss: 0.46088117361068726\n",
      "E loss:  0.6493999361991882\n",
      "G loss: 0.4132651388645172\n",
      "Training Model  ...\n",
      "E loss:  0.6726093888282776\n",
      "G loss: 0.4723449945449829\n",
      "E loss:  0.6780295968055725\n",
      "G loss: 0.502714216709137\n",
      "E loss:  0.6637376546859741\n",
      "G loss: 0.5676966905593872\n",
      "E loss:  0.6440134048461914\n",
      "G loss: 0.5768061876296997\n",
      "E loss:  0.6420835852622986\n",
      "G loss: 0.6078674793243408\n",
      "Training Model  ...\n",
      "E loss:  0.720911979675293\n",
      "G loss: 0.6054791212081909\n",
      "E loss:  0.7058578133583069\n",
      "G loss: 0.5760630369186401\n",
      "E loss:  0.7095403671264648\n",
      "G loss: 0.5423711538314819\n",
      "E loss:  0.7101749181747437\n",
      "G loss: 0.4601529538631439\n",
      "E loss:  0.7181385159492493\n",
      "G loss: 0.4497101306915283\n",
      "Training Model  ...\n",
      "E loss:  0.6877533197402954\n",
      "G loss: 0.4646138548851013\n",
      "E loss:  0.6902633905410767\n",
      "G loss: 0.4587239623069763\n",
      "E loss:  0.6984940767288208\n",
      "G loss: 0.5203144550323486\n",
      "E loss:  0.6871218681335449\n",
      "G loss: 0.529885470867157\n",
      "E loss:  0.6865741014480591\n",
      "G loss: 0.5973718166351318\n",
      "Training Model  ...\n",
      "E loss:  0.7215179800987244\n",
      "G loss: 0.585740864276886\n",
      "E loss:  0.7213090658187866\n",
      "G loss: 0.5628380179405212\n",
      "E loss:  0.7119212746620178\n",
      "G loss: 0.5216118693351746\n",
      "E loss:  0.722923755645752\n",
      "G loss: 0.4889211058616638\n",
      "E loss:  0.7175715565681458\n",
      "G loss: 0.4648633599281311\n",
      "Training Model  ...\n",
      "E loss:  0.7353887557983398\n",
      "G loss: 0.4800579249858856\n",
      "E loss:  0.7169574499130249\n",
      "G loss: 0.493085116147995\n",
      "E loss:  0.7135082483291626\n",
      "G loss: 0.5111742615699768\n",
      "E loss:  0.7088302373886108\n",
      "G loss: 0.5704204440116882\n",
      "E loss:  0.6999777555465698\n",
      "G loss: 0.5905330777168274\n",
      "Training Model  ...\n",
      "E loss:  0.6616072654724121\n",
      "G loss: 0.5646364688873291\n",
      "E loss:  0.6657168865203857\n",
      "G loss: 0.561607837677002\n",
      "E loss:  0.6570460200309753\n",
      "G loss: 0.5441498160362244\n",
      "E loss:  0.6601496338844299\n",
      "G loss: 0.49717915058135986\n",
      "E loss:  0.6584888100624084\n",
      "G loss: 0.4927919805049896\n",
      "Training Model  ...\n",
      "E loss:  0.7011174559593201\n",
      "G loss: 0.4995957016944885\n",
      "E loss:  0.6985009908676147\n",
      "G loss: 0.513177752494812\n",
      "E loss:  0.6911607384681702\n",
      "G loss: 0.48771071434020996\n",
      "E loss:  0.6815569400787354\n",
      "G loss: 0.4862050414085388\n",
      "E loss:  0.6853289604187012\n",
      "G loss: 0.5007527470588684\n",
      "Training Model  ...\n",
      "E loss:  0.7608618140220642\n",
      "G loss: 0.5121786594390869\n",
      "E loss:  0.7538184523582458\n",
      "G loss: 0.5100482702255249\n",
      "E loss:  0.7283046245574951\n",
      "G loss: 0.5186891555786133\n",
      "E loss:  0.7307217121124268\n",
      "G loss: 0.5038584470748901\n",
      "E loss:  0.7333987355232239\n",
      "G loss: 0.5475208163261414\n",
      "Training Model  ...\n",
      "E loss:  0.6511002779006958\n",
      "G loss: 0.5331528186798096\n",
      "E loss:  0.664614200592041\n",
      "G loss: 0.5000030398368835\n",
      "E loss:  0.6562185883522034\n",
      "G loss: 0.505605161190033\n",
      "E loss:  0.6600085496902466\n",
      "G loss: 0.4396723210811615\n",
      "E loss:  0.6641456484794617\n",
      "G loss: 0.4337006211280823\n",
      "Training Model  ...\n",
      "E loss:  0.7119847536087036\n",
      "G loss: 0.4661446809768677\n",
      "E loss:  0.7103558778762817\n",
      "G loss: 0.4859090745449066\n",
      "E loss:  0.6788184642791748\n",
      "G loss: 0.5162519812583923\n",
      "E loss:  0.6716535687446594\n",
      "G loss: 0.6064740419387817\n",
      "E loss:  0.6767219305038452\n",
      "G loss: 0.6610370874404907\n",
      "Training Model  ...\n",
      "E loss:  0.7138896584510803\n",
      "G loss: 0.6133459806442261\n",
      "E loss:  0.7007725238800049\n",
      "G loss: 0.5892685651779175\n",
      "E loss:  0.7098095417022705\n",
      "G loss: 0.5258749723434448\n",
      "E loss:  0.7083941698074341\n",
      "G loss: 0.44727638363838196\n",
      "E loss:  0.7061023712158203\n",
      "G loss: 0.41821587085723877\n",
      "Training Model  ...\n",
      "E loss:  0.7129565477371216\n",
      "G loss: 0.41255587339401245\n",
      "E loss:  0.6985697150230408\n",
      "G loss: 0.44790196418762207\n",
      "E loss:  0.7073573470115662\n",
      "G loss: 0.4623500108718872\n",
      "E loss:  0.7108626365661621\n",
      "G loss: 0.47842931747436523\n",
      "E loss:  0.703506588935852\n",
      "G loss: 0.5510096549987793\n",
      "Training Model  ...\n",
      "E loss:  0.6965909600257874\n",
      "G loss: 0.5121400356292725\n",
      "E loss:  0.7060346007347107\n",
      "G loss: 0.49943551421165466\n",
      "E loss:  0.7111836075782776\n",
      "G loss: 0.5013682246208191\n",
      "E loss:  0.7088490128517151\n",
      "G loss: 0.5056437253952026\n",
      "E loss:  0.706845223903656\n",
      "G loss: 0.5091119408607483\n",
      "Training Model  ...\n",
      "E loss:  0.6189889311790466\n",
      "G loss: 0.48887014389038086\n",
      "E loss:  0.6194701194763184\n",
      "G loss: 0.5075967311859131\n",
      "E loss:  0.5997636914253235\n",
      "G loss: 0.5448803305625916\n",
      "E loss:  0.6103131771087646\n",
      "G loss: 0.5620960593223572\n",
      "E loss:  0.6119557619094849\n",
      "G loss: 0.5729321241378784\n",
      "Training Model  ...\n",
      "E loss:  0.6412068605422974\n",
      "G loss: 0.5676794648170471\n",
      "E loss:  0.6347886323928833\n",
      "G loss: 0.5190427899360657\n",
      "E loss:  0.6217063069343567\n",
      "G loss: 0.5228918194770813\n",
      "E loss:  0.601792573928833\n",
      "G loss: 0.457523375749588\n",
      "E loss:  0.601811945438385\n",
      "G loss: 0.4209212064743042\n",
      "Training Model  ...\n",
      "E loss:  0.7029876112937927\n",
      "G loss: 0.4578152298927307\n",
      "E loss:  0.6923416256904602\n",
      "G loss: 0.4598216712474823\n",
      "E loss:  0.69580078125\n",
      "G loss: 0.47618991136550903\n",
      "E loss:  0.6927591562271118\n",
      "G loss: 0.5048185586929321\n",
      "E loss:  0.6926943063735962\n",
      "G loss: 0.5172717571258545\n",
      "Training Model  ...\n",
      "E loss:  0.7717210650444031\n",
      "G loss: 0.518058180809021\n",
      "E loss:  0.7555946111679077\n",
      "G loss: 0.509247362613678\n",
      "E loss:  0.7493892312049866\n",
      "G loss: 0.567332923412323\n",
      "E loss:  0.7448327541351318\n",
      "G loss: 0.564380943775177\n",
      "E loss:  0.7442225813865662\n",
      "G loss: 0.5707319378852844\n",
      "Training Model  ...\n",
      "E loss:  0.6986727714538574\n",
      "G loss: 0.5457273721694946\n",
      "E loss:  0.6883070468902588\n",
      "G loss: 0.5613595843315125\n",
      "E loss:  0.6939573287963867\n",
      "G loss: 0.5705273151397705\n",
      "E loss:  0.6914536356925964\n",
      "G loss: 0.6610983610153198\n",
      "E loss:  0.7059091329574585\n",
      "G loss: 0.6743109226226807\n",
      "Training Model  ...\n",
      "E loss:  0.7757189869880676\n",
      "G loss: 0.676247775554657\n",
      "E loss:  0.7763519287109375\n",
      "G loss: 0.6446073651313782\n",
      "E loss:  0.7667949199676514\n",
      "G loss: 0.5811930894851685\n",
      "E loss:  0.7663149833679199\n",
      "G loss: 0.5678680539131165\n",
      "E loss:  0.7596952319145203\n",
      "G loss: 0.5488770604133606\n",
      "Training Model  ...\n",
      "E loss:  0.711782693862915\n",
      "G loss: 0.5448986887931824\n",
      "E loss:  0.7087808847427368\n",
      "G loss: 0.5368692874908447\n",
      "E loss:  0.7249540686607361\n",
      "G loss: 0.5174089670181274\n",
      "E loss:  0.7179707884788513\n",
      "G loss: 0.5152232050895691\n",
      "E loss:  0.7228055000305176\n",
      "G loss: 0.5138354301452637\n",
      "Training Model  ...\n",
      "E loss:  0.6950188875198364\n",
      "G loss: 0.48984354734420776\n",
      "E loss:  0.7006898522377014\n",
      "G loss: 0.4469830393791199\n",
      "E loss:  0.6927613615989685\n",
      "G loss: 0.4531092047691345\n",
      "E loss:  0.6910173296928406\n",
      "G loss: 0.4248204827308655\n",
      "E loss:  0.6824502944946289\n",
      "G loss: 0.40545743703842163\n",
      "Training Model  ...\n",
      "E loss:  0.6818214058876038\n",
      "G loss: 0.45787346363067627\n",
      "E loss:  0.6757157444953918\n",
      "G loss: 0.45533487200737\n",
      "E loss:  0.6933066844940186\n",
      "G loss: 0.49497172236442566\n",
      "E loss:  0.6947927474975586\n",
      "G loss: 0.5604522228240967\n",
      "E loss:  0.6887513399124146\n",
      "G loss: 0.5365462303161621\n",
      "Training Model  ...\n",
      "E loss:  0.636025071144104\n",
      "G loss: 0.5380598306655884\n",
      "E loss:  0.6439772844314575\n",
      "G loss: 0.5057252049446106\n",
      "E loss:  0.6402325630187988\n",
      "G loss: 0.5245756506919861\n",
      "E loss:  0.6247881650924683\n",
      "G loss: 0.5043957233428955\n",
      "E loss:  0.6300740242004395\n",
      "G loss: 0.48931705951690674\n",
      "Training Model  ...\n",
      "E loss:  0.6259744763374329\n",
      "G loss: 0.5143727660179138\n",
      "E loss:  0.6299982666969299\n",
      "G loss: 0.5214370489120483\n",
      "E loss:  0.6583769917488098\n",
      "G loss: 0.4034753739833832\n",
      "E loss:  0.6523022651672363\n",
      "G loss: 0.4240489602088928\n",
      "E loss:  0.6409693360328674\n",
      "G loss: 0.4187450408935547\n",
      "Training Model  ...\n",
      "E loss:  0.7114845514297485\n",
      "G loss: 0.4214026927947998\n",
      "E loss:  0.6809754967689514\n",
      "G loss: 0.4311322867870331\n",
      "E loss:  0.6807260513305664\n",
      "G loss: 0.4920431971549988\n",
      "E loss:  0.6887991428375244\n",
      "G loss: 0.5151627659797668\n",
      "E loss:  0.6889355182647705\n",
      "G loss: 0.5850889086723328\n",
      "Training Model  ...\n",
      "E loss:  0.6183990240097046\n",
      "G loss: 0.5495163202285767\n",
      "E loss:  0.5986658930778503\n",
      "G loss: 0.49522796273231506\n",
      "E loss:  0.5948368310928345\n",
      "G loss: 0.46568113565444946\n",
      "E loss:  0.5946502685546875\n",
      "G loss: 0.4518648386001587\n",
      "E loss:  0.5838480591773987\n",
      "G loss: 0.4421202540397644\n",
      "Training Model  ...\n",
      "E loss:  0.6689220070838928\n",
      "G loss: 0.384611576795578\n",
      "E loss:  0.6554613709449768\n",
      "G loss: 0.42136111855506897\n",
      "E loss:  0.6321682929992676\n",
      "G loss: 0.4457707703113556\n",
      "E loss:  0.6473099589347839\n",
      "G loss: 0.4764891266822815\n",
      "E loss:  0.6510317921638489\n",
      "G loss: 0.5592542886734009\n",
      "Training Model  ...\n",
      "E loss:  0.7008428573608398\n",
      "G loss: 0.5028291344642639\n",
      "E loss:  0.695741593837738\n",
      "G loss: 0.47699931263923645\n",
      "E loss:  0.7110869884490967\n",
      "G loss: 0.47997546195983887\n",
      "E loss:  0.7100868225097656\n",
      "G loss: 0.4272741973400116\n",
      "E loss:  0.7000272274017334\n",
      "G loss: 0.45331719517707825\n",
      "Training Model  ...\n",
      "E loss:  0.725056529045105\n",
      "G loss: 0.43934470415115356\n",
      "E loss:  0.7259149551391602\n",
      "G loss: 0.5026040077209473\n",
      "E loss:  0.7171910405158997\n",
      "G loss: 0.5083834528923035\n",
      "E loss:  0.7072710394859314\n",
      "G loss: 0.5310858488082886\n",
      "E loss:  0.6965522766113281\n",
      "G loss: 0.5478689670562744\n",
      "Training Model  ...\n",
      "E loss:  0.6269134283065796\n",
      "G loss: 0.5358977317810059\n",
      "E loss:  0.6233197450637817\n",
      "G loss: 0.491351842880249\n",
      "E loss:  0.6265190839767456\n",
      "G loss: 0.4530021548271179\n",
      "E loss:  0.6224538087844849\n",
      "G loss: 0.4767377972602844\n",
      "E loss:  0.6207548379898071\n",
      "G loss: 0.4426993131637573\n",
      "Training Model  ...\n",
      "E loss:  0.6241322755813599\n",
      "G loss: 0.46682125329971313\n",
      "E loss:  0.635402262210846\n",
      "G loss: 0.47338181734085083\n",
      "E loss:  0.6330726742744446\n",
      "G loss: 0.5187000632286072\n",
      "E loss:  0.6379702687263489\n",
      "G loss: 0.5343775749206543\n",
      "E loss:  0.6337950229644775\n",
      "G loss: 0.5392196178436279\n",
      "Training Model  ...\n",
      "E loss:  0.6395674347877502\n",
      "G loss: 0.5156949162483215\n",
      "E loss:  0.6427057981491089\n",
      "G loss: 0.4653739631175995\n",
      "E loss:  0.6338408589363098\n",
      "G loss: 0.43668463826179504\n",
      "E loss:  0.6219319105148315\n",
      "G loss: 0.41781118512153625\n",
      "E loss:  0.6165655851364136\n",
      "G loss: 0.38625073432922363\n",
      "Training Model  ...\n",
      "E loss:  0.6289677619934082\n",
      "G loss: 0.41824209690093994\n",
      "E loss:  0.624672532081604\n",
      "G loss: 0.38875332474708557\n",
      "E loss:  0.6220760941505432\n",
      "G loss: 0.4286448061466217\n",
      "E loss:  0.6193255186080933\n",
      "G loss: 0.4547048807144165\n",
      "E loss:  0.6137279272079468\n",
      "G loss: 0.4849073886871338\n",
      "Training Model  ...\n",
      "E loss:  0.7784734964370728\n",
      "G loss: 0.4702245891094208\n",
      "E loss:  0.7641808390617371\n",
      "G loss: 0.47097206115722656\n",
      "E loss:  0.7806493639945984\n",
      "G loss: 0.5297008752822876\n",
      "E loss:  0.7674055695533752\n",
      "G loss: 0.522576630115509\n",
      "E loss:  0.7446199059486389\n",
      "G loss: 0.498201459646225\n",
      "Training Model  ...\n",
      "E loss:  0.7229906320571899\n",
      "G loss: 0.5471070408821106\n",
      "E loss:  0.7139521241188049\n",
      "G loss: 0.49496179819107056\n",
      "E loss:  0.7185917496681213\n",
      "G loss: 0.5512725114822388\n",
      "E loss:  0.7182902097702026\n",
      "G loss: 0.5320132970809937\n",
      "E loss:  0.730380654335022\n",
      "G loss: 0.5384427309036255\n",
      "Training Model  ...\n",
      "E loss:  0.6672049760818481\n",
      "G loss: 0.5573354959487915\n",
      "E loss:  0.6714150309562683\n",
      "G loss: 0.502086877822876\n",
      "E loss:  0.6627464294433594\n",
      "G loss: 0.4591761827468872\n",
      "E loss:  0.6566917896270752\n",
      "G loss: 0.412874698638916\n",
      "E loss:  0.6501700282096863\n",
      "G loss: 0.3693769574165344\n",
      "Training Model  ...\n",
      "E loss:  0.6497377157211304\n",
      "G loss: 0.3964054584503174\n",
      "E loss:  0.6622332334518433\n",
      "G loss: 0.4232867956161499\n",
      "E loss:  0.6621322631835938\n",
      "G loss: 0.456447571516037\n",
      "E loss:  0.6570629477500916\n",
      "G loss: 0.4376329779624939\n",
      "E loss:  0.6490617394447327\n",
      "G loss: 0.49600619077682495\n",
      "Training Model  ...\n",
      "E loss:  0.7418159246444702\n",
      "G loss: 0.5223328471183777\n",
      "E loss:  0.7413458228111267\n",
      "G loss: 0.5139014720916748\n",
      "E loss:  0.7370692491531372\n",
      "G loss: 0.5384435653686523\n",
      "E loss:  0.7305070161819458\n",
      "G loss: 0.5840003490447998\n",
      "E loss:  0.7270214557647705\n",
      "G loss: 0.6215065717697144\n",
      "Training Model  ...\n",
      "E loss:  0.578167736530304\n",
      "G loss: 0.5616512894630432\n",
      "E loss:  0.5787323117256165\n",
      "G loss: 0.5188482403755188\n",
      "E loss:  0.5683807730674744\n",
      "G loss: 0.4846535623073578\n",
      "E loss:  0.5561935901641846\n",
      "G loss: 0.4704934358596802\n",
      "E loss:  0.5490424633026123\n",
      "G loss: 0.3796122968196869\n",
      "Training Model  ...\n",
      "E loss:  0.7096633315086365\n",
      "G loss: 0.4156193733215332\n",
      "E loss:  0.6939464807510376\n",
      "G loss: 0.42364099621772766\n",
      "E loss:  0.7013066411018372\n",
      "G loss: 0.5037004351615906\n",
      "E loss:  0.6788914799690247\n",
      "G loss: 0.5159757137298584\n",
      "E loss:  0.6936783790588379\n",
      "G loss: 0.5605888962745667\n",
      "Training Model  ...\n",
      "E loss:  0.7226187586784363\n",
      "G loss: 0.5497307181358337\n",
      "E loss:  0.6983614563941956\n",
      "G loss: 0.5254920721054077\n",
      "E loss:  0.6970595717430115\n",
      "G loss: 0.5664919018745422\n",
      "E loss:  0.6952773332595825\n",
      "G loss: 0.5378065705299377\n",
      "E loss:  0.7076112031936646\n",
      "G loss: 0.5230277180671692\n",
      "Training Model  ...\n",
      "E loss:  0.6197742819786072\n",
      "G loss: 0.504330575466156\n",
      "E loss:  0.6023975014686584\n",
      "G loss: 0.49588626623153687\n",
      "E loss:  0.6109192967414856\n",
      "G loss: 0.4685911536216736\n",
      "E loss:  0.612879753112793\n",
      "G loss: 0.44775670766830444\n",
      "E loss:  0.6139405965805054\n",
      "G loss: 0.4073335826396942\n",
      "Training Model  ...\n",
      "E loss:  0.6703191995620728\n",
      "G loss: 0.43430081009864807\n",
      "E loss:  0.6509995460510254\n",
      "G loss: 0.4371471703052521\n",
      "E loss:  0.6473570466041565\n",
      "G loss: 0.4709520936012268\n",
      "E loss:  0.6291552186012268\n",
      "G loss: 0.5016545653343201\n",
      "E loss:  0.6378257870674133\n",
      "G loss: 0.5181013345718384\n",
      "Training Model  ...\n",
      "E loss:  0.7428505420684814\n",
      "G loss: 0.5118252038955688\n",
      "E loss:  0.7507786750793457\n",
      "G loss: 0.4938381314277649\n",
      "E loss:  0.7530250549316406\n",
      "G loss: 0.4927569031715393\n",
      "E loss:  0.7535900473594666\n",
      "G loss: 0.45745646953582764\n",
      "E loss:  0.7524584531784058\n",
      "G loss: 0.43375545740127563\n",
      "Training Model  ...\n",
      "E loss:  0.6509652733802795\n",
      "G loss: 0.4629721939563751\n",
      "E loss:  0.6527253985404968\n",
      "G loss: 0.4474404454231262\n",
      "E loss:  0.6555144190788269\n",
      "G loss: 0.4306146800518036\n",
      "E loss:  0.6497087478637695\n",
      "G loss: 0.4488306939601898\n",
      "E loss:  0.6483119130134583\n",
      "G loss: 0.4427390992641449\n",
      "Training Model  ...\n",
      "E loss:  0.6709869503974915\n",
      "G loss: 0.458771288394928\n",
      "E loss:  0.6662893891334534\n",
      "G loss: 0.4345070719718933\n",
      "E loss:  0.6717863082885742\n",
      "G loss: 0.45306986570358276\n",
      "E loss:  0.6771356463432312\n",
      "G loss: 0.4502643942832947\n",
      "E loss:  0.681357204914093\n",
      "G loss: 0.4810482859611511\n",
      "Training Model  ...\n",
      "E loss:  0.6558451056480408\n",
      "G loss: 0.5073370933532715\n",
      "E loss:  0.6730250716209412\n",
      "G loss: 0.4784295856952667\n",
      "E loss:  0.6781272888183594\n",
      "G loss: 0.4242091774940491\n",
      "E loss:  0.6695768237113953\n",
      "G loss: 0.4469102621078491\n",
      "E loss:  0.6688794493675232\n",
      "G loss: 0.47419416904449463\n",
      "Training Model  ...\n",
      "E loss:  0.6930910348892212\n",
      "G loss: 0.42161086201667786\n",
      "E loss:  0.6872239112854004\n",
      "G loss: 0.43057942390441895\n",
      "E loss:  0.6864633560180664\n",
      "G loss: 0.4728432595729828\n",
      "E loss:  0.6775971055030823\n",
      "G loss: 0.42751508951187134\n",
      "E loss:  0.6667134165763855\n",
      "G loss: 0.4587559700012207\n",
      "Training Model  ...\n",
      "E loss:  0.725182294845581\n",
      "G loss: 0.42704933881759644\n",
      "E loss:  0.7260530591011047\n",
      "G loss: 0.48440098762512207\n",
      "E loss:  0.7182784080505371\n",
      "G loss: 0.46213147044181824\n",
      "E loss:  0.7284889817237854\n",
      "G loss: 0.5107444524765015\n",
      "E loss:  0.7520201802253723\n",
      "G loss: 0.4891852140426636\n",
      "Training Model  ...\n",
      "E loss:  0.7199530601501465\n",
      "G loss: 0.4977666735649109\n",
      "E loss:  0.704904317855835\n",
      "G loss: 0.4805198013782501\n",
      "E loss:  0.7088383436203003\n",
      "G loss: 0.48588043451309204\n",
      "E loss:  0.7153639793395996\n",
      "G loss: 0.4547104835510254\n",
      "E loss:  0.7135472893714905\n",
      "G loss: 0.46076515316963196\n",
      "Training Model  ...\n",
      "E loss:  0.6553367376327515\n",
      "G loss: 0.45701685547828674\n",
      "E loss:  0.6665664315223694\n",
      "G loss: 0.4646138846874237\n",
      "E loss:  0.6734564900398254\n",
      "G loss: 0.46600648760795593\n",
      "E loss:  0.6733742952346802\n",
      "G loss: 0.5250721573829651\n",
      "E loss:  0.6805786490440369\n",
      "G loss: 0.5008126497268677\n",
      "Training Model  ...\n",
      "E loss:  0.6714214086532593\n",
      "G loss: 0.480638325214386\n",
      "E loss:  0.6704068779945374\n",
      "G loss: 0.5137799978256226\n",
      "E loss:  0.6677452921867371\n",
      "G loss: 0.4500586688518524\n",
      "E loss:  0.6599058508872986\n",
      "G loss: 0.507819414138794\n",
      "E loss:  0.6521797180175781\n",
      "G loss: 0.5275608897209167\n",
      "Training Model  ...\n",
      "E loss:  0.6918132305145264\n",
      "G loss: 0.4903046488761902\n",
      "E loss:  0.7111523151397705\n",
      "G loss: 0.4741951525211334\n",
      "E loss:  0.7252353429794312\n",
      "G loss: 0.49858659505844116\n",
      "E loss:  0.7285802364349365\n",
      "G loss: 0.4885374903678894\n",
      "E loss:  0.718963086605072\n",
      "G loss: 0.509421169757843\n",
      "Training Model  ...\n",
      "E loss:  0.6366846561431885\n",
      "G loss: 0.4824661314487457\n",
      "E loss:  0.6490365266799927\n",
      "G loss: 0.43733975291252136\n",
      "E loss:  0.6429598331451416\n",
      "G loss: 0.41255345940589905\n",
      "E loss:  0.6444956064224243\n",
      "G loss: 0.3579108715057373\n",
      "E loss:  0.651078999042511\n",
      "G loss: 0.35589125752449036\n",
      "Training Model  ...\n",
      "E loss:  0.7651791572570801\n",
      "G loss: 0.34469708800315857\n",
      "E loss:  0.7766146659851074\n",
      "G loss: 0.3973357081413269\n",
      "E loss:  0.7450862526893616\n",
      "G loss: 0.4912892282009125\n",
      "E loss:  0.7341493964195251\n",
      "G loss: 0.5314751267433167\n",
      "E loss:  0.7489455938339233\n",
      "G loss: 0.5974948406219482\n",
      "Training Model  ...\n",
      "E loss:  0.6605000495910645\n",
      "G loss: 0.5724093914031982\n",
      "E loss:  0.6602041125297546\n",
      "G loss: 0.5329115390777588\n",
      "E loss:  0.674813985824585\n",
      "G loss: 0.4923402667045593\n",
      "E loss:  0.6745150089263916\n",
      "G loss: 0.4441642165184021\n",
      "E loss:  0.6604701280593872\n",
      "G loss: 0.4366864562034607\n",
      "Training Model  ...\n",
      "E loss:  0.6475666165351868\n",
      "G loss: 0.4224797189235687\n",
      "E loss:  0.6314120888710022\n",
      "G loss: 0.38555294275283813\n",
      "E loss:  0.6365418434143066\n",
      "G loss: 0.40539711713790894\n",
      "E loss:  0.6431910395622253\n",
      "G loss: 0.3848016560077667\n",
      "E loss:  0.6359177827835083\n",
      "G loss: 0.37177926301956177\n",
      "Training Model  ...\n",
      "E loss:  0.6528075933456421\n",
      "G loss: 0.38890865445137024\n",
      "E loss:  0.6470631957054138\n",
      "G loss: 0.35695937275886536\n",
      "E loss:  0.6343080997467041\n",
      "G loss: 0.41948944330215454\n",
      "E loss:  0.6209595203399658\n",
      "G loss: 0.4361792802810669\n",
      "E loss:  0.6236404776573181\n",
      "G loss: 0.45560410618782043\n",
      "Training Model  ...\n",
      "E loss:  0.6691682934761047\n",
      "G loss: 0.41772860288619995\n",
      "E loss:  0.664362370967865\n",
      "G loss: 0.4545218050479889\n",
      "E loss:  0.6865136623382568\n",
      "G loss: 0.4350280165672302\n",
      "E loss:  0.6755548119544983\n",
      "G loss: 0.4657163918018341\n",
      "E loss:  0.67106032371521\n",
      "G loss: 0.4586949348449707\n",
      "Training Model  ...\n",
      "E loss:  0.7072945237159729\n",
      "G loss: 0.4643529951572418\n",
      "E loss:  0.6986256837844849\n",
      "G loss: 0.4624038636684418\n",
      "E loss:  0.7003272771835327\n",
      "G loss: 0.42619556188583374\n",
      "E loss:  0.6865098476409912\n",
      "G loss: 0.4426906406879425\n",
      "E loss:  0.6877049803733826\n",
      "G loss: 0.4002399146556854\n",
      "Training Model  ...\n",
      "E loss:  0.7144730687141418\n",
      "G loss: 0.4222288429737091\n",
      "E loss:  0.709100067615509\n",
      "G loss: 0.42679163813591003\n",
      "E loss:  0.686282217502594\n",
      "G loss: 0.43975144624710083\n",
      "E loss:  0.6878702044487\n",
      "G loss: 0.4878782629966736\n",
      "E loss:  0.680764377117157\n",
      "G loss: 0.4766841232776642\n",
      "Training Model  ...\n",
      "E loss:  0.7178772687911987\n",
      "G loss: 0.48840227723121643\n",
      "E loss:  0.7179301977157593\n",
      "G loss: 0.4959349036216736\n",
      "E loss:  0.713402271270752\n",
      "G loss: 0.4735870957374573\n",
      "E loss:  0.7021877765655518\n",
      "G loss: 0.46530473232269287\n",
      "E loss:  0.707781195640564\n",
      "G loss: 0.4889407753944397\n",
      "Training Model  ...\n",
      "E loss:  0.6498125791549683\n",
      "G loss: 0.4910266399383545\n",
      "E loss:  0.6459159255027771\n",
      "G loss: 0.4650101363658905\n",
      "E loss:  0.6570882201194763\n",
      "G loss: 0.4840407967567444\n",
      "E loss:  0.658421516418457\n",
      "G loss: 0.43245729804039\n",
      "E loss:  0.6534463763237\n",
      "G loss: 0.4575093984603882\n",
      "Training Model  ...\n",
      "E loss:  0.6786676645278931\n",
      "G loss: 0.43392348289489746\n",
      "E loss:  0.6609911918640137\n",
      "G loss: 0.4342740476131439\n",
      "E loss:  0.6704569458961487\n",
      "G loss: 0.46556466817855835\n",
      "E loss:  0.6748952269554138\n",
      "G loss: 0.48869892954826355\n",
      "E loss:  0.6768938302993774\n",
      "G loss: 0.452404260635376\n",
      "Training Model  ...\n",
      "E loss:  0.6737353801727295\n",
      "G loss: 0.4531887173652649\n",
      "E loss:  0.6733337640762329\n",
      "G loss: 0.4602251648902893\n",
      "E loss:  0.6688132286071777\n",
      "G loss: 0.41325247287750244\n",
      "E loss:  0.6672166585922241\n",
      "G loss: 0.38626575469970703\n",
      "E loss:  0.6729205250740051\n",
      "G loss: 0.3979318141937256\n",
      "Training Model  ...\n",
      "E loss:  0.6745897531509399\n",
      "G loss: 0.361768513917923\n",
      "E loss:  0.6758034825325012\n",
      "G loss: 0.38925445079803467\n",
      "E loss:  0.672566294670105\n",
      "G loss: 0.3790266513824463\n",
      "E loss:  0.6705982685089111\n",
      "G loss: 0.39894407987594604\n",
      "E loss:  0.6597146987915039\n",
      "G loss: 0.3715948462486267\n",
      "Training Model  ...\n",
      "E loss:  0.627558708190918\n",
      "G loss: 0.4095536470413208\n",
      "E loss:  0.6338768601417542\n",
      "G loss: 0.39421334862709045\n",
      "E loss:  0.6171793937683105\n",
      "G loss: 0.4085259735584259\n",
      "E loss:  0.6283172965049744\n",
      "G loss: 0.44424527883529663\n",
      "E loss:  0.6288793683052063\n",
      "G loss: 0.47308969497680664\n",
      "Training Model  ...\n",
      "E loss:  0.635942280292511\n",
      "G loss: 0.4401952624320984\n",
      "E loss:  0.6250213384628296\n",
      "G loss: 0.4652552008628845\n",
      "E loss:  0.6255187392234802\n",
      "G loss: 0.46858125925064087\n",
      "E loss:  0.6103415489196777\n",
      "G loss: 0.5160701870918274\n",
      "E loss:  0.6258593797683716\n",
      "G loss: 0.5015318393707275\n",
      "Training Model  ...\n",
      "E loss:  0.6668922901153564\n",
      "G loss: 0.4855385422706604\n",
      "E loss:  0.6728936433792114\n",
      "G loss: 0.44695353507995605\n",
      "E loss:  0.6769253015518188\n",
      "G loss: 0.5116521120071411\n",
      "E loss:  0.6723701357841492\n",
      "G loss: 0.4562795162200928\n",
      "E loss:  0.6805171370506287\n",
      "G loss: 0.4372410774230957\n",
      "Training Model  ...\n",
      "E loss:  0.7085281014442444\n",
      "G loss: 0.4201093316078186\n",
      "E loss:  0.6890621185302734\n",
      "G loss: 0.4330272674560547\n",
      "E loss:  0.6834911704063416\n",
      "G loss: 0.4389569163322449\n",
      "E loss:  0.7027380466461182\n",
      "G loss: 0.44980210065841675\n",
      "E loss:  0.6984252333641052\n",
      "G loss: 0.5044705867767334\n",
      "Training Model  ...\n",
      "E loss:  0.7325609922409058\n",
      "G loss: 0.46251559257507324\n",
      "E loss:  0.7278316020965576\n",
      "G loss: 0.4539673626422882\n",
      "E loss:  0.7266780138015747\n",
      "G loss: 0.42084646224975586\n",
      "E loss:  0.7347594499588013\n",
      "G loss: 0.38561591506004333\n",
      "E loss:  0.7242586016654968\n",
      "G loss: 0.39538127183914185\n",
      "Training Model  ...\n",
      "E loss:  0.6035308241844177\n",
      "G loss: 0.41167202591896057\n",
      "E loss:  0.620013415813446\n",
      "G loss: 0.41765540838241577\n",
      "E loss:  0.6294428110122681\n",
      "G loss: 0.407429575920105\n",
      "E loss:  0.6336996555328369\n",
      "G loss: 0.39594554901123047\n",
      "E loss:  0.6316115856170654\n",
      "G loss: 0.45973289012908936\n",
      "Training Model  ...\n",
      "E loss:  0.6859927177429199\n",
      "G loss: 0.45429834723472595\n",
      "E loss:  0.6956807374954224\n",
      "G loss: 0.47100257873535156\n",
      "E loss:  0.7153462767601013\n",
      "G loss: 0.496606707572937\n",
      "E loss:  0.7071982622146606\n",
      "G loss: 0.512762725353241\n",
      "E loss:  0.716339111328125\n",
      "G loss: 0.538815975189209\n",
      "Training Model  ...\n",
      "E loss:  0.6978834271430969\n",
      "G loss: 0.5247361660003662\n",
      "E loss:  0.7034857273101807\n",
      "G loss: 0.4827200770378113\n",
      "E loss:  0.7048577070236206\n",
      "G loss: 0.4548967480659485\n",
      "E loss:  0.7054870128631592\n",
      "G loss: 0.45782631635665894\n",
      "E loss:  0.6974915862083435\n",
      "G loss: 0.41540899872779846\n",
      "Training Model  ...\n",
      "E loss:  0.6265228986740112\n",
      "G loss: 0.4513280987739563\n",
      "E loss:  0.6090430021286011\n",
      "G loss: 0.3945311903953552\n",
      "E loss:  0.62186199426651\n",
      "G loss: 0.37406039237976074\n",
      "E loss:  0.6080646514892578\n",
      "G loss: 0.363925039768219\n",
      "E loss:  0.6154918074607849\n",
      "G loss: 0.339874804019928\n",
      "Training Model  ...\n",
      "E loss:  0.7424684166908264\n",
      "G loss: 0.3290407359600067\n",
      "E loss:  0.7370423674583435\n",
      "G loss: 0.42500361800193787\n",
      "E loss:  0.7283583283424377\n",
      "G loss: 0.4528887867927551\n",
      "E loss:  0.7312731146812439\n",
      "G loss: 0.5056719779968262\n",
      "E loss:  0.7310844659805298\n",
      "G loss: 0.521124541759491\n",
      "Training Model  ...\n",
      "E loss:  0.669052243232727\n",
      "G loss: 0.5082964897155762\n",
      "E loss:  0.6548836827278137\n",
      "G loss: 0.46223700046539307\n",
      "E loss:  0.6412619352340698\n",
      "G loss: 0.42706406116485596\n",
      "E loss:  0.6439533233642578\n",
      "G loss: 0.3782382607460022\n",
      "E loss:  0.6430480480194092\n",
      "G loss: 0.3617880940437317\n",
      "Training Model  ...\n",
      "E loss:  0.6799338459968567\n",
      "G loss: 0.3566417694091797\n",
      "E loss:  0.6869022846221924\n",
      "G loss: 0.41696086525917053\n",
      "E loss:  0.6730852127075195\n",
      "G loss: 0.4279527962207794\n",
      "E loss:  0.6546196937561035\n",
      "G loss: 0.4610605239868164\n",
      "E loss:  0.6723757982254028\n",
      "G loss: 0.4737326204776764\n",
      "Training Model  ...\n",
      "E loss:  0.5974647998809814\n",
      "G loss: 0.4656532406806946\n",
      "E loss:  0.5920116305351257\n",
      "G loss: 0.46593669056892395\n",
      "E loss:  0.5908268690109253\n",
      "G loss: 0.37925058603286743\n",
      "E loss:  0.584640383720398\n",
      "G loss: 0.37220844626426697\n",
      "E loss:  0.5883877277374268\n",
      "G loss: 0.36129048466682434\n",
      "Training Model  ...\n",
      "E loss:  0.6787371039390564\n",
      "G loss: 0.38831210136413574\n",
      "E loss:  0.6995725631713867\n",
      "G loss: 0.3818736970424652\n",
      "E loss:  0.6948454976081848\n",
      "G loss: 0.4127328395843506\n",
      "E loss:  0.6787133812904358\n",
      "G loss: 0.4605262279510498\n",
      "E loss:  0.6792842745780945\n",
      "G loss: 0.5022415518760681\n",
      "Training Model  ...\n",
      "E loss:  0.6589587926864624\n",
      "G loss: 0.5333436131477356\n",
      "E loss:  0.6538055539131165\n",
      "G loss: 0.4646657705307007\n",
      "E loss:  0.6443982720375061\n",
      "G loss: 0.4812576174736023\n",
      "E loss:  0.6486285924911499\n",
      "G loss: 0.4235879182815552\n",
      "E loss:  0.6394920945167542\n",
      "G loss: 0.39725545048713684\n",
      "Training Model  ...\n",
      "E loss:  0.5921716690063477\n",
      "G loss: 0.4028700888156891\n",
      "E loss:  0.6051892042160034\n",
      "G loss: 0.38157469034194946\n",
      "E loss:  0.6129924058914185\n",
      "G loss: 0.3797334134578705\n",
      "E loss:  0.6080039739608765\n",
      "G loss: 0.39270538091659546\n",
      "E loss:  0.6317515969276428\n",
      "G loss: 0.37267962098121643\n",
      "Training Model  ...\n",
      "E loss:  0.6544240713119507\n",
      "G loss: 0.3871467709541321\n",
      "E loss:  0.6499927639961243\n",
      "G loss: 0.4239237308502197\n",
      "E loss:  0.631992518901825\n",
      "G loss: 0.46381282806396484\n",
      "E loss:  0.6425182819366455\n",
      "G loss: 0.4790111184120178\n",
      "E loss:  0.6541687846183777\n",
      "G loss: 0.5038221478462219\n",
      "Training Model  ...\n",
      "E loss:  0.6388862133026123\n",
      "G loss: 0.5070038437843323\n",
      "E loss:  0.6211965680122375\n",
      "G loss: 0.46972835063934326\n",
      "E loss:  0.6210769414901733\n",
      "G loss: 0.4874252378940582\n",
      "E loss:  0.6143074631690979\n",
      "G loss: 0.4182765781879425\n",
      "E loss:  0.614538311958313\n",
      "G loss: 0.4248814582824707\n",
      "Training Model  ...\n",
      "E loss:  0.6478168964385986\n",
      "G loss: 0.44262656569480896\n",
      "E loss:  0.6443882584571838\n",
      "G loss: 0.42096182703971863\n",
      "E loss:  0.6366551518440247\n",
      "G loss: 0.4291418790817261\n",
      "E loss:  0.6302640438079834\n",
      "G loss: 0.4094048738479614\n",
      "E loss:  0.6319704651832581\n",
      "G loss: 0.46192964911460876\n",
      "Training Model  ...\n",
      "E loss:  0.706437349319458\n",
      "G loss: 0.41957342624664307\n",
      "E loss:  0.6981689929962158\n",
      "G loss: 0.44840848445892334\n",
      "E loss:  0.6914090514183044\n",
      "G loss: 0.4396328926086426\n",
      "E loss:  0.6946542263031006\n",
      "G loss: 0.46015048027038574\n",
      "E loss:  0.6867534518241882\n",
      "G loss: 0.4576939344406128\n",
      "Training Model  ...\n",
      "E loss:  0.6339846253395081\n",
      "G loss: 0.4191027283668518\n",
      "E loss:  0.6364697217941284\n",
      "G loss: 0.45119065046310425\n",
      "E loss:  0.6367009878158569\n",
      "G loss: 0.4557057023048401\n",
      "E loss:  0.6341951489448547\n",
      "G loss: 0.4030367434024811\n",
      "E loss:  0.6425809860229492\n",
      "G loss: 0.42263495922088623\n",
      "Training Model  ...\n",
      "E loss:  0.6540145874023438\n",
      "G loss: 0.4552356004714966\n",
      "E loss:  0.6528792977333069\n",
      "G loss: 0.4309278726577759\n",
      "E loss:  0.6575881242752075\n",
      "G loss: 0.40397322177886963\n",
      "E loss:  0.6644997596740723\n",
      "G loss: 0.45503196120262146\n",
      "E loss:  0.6610684990882874\n",
      "G loss: 0.4341583251953125\n",
      "Training Model  ...\n",
      "E loss:  0.6591011881828308\n",
      "G loss: 0.4190383553504944\n",
      "E loss:  0.6691369414329529\n",
      "G loss: 0.45278680324554443\n",
      "E loss:  0.6601290106773376\n",
      "G loss: 0.40334179997444153\n",
      "E loss:  0.6418620347976685\n",
      "G loss: 0.41832953691482544\n",
      "E loss:  0.6382922530174255\n",
      "G loss: 0.4069761335849762\n",
      "Training Model  ...\n",
      "E loss:  0.5952622890472412\n",
      "G loss: 0.4137847125530243\n",
      "E loss:  0.6029001474380493\n",
      "G loss: 0.4316575229167938\n",
      "E loss:  0.6017724275588989\n",
      "G loss: 0.3910007178783417\n",
      "E loss:  0.6123253703117371\n",
      "G loss: 0.38126224279403687\n",
      "E loss:  0.6226158738136292\n",
      "G loss: 0.4007289707660675\n",
      "Training Model  ...\n",
      "E loss:  0.6988896131515503\n",
      "G loss: 0.36613067984580994\n",
      "E loss:  0.6976538300514221\n",
      "G loss: 0.40005308389663696\n",
      "E loss:  0.6840922832489014\n",
      "G loss: 0.38337382674217224\n",
      "E loss:  0.6878247857093811\n",
      "G loss: 0.44349175691604614\n",
      "E loss:  0.6856724619865417\n",
      "G loss: 0.3974090814590454\n",
      "Training Model  ...\n",
      "E loss:  0.6949790716171265\n",
      "G loss: 0.4411277770996094\n",
      "E loss:  0.695878267288208\n",
      "G loss: 0.4249855577945709\n",
      "E loss:  0.6882572174072266\n",
      "G loss: 0.397807776927948\n",
      "E loss:  0.6866601705551147\n",
      "G loss: 0.38349294662475586\n",
      "E loss:  0.6793177723884583\n",
      "G loss: 0.3790713846683502\n",
      "Training Model  ...\n",
      "E loss:  0.6427482962608337\n",
      "G loss: 0.3661743104457855\n",
      "E loss:  0.6503227949142456\n",
      "G loss: 0.38411927223205566\n",
      "E loss:  0.6645433306694031\n",
      "G loss: 0.4391879737377167\n",
      "E loss:  0.6516775488853455\n",
      "G loss: 0.43131396174430847\n",
      "E loss:  0.664837121963501\n",
      "G loss: 0.46158653497695923\n",
      "Training Model  ...\n",
      "E loss:  0.6329643726348877\n",
      "G loss: 0.4678187668323517\n",
      "E loss:  0.6288713216781616\n",
      "G loss: 0.36722251772880554\n",
      "E loss:  0.6165881156921387\n",
      "G loss: 0.3541264832019806\n",
      "E loss:  0.622397780418396\n",
      "G loss: 0.31724223494529724\n",
      "E loss:  0.6115804314613342\n",
      "G loss: 0.29146748781204224\n",
      "Training Model  ...\n",
      "E loss:  0.7104384899139404\n",
      "G loss: 0.32043230533599854\n",
      "E loss:  0.6929278373718262\n",
      "G loss: 0.3787294030189514\n",
      "E loss:  0.6812720894813538\n",
      "G loss: 0.43689724802970886\n",
      "E loss:  0.6833608746528625\n",
      "G loss: 0.5016856789588928\n",
      "E loss:  0.6886876225471497\n",
      "G loss: 0.5629318356513977\n",
      "Training Model  ...\n",
      "E loss:  0.5821270942687988\n",
      "G loss: 0.5537407398223877\n",
      "E loss:  0.5818819999694824\n",
      "G loss: 0.535132110118866\n",
      "E loss:  0.576441764831543\n",
      "G loss: 0.4798852205276489\n",
      "E loss:  0.5836561322212219\n",
      "G loss: 0.44835782051086426\n",
      "E loss:  0.5677429437637329\n",
      "G loss: 0.4302089810371399\n",
      "Training Model  ...\n",
      "E loss:  0.6346460580825806\n",
      "G loss: 0.41581010818481445\n",
      "E loss:  0.6413156986236572\n",
      "G loss: 0.4019762873649597\n",
      "E loss:  0.6460691690444946\n",
      "G loss: 0.40175962448120117\n",
      "E loss:  0.6310804486274719\n",
      "G loss: 0.3965960741043091\n",
      "E loss:  0.6424415111541748\n",
      "G loss: 0.4123198986053467\n",
      "Training Model  ...\n",
      "E loss:  0.6471579074859619\n",
      "G loss: 0.36895012855529785\n",
      "E loss:  0.6645277142524719\n",
      "G loss: 0.38843604922294617\n",
      "E loss:  0.650375247001648\n",
      "G loss: 0.42706677317619324\n",
      "E loss:  0.6579217910766602\n",
      "G loss: 0.42206549644470215\n",
      "E loss:  0.6398974657058716\n",
      "G loss: 0.4413927495479584\n",
      "Training Model  ...\n",
      "E loss:  0.6980181336402893\n",
      "G loss: 0.4122841954231262\n",
      "E loss:  0.6931971311569214\n",
      "G loss: 0.4128592014312744\n",
      "E loss:  0.6935547590255737\n",
      "G loss: 0.4132503271102905\n",
      "E loss:  0.7130358219146729\n",
      "G loss: 0.3968448340892792\n",
      "E loss:  0.7084652781486511\n",
      "G loss: 0.37194663286209106\n",
      "Training Model  ...\n",
      "E loss:  0.5839408040046692\n",
      "G loss: 0.3916515111923218\n",
      "E loss:  0.5846588015556335\n",
      "G loss: 0.39376774430274963\n",
      "E loss:  0.58563631772995\n",
      "G loss: 0.35552728176116943\n",
      "E loss:  0.6067217588424683\n",
      "G loss: 0.4000321924686432\n",
      "E loss:  0.5995438098907471\n",
      "G loss: 0.42001140117645264\n",
      "Training Model  ...\n",
      "E loss:  0.597818911075592\n",
      "G loss: 0.37620681524276733\n",
      "E loss:  0.5820814967155457\n",
      "G loss: 0.38832634687423706\n",
      "E loss:  0.5784088969230652\n",
      "G loss: 0.3922134041786194\n",
      "E loss:  0.593946635723114\n",
      "G loss: 0.32962435483932495\n",
      "E loss:  0.5905089378356934\n",
      "G loss: 0.3712354898452759\n",
      "Training Model  ...\n",
      "E loss:  0.6526122093200684\n",
      "G loss: 0.3875672221183777\n",
      "E loss:  0.6542518734931946\n",
      "G loss: 0.408835768699646\n",
      "E loss:  0.63199383020401\n",
      "G loss: 0.4237213730812073\n",
      "E loss:  0.638412356376648\n",
      "G loss: 0.4918491542339325\n",
      "E loss:  0.6322059035301208\n",
      "G loss: 0.49794691801071167\n",
      "Training Model  ...\n",
      "E loss:  0.6573671102523804\n",
      "G loss: 0.4820092022418976\n",
      "E loss:  0.6482444405555725\n",
      "G loss: 0.45798826217651367\n",
      "E loss:  0.6539418697357178\n",
      "G loss: 0.42931118607521057\n",
      "E loss:  0.6566002368927002\n",
      "G loss: 0.4425281286239624\n",
      "E loss:  0.6605018973350525\n",
      "G loss: 0.38486844301223755\n",
      "Training Model  ...\n",
      "E loss:  0.7350304126739502\n",
      "G loss: 0.39190012216567993\n",
      "E loss:  0.7218159437179565\n",
      "G loss: 0.4383455514907837\n",
      "E loss:  0.7194458842277527\n",
      "G loss: 0.4402686357498169\n",
      "E loss:  0.7078660130500793\n",
      "G loss: 0.4664939045906067\n",
      "E loss:  0.7033904790878296\n",
      "G loss: 0.4545057713985443\n",
      "Training Model  ...\n",
      "E loss:  0.6633521914482117\n",
      "G loss: 0.43171021342277527\n",
      "E loss:  0.6511856913566589\n",
      "G loss: 0.43624889850616455\n",
      "E loss:  0.6744364500045776\n",
      "G loss: 0.4173036813735962\n",
      "E loss:  0.6797310709953308\n",
      "G loss: 0.42597496509552\n",
      "E loss:  0.6672751903533936\n",
      "G loss: 0.4049358069896698\n",
      "Training Model  ...\n",
      "E loss:  0.6596351265907288\n",
      "G loss: 0.3762666583061218\n",
      "E loss:  0.6503543257713318\n",
      "G loss: 0.38271623849868774\n",
      "E loss:  0.638486385345459\n",
      "G loss: 0.40568870306015015\n",
      "E loss:  0.6413393616676331\n",
      "G loss: 0.37350472807884216\n",
      "E loss:  0.6253150105476379\n",
      "G loss: 0.3226877450942993\n",
      "Training Model  ...\n",
      "E loss:  0.7040528655052185\n",
      "G loss: 0.37018874287605286\n",
      "E loss:  0.6914569139480591\n",
      "G loss: 0.36441320180892944\n",
      "E loss:  0.6762157082557678\n",
      "G loss: 0.39875781536102295\n",
      "E loss:  0.6776514649391174\n",
      "G loss: 0.46408262848854065\n",
      "E loss:  0.6786671876907349\n",
      "G loss: 0.4507947862148285\n",
      "Training Model  ...\n",
      "E loss:  0.6326443552970886\n",
      "G loss: 0.42677444219589233\n",
      "E loss:  0.6296266317367554\n",
      "G loss: 0.41020506620407104\n",
      "E loss:  0.623265266418457\n",
      "G loss: 0.4171512722969055\n",
      "E loss:  0.620418131351471\n",
      "G loss: 0.4454996585845947\n",
      "E loss:  0.620293140411377\n",
      "G loss: 0.4124855399131775\n",
      "Training Model  ...\n",
      "E loss:  0.5722022652626038\n",
      "G loss: 0.4225015342235565\n",
      "E loss:  0.5623993277549744\n",
      "G loss: 0.3965001106262207\n",
      "E loss:  0.568138062953949\n",
      "G loss: 0.38614025712013245\n",
      "E loss:  0.563408613204956\n",
      "G loss: 0.3985201418399811\n",
      "E loss:  0.5654589533805847\n",
      "G loss: 0.3695249855518341\n",
      "Training Model  ...\n",
      "E loss:  0.600629985332489\n",
      "G loss: 0.36066243052482605\n",
      "E loss:  0.5936988592147827\n",
      "G loss: 0.3746235966682434\n",
      "E loss:  0.5913755893707275\n",
      "G loss: 0.3918202221393585\n",
      "E loss:  0.58583003282547\n",
      "G loss: 0.40023550391197205\n",
      "E loss:  0.5786935687065125\n",
      "G loss: 0.3927532434463501\n",
      "Training Model  ...\n",
      "E loss:  0.6898812055587769\n",
      "G loss: 0.3809531331062317\n",
      "E loss:  0.6977514028549194\n",
      "G loss: 0.4155614674091339\n",
      "E loss:  0.6894340515136719\n",
      "G loss: 0.44481563568115234\n",
      "E loss:  0.6938991546630859\n",
      "G loss: 0.44654226303100586\n",
      "E loss:  0.6955564618110657\n",
      "G loss: 0.417233943939209\n",
      "Training Model  ...\n",
      "E loss:  0.6155495643615723\n",
      "G loss: 0.4230678975582123\n",
      "E loss:  0.6242186427116394\n",
      "G loss: 0.4208402633666992\n",
      "E loss:  0.622821569442749\n",
      "G loss: 0.4174247682094574\n",
      "E loss:  0.633038341999054\n",
      "G loss: 0.42150813341140747\n",
      "E loss:  0.6422683000564575\n",
      "G loss: 0.4553324580192566\n",
      "Training Model  ...\n",
      "E loss:  0.6774193644523621\n",
      "G loss: 0.4353153705596924\n",
      "E loss:  0.6693530082702637\n",
      "G loss: 0.40643852949142456\n",
      "E loss:  0.6720952987670898\n",
      "G loss: 0.4479343593120575\n",
      "E loss:  0.6583977937698364\n",
      "G loss: 0.4472460150718689\n",
      "E loss:  0.6653891801834106\n",
      "G loss: 0.3791027069091797\n",
      "Training Model  ...\n",
      "E loss:  0.6603334546089172\n",
      "G loss: 0.41180258989334106\n",
      "E loss:  0.663353681564331\n",
      "G loss: 0.37666475772857666\n",
      "E loss:  0.6528217792510986\n",
      "G loss: 0.41142022609710693\n",
      "E loss:  0.6394727230072021\n",
      "G loss: 0.37616631388664246\n",
      "E loss:  0.6456525325775146\n",
      "G loss: 0.37476396560668945\n",
      "Training Model  ...\n",
      "E loss:  0.6556340456008911\n",
      "G loss: 0.33275941014289856\n",
      "E loss:  0.6520401835441589\n",
      "G loss: 0.38291046023368835\n",
      "E loss:  0.6592634320259094\n",
      "G loss: 0.32382476329803467\n",
      "E loss:  0.6518933773040771\n",
      "G loss: 0.31369078159332275\n",
      "E loss:  0.6661985516548157\n",
      "G loss: 0.3372045159339905\n",
      "Training Model  ...\n",
      "E loss:  0.6372590661048889\n",
      "G loss: 0.3248477578163147\n",
      "E loss:  0.6577575206756592\n",
      "G loss: 0.3773382902145386\n",
      "E loss:  0.6481703519821167\n",
      "G loss: 0.396761029958725\n",
      "E loss:  0.6477803587913513\n",
      "G loss: 0.4312102496623993\n",
      "E loss:  0.6557608842849731\n",
      "G loss: 0.4749869108200073\n",
      "Training Model  ...\n",
      "E loss:  0.7022223472595215\n",
      "G loss: 0.4584611654281616\n",
      "E loss:  0.7059448957443237\n",
      "G loss: 0.46076804399490356\n",
      "E loss:  0.7015661597251892\n",
      "G loss: 0.42236456274986267\n",
      "E loss:  0.7099699974060059\n",
      "G loss: 0.40789854526519775\n",
      "E loss:  0.7003864645957947\n",
      "G loss: 0.41513383388519287\n",
      "Training Model  ...\n",
      "E loss:  0.6119498610496521\n",
      "G loss: 0.3853357136249542\n",
      "E loss:  0.6180484890937805\n",
      "G loss: 0.41620755195617676\n",
      "E loss:  0.6269888281822205\n",
      "G loss: 0.4227658808231354\n",
      "E loss:  0.6123905181884766\n",
      "G loss: 0.4198608696460724\n",
      "E loss:  0.6153360605239868\n",
      "G loss: 0.4585014581680298\n",
      "Training Model  ...\n",
      "E loss:  0.6709474325180054\n",
      "G loss: 0.44659194350242615\n",
      "E loss:  0.6600017547607422\n",
      "G loss: 0.47116607427597046\n",
      "E loss:  0.6692968010902405\n",
      "G loss: 0.45653650164604187\n",
      "E loss:  0.6989808082580566\n",
      "G loss: 0.45497193932533264\n",
      "E loss:  0.6677542328834534\n",
      "G loss: 0.42851731181144714\n",
      "Training Model  ...\n",
      "E loss:  0.6638121604919434\n",
      "G loss: 0.4124462902545929\n",
      "E loss:  0.6718177199363708\n",
      "G loss: 0.3871198296546936\n",
      "E loss:  0.6644894480705261\n",
      "G loss: 0.4137406051158905\n",
      "E loss:  0.6655935645103455\n",
      "G loss: 0.3630448579788208\n",
      "E loss:  0.6708893775939941\n",
      "G loss: 0.3764209747314453\n",
      "Training Model  ...\n",
      "E loss:  0.6593706607818604\n",
      "G loss: 0.3511623442173004\n",
      "E loss:  0.6553813815116882\n",
      "G loss: 0.40785250067710876\n",
      "E loss:  0.6406476497650146\n",
      "G loss: 0.41652655601501465\n",
      "E loss:  0.6329799294471741\n",
      "G loss: 0.44610098004341125\n",
      "E loss:  0.6294876933097839\n",
      "G loss: 0.43555396795272827\n",
      "Training Model  ...\n",
      "E loss:  0.6520370841026306\n",
      "G loss: 0.4375612139701843\n",
      "E loss:  0.6849140524864197\n",
      "G loss: 0.4175651967525482\n",
      "E loss:  0.6549642086029053\n",
      "G loss: 0.388738751411438\n",
      "E loss:  0.6672577261924744\n",
      "G loss: 0.34886226058006287\n",
      "E loss:  0.6526480317115784\n",
      "G loss: 0.3432951867580414\n",
      "Training Model  ...\n",
      "E loss:  0.6857610940933228\n",
      "G loss: 0.3651493191719055\n",
      "E loss:  0.6800373792648315\n",
      "G loss: 0.33071649074554443\n",
      "E loss:  0.6851358413696289\n",
      "G loss: 0.3350905776023865\n",
      "E loss:  0.6644312739372253\n",
      "G loss: 0.319119930267334\n",
      "E loss:  0.6569774746894836\n",
      "G loss: 0.3500414788722992\n",
      "Training Model  ...\n",
      "E loss:  0.5914385914802551\n",
      "G loss: 0.3580021858215332\n",
      "E loss:  0.5807970762252808\n",
      "G loss: 0.36281803250312805\n",
      "E loss:  0.585686206817627\n",
      "G loss: 0.35249075293540955\n",
      "E loss:  0.586698055267334\n",
      "G loss: 0.37721046805381775\n",
      "E loss:  0.5863098502159119\n",
      "G loss: 0.3745864927768707\n",
      "Training Model  ...\n",
      "E loss:  0.6351763606071472\n",
      "G loss: 0.39755693078041077\n",
      "E loss:  0.6123605370521545\n",
      "G loss: 0.38911643624305725\n",
      "E loss:  0.6093336939811707\n",
      "G loss: 0.3448306620121002\n",
      "E loss:  0.6183941960334778\n",
      "G loss: 0.3279576003551483\n",
      "E loss:  0.62382572889328\n",
      "G loss: 0.3612603545188904\n",
      "Training Model  ...\n",
      "E loss:  0.657732367515564\n",
      "G loss: 0.3363412618637085\n",
      "E loss:  0.656274139881134\n",
      "G loss: 0.3467026352882385\n",
      "E loss:  0.6503080129623413\n",
      "G loss: 0.3745972514152527\n",
      "E loss:  0.6450300812721252\n",
      "G loss: 0.37266433238983154\n",
      "E loss:  0.6341851949691772\n",
      "G loss: 0.3972829580307007\n",
      "Training Model  ...\n",
      "E loss:  0.6149445176124573\n",
      "G loss: 0.39496397972106934\n",
      "E loss:  0.6256280541419983\n",
      "G loss: 0.40720024704933167\n",
      "E loss:  0.6072848439216614\n",
      "G loss: 0.36957427859306335\n",
      "E loss:  0.6041804552078247\n",
      "G loss: 0.3928241729736328\n",
      "E loss:  0.609526515007019\n",
      "G loss: 0.3800258934497833\n",
      "Training Model  ...\n",
      "E loss:  0.6294124722480774\n",
      "G loss: 0.350925475358963\n",
      "E loss:  0.636296272277832\n",
      "G loss: 0.3099060654640198\n",
      "E loss:  0.641374945640564\n",
      "G loss: 0.34849968552589417\n",
      "E loss:  0.6385387182235718\n",
      "G loss: 0.36358222365379333\n",
      "E loss:  0.6276478171348572\n",
      "G loss: 0.38948190212249756\n",
      "Training Model  ...\n",
      "E loss:  0.6893512010574341\n",
      "G loss: 0.3421032428741455\n",
      "E loss:  0.6785915493965149\n",
      "G loss: 0.3816888928413391\n",
      "E loss:  0.6712557673454285\n",
      "G loss: 0.35147249698638916\n",
      "E loss:  0.6627559661865234\n",
      "G loss: 0.33793535828590393\n",
      "E loss:  0.6577826738357544\n",
      "G loss: 0.3957676589488983\n",
      "Training Model  ...\n",
      "E loss:  0.6747741103172302\n",
      "G loss: 0.36133092641830444\n",
      "E loss:  0.683363139629364\n",
      "G loss: 0.3819155693054199\n",
      "E loss:  0.6834178566932678\n",
      "G loss: 0.393111914396286\n",
      "E loss:  0.677996039390564\n",
      "G loss: 0.44072243571281433\n",
      "E loss:  0.6642618775367737\n",
      "G loss: 0.4430348873138428\n",
      "Training Model  ...\n",
      "E loss:  0.6853044033050537\n",
      "G loss: 0.39832037687301636\n",
      "E loss:  0.6845097541809082\n",
      "G loss: 0.4230225682258606\n",
      "E loss:  0.6758431792259216\n",
      "G loss: 0.411935031414032\n",
      "E loss:  0.6697705984115601\n",
      "G loss: 0.459087610244751\n",
      "E loss:  0.6749609112739563\n",
      "G loss: 0.43166694045066833\n",
      "Training Model  ...\n",
      "E loss:  0.6163052916526794\n",
      "G loss: 0.43692073225975037\n",
      "E loss:  0.6091808080673218\n",
      "G loss: 0.46018338203430176\n",
      "E loss:  0.6067346334457397\n",
      "G loss: 0.45354118943214417\n",
      "E loss:  0.6021789312362671\n",
      "G loss: 0.4561099112033844\n",
      "E loss:  0.6068695783615112\n",
      "G loss: 0.4760114252567291\n",
      "Training Model  ...\n",
      "E loss:  0.6224373579025269\n",
      "G loss: 0.47219669818878174\n",
      "E loss:  0.6132251024246216\n",
      "G loss: 0.41368359327316284\n",
      "E loss:  0.6137039661407471\n",
      "G loss: 0.38048648834228516\n",
      "E loss:  0.615821897983551\n",
      "G loss: 0.35125064849853516\n",
      "E loss:  0.6115738153457642\n",
      "G loss: 0.31554102897644043\n",
      "Training Model  ...\n",
      "E loss:  0.6219139695167542\n",
      "G loss: 0.3131273090839386\n",
      "E loss:  0.6273370981216431\n",
      "G loss: 0.29075753688812256\n",
      "E loss:  0.6225947141647339\n",
      "G loss: 0.31167277693748474\n",
      "E loss:  0.6279218196868896\n",
      "G loss: 0.3233155310153961\n",
      "E loss:  0.6272077560424805\n",
      "G loss: 0.3297613263130188\n",
      "Training Model  ...\n",
      "E loss:  0.6199355721473694\n",
      "G loss: 0.3398323655128479\n",
      "E loss:  0.629835307598114\n",
      "G loss: 0.3243609666824341\n",
      "E loss:  0.618287980556488\n",
      "G loss: 0.3618324100971222\n",
      "E loss:  0.6109345555305481\n",
      "G loss: 0.3652716875076294\n",
      "E loss:  0.607010006904602\n",
      "G loss: 0.40601590275764465\n",
      "Training Model  ...\n",
      "E loss:  0.6624141931533813\n",
      "G loss: 0.36574581265449524\n",
      "E loss:  0.6618210077285767\n",
      "G loss: 0.3607465922832489\n",
      "E loss:  0.6422433853149414\n",
      "G loss: 0.40280190110206604\n",
      "E loss:  0.6411466598510742\n",
      "G loss: 0.379395991563797\n",
      "E loss:  0.6474800705909729\n",
      "G loss: 0.35433685779571533\n",
      "Training Model  ...\n",
      "E loss:  0.6112919449806213\n",
      "G loss: 0.3569033145904541\n",
      "E loss:  0.6005382537841797\n",
      "G loss: 0.33349159359931946\n",
      "E loss:  0.5902611613273621\n",
      "G loss: 0.3050000071525574\n",
      "E loss:  0.5990619659423828\n",
      "G loss: 0.2584197521209717\n",
      "E loss:  0.5997709035873413\n",
      "G loss: 0.27177077531814575\n",
      "Training Model  ...\n",
      "E loss:  0.5845930576324463\n",
      "G loss: 0.26067355275154114\n",
      "E loss:  0.5615554451942444\n",
      "G loss: 0.27107930183410645\n",
      "E loss:  0.5545111894607544\n",
      "G loss: 0.2946055829524994\n",
      "E loss:  0.5649503469467163\n",
      "G loss: 0.3278154730796814\n",
      "E loss:  0.561416506767273\n",
      "G loss: 0.35951685905456543\n",
      "Training Model  ...\n",
      "E loss:  0.6665968298912048\n",
      "G loss: 0.40138477087020874\n",
      "E loss:  0.646517276763916\n",
      "G loss: 0.4075787365436554\n",
      "E loss:  0.6548070311546326\n",
      "G loss: 0.4444960355758667\n",
      "E loss:  0.6313174962997437\n",
      "G loss: 0.45138370990753174\n",
      "E loss:  0.6373133659362793\n",
      "G loss: 0.4627751111984253\n",
      "Training Model  ...\n",
      "E loss:  0.6277493238449097\n",
      "G loss: 0.42374157905578613\n",
      "E loss:  0.6239287853240967\n",
      "G loss: 0.42329129576683044\n",
      "E loss:  0.6112580299377441\n",
      "G loss: 0.36373692750930786\n",
      "E loss:  0.6155408620834351\n",
      "G loss: 0.3319093883037567\n",
      "E loss:  0.6228408217430115\n",
      "G loss: 0.31552064418792725\n",
      "Training Model  ...\n",
      "E loss:  0.6488758325576782\n",
      "G loss: 0.3507687449455261\n",
      "E loss:  0.6507523655891418\n",
      "G loss: 0.3286930024623871\n",
      "E loss:  0.6443230509757996\n",
      "G loss: 0.39427611231803894\n",
      "E loss:  0.63758784532547\n",
      "G loss: 0.4063730239868164\n",
      "E loss:  0.6430198550224304\n",
      "G loss: 0.41761404275894165\n",
      "Training Model  ...\n",
      "E loss:  0.6502959728240967\n",
      "G loss: 0.41650331020355225\n",
      "E loss:  0.665917158126831\n",
      "G loss: 0.3727284371852875\n",
      "E loss:  0.6608772277832031\n",
      "G loss: 0.34715399146080017\n",
      "E loss:  0.6443161964416504\n",
      "G loss: 0.35359737277030945\n",
      "E loss:  0.6312328577041626\n",
      "G loss: 0.3406025767326355\n",
      "Training Model  ...\n",
      "E loss:  0.6790590286254883\n",
      "G loss: 0.3441128432750702\n",
      "E loss:  0.6763219833374023\n",
      "G loss: 0.37674659490585327\n",
      "E loss:  0.6647164821624756\n",
      "G loss: 0.4154801666736603\n",
      "E loss:  0.6681579351425171\n",
      "G loss: 0.4981454312801361\n",
      "E loss:  0.6716846227645874\n",
      "G loss: 0.483042448759079\n",
      "Training Model  ...\n",
      "E loss:  0.6014699935913086\n",
      "G loss: 0.45813846588134766\n",
      "E loss:  0.5896707773208618\n",
      "G loss: 0.431306391954422\n",
      "E loss:  0.57411128282547\n",
      "G loss: 0.3509179353713989\n",
      "E loss:  0.5738390684127808\n",
      "G loss: 0.2912803888320923\n",
      "E loss:  0.5732406377792358\n",
      "G loss: 0.27246761322021484\n",
      "Training Model  ...\n",
      "E loss:  0.6829333305358887\n",
      "G loss: 0.28136271238327026\n",
      "E loss:  0.6701477766036987\n",
      "G loss: 0.30915313959121704\n",
      "E loss:  0.6577398777008057\n",
      "G loss: 0.3874455690383911\n",
      "E loss:  0.6531575322151184\n",
      "G loss: 0.40213292837142944\n",
      "E loss:  0.6568240523338318\n",
      "G loss: 0.49108970165252686\n",
      "Training Model  ...\n",
      "E loss:  0.6238178610801697\n",
      "G loss: 0.46792200207710266\n",
      "E loss:  0.6038156151771545\n",
      "G loss: 0.4089322090148926\n",
      "E loss:  0.5840187668800354\n",
      "G loss: 0.3398626744747162\n",
      "E loss:  0.5723357796669006\n",
      "G loss: 0.2908998131752014\n",
      "E loss:  0.5838000178337097\n",
      "G loss: 0.26086944341659546\n",
      "Training Model  ...\n",
      "E loss:  0.6268601417541504\n",
      "G loss: 0.29182812571525574\n",
      "E loss:  0.6327773332595825\n",
      "G loss: 0.3176364004611969\n",
      "E loss:  0.6285905838012695\n",
      "G loss: 0.3191809058189392\n",
      "E loss:  0.6218858957290649\n",
      "G loss: 0.3987990915775299\n",
      "E loss:  0.5990428328514099\n",
      "G loss: 0.43619123101234436\n",
      "Training Model  ...\n",
      "E loss:  0.5755112767219543\n",
      "G loss: 0.42386555671691895\n",
      "E loss:  0.5840153694152832\n",
      "G loss: 0.3585473299026489\n",
      "E loss:  0.5819276571273804\n",
      "G loss: 0.35746169090270996\n",
      "E loss:  0.5818431377410889\n",
      "G loss: 0.3343186676502228\n",
      "E loss:  0.593181848526001\n",
      "G loss: 0.3188145160675049\n",
      "Training Model  ...\n",
      "E loss:  0.6701608896255493\n",
      "G loss: 0.3418099582195282\n",
      "E loss:  0.653621256351471\n",
      "G loss: 0.3764827251434326\n",
      "E loss:  0.6606702208518982\n",
      "G loss: 0.41059625148773193\n",
      "E loss:  0.658536970615387\n",
      "G loss: 0.43138444423675537\n",
      "E loss:  0.6681973934173584\n",
      "G loss: 0.4703262150287628\n",
      "Training Model  ...\n",
      "E loss:  0.5769190192222595\n",
      "G loss: 0.42509275674819946\n",
      "E loss:  0.5785384774208069\n",
      "G loss: 0.4085729718208313\n",
      "E loss:  0.5772294998168945\n",
      "G loss: 0.3240508437156677\n",
      "E loss:  0.5809689164161682\n",
      "G loss: 0.3192194402217865\n",
      "E loss:  0.5964406728744507\n",
      "G loss: 0.30536407232284546\n",
      "Training Model  ...\n",
      "E loss:  0.5727968215942383\n",
      "G loss: 0.29312199354171753\n",
      "E loss:  0.5691510438919067\n",
      "G loss: 0.32107529044151306\n",
      "E loss:  0.5817408561706543\n",
      "G loss: 0.35843366384506226\n",
      "E loss:  0.5726187825202942\n",
      "G loss: 0.3440436124801636\n",
      "E loss:  0.5774136781692505\n",
      "G loss: 0.37993839383125305\n",
      "Training Model  ...\n",
      "E loss:  0.5697726011276245\n",
      "G loss: 0.3815990686416626\n",
      "E loss:  0.5631770491600037\n",
      "G loss: 0.4205259382724762\n",
      "E loss:  0.5613146424293518\n",
      "G loss: 0.41892245411872864\n",
      "E loss:  0.5703654289245605\n",
      "G loss: 0.4381936490535736\n",
      "E loss:  0.5645662546157837\n",
      "G loss: 0.43233200907707214\n",
      "Training Model  ...\n",
      "E loss:  0.6143040060997009\n",
      "G loss: 0.42747294902801514\n",
      "E loss:  0.6147249937057495\n",
      "G loss: 0.35687410831451416\n",
      "E loss:  0.6127892732620239\n",
      "G loss: 0.38936325907707214\n",
      "E loss:  0.6195613145828247\n",
      "G loss: 0.3617672026157379\n",
      "E loss:  0.6179806590080261\n",
      "G loss: 0.37119078636169434\n",
      "Training Model  ...\n",
      "E loss:  0.5600675940513611\n",
      "G loss: 0.32144778966903687\n",
      "E loss:  0.5771773457527161\n",
      "G loss: 0.36738908290863037\n",
      "E loss:  0.5649732351303101\n",
      "G loss: 0.3725193738937378\n",
      "E loss:  0.568719208240509\n",
      "G loss: 0.382874459028244\n",
      "E loss:  0.5755571722984314\n",
      "G loss: 0.39268502593040466\n",
      "Training Model  ...\n",
      "E loss:  0.5901381969451904\n",
      "G loss: 0.3772745132446289\n",
      "E loss:  0.5853226780891418\n",
      "G loss: 0.3492645025253296\n",
      "E loss:  0.5773294568061829\n",
      "G loss: 0.33115747570991516\n",
      "E loss:  0.5781590342521667\n",
      "G loss: 0.2855762541294098\n",
      "E loss:  0.5798488855361938\n",
      "G loss: 0.29642462730407715\n",
      "Training Model  ...\n",
      "E loss:  0.5897990465164185\n",
      "G loss: 0.27475646138191223\n",
      "E loss:  0.5725142955780029\n",
      "G loss: 0.2933734357357025\n",
      "E loss:  0.576030969619751\n",
      "G loss: 0.31444087624549866\n",
      "E loss:  0.5762026309967041\n",
      "G loss: 0.34983402490615845\n",
      "E loss:  0.5766891241073608\n",
      "G loss: 0.3591264486312866\n",
      "Training Model  ...\n",
      "E loss:  0.6244668960571289\n",
      "G loss: 0.3469148576259613\n",
      "E loss:  0.6034435033798218\n",
      "G loss: 0.3500112295150757\n",
      "E loss:  0.6024320125579834\n",
      "G loss: 0.33809947967529297\n",
      "E loss:  0.604320764541626\n",
      "G loss: 0.2919967770576477\n",
      "E loss:  0.6042218804359436\n",
      "G loss: 0.2988685667514801\n",
      "Training Model  ...\n",
      "E loss:  0.6295961141586304\n",
      "G loss: 0.28607675433158875\n",
      "E loss:  0.6448097229003906\n",
      "G loss: 0.2837790548801422\n",
      "E loss:  0.6334437131881714\n",
      "G loss: 0.3176139295101166\n",
      "E loss:  0.6205978393554688\n",
      "G loss: 0.3003371059894562\n",
      "E loss:  0.6186202764511108\n",
      "G loss: 0.3136192560195923\n",
      "Training Model  ...\n",
      "E loss:  0.6322078108787537\n",
      "G loss: 0.3639431297779083\n",
      "E loss:  0.6278306841850281\n",
      "G loss: 0.34246426820755005\n",
      "E loss:  0.6165673732757568\n",
      "G loss: 0.37005695700645447\n",
      "E loss:  0.6199069619178772\n",
      "G loss: 0.3486442267894745\n",
      "E loss:  0.6238762140274048\n",
      "G loss: 0.36026352643966675\n",
      "Training Model  ...\n",
      "E loss:  0.621034562587738\n",
      "G loss: 0.3299868404865265\n",
      "E loss:  0.6267532110214233\n",
      "G loss: 0.3099360466003418\n",
      "E loss:  0.6251169443130493\n",
      "G loss: 0.36684221029281616\n",
      "E loss:  0.6382470726966858\n",
      "G loss: 0.3206920325756073\n",
      "E loss:  0.6436452865600586\n",
      "G loss: 0.31182271242141724\n",
      "Training Model  ...\n",
      "E loss:  0.6709694862365723\n",
      "G loss: 0.3221339285373688\n",
      "E loss:  0.6794827580451965\n",
      "G loss: 0.35098889470100403\n",
      "E loss:  0.6756682395935059\n",
      "G loss: 0.3774341642856598\n",
      "E loss:  0.6740163564682007\n",
      "G loss: 0.36288076639175415\n",
      "E loss:  0.653715968132019\n",
      "G loss: 0.4012291729450226\n",
      "Training Model  ...\n",
      "E loss:  0.6023202538490295\n",
      "G loss: 0.4067501127719879\n",
      "E loss:  0.5938568115234375\n",
      "G loss: 0.4134831726551056\n",
      "E loss:  0.5873453617095947\n",
      "G loss: 0.36744463443756104\n",
      "E loss:  0.5899964570999146\n",
      "G loss: 0.34185391664505005\n",
      "E loss:  0.6064468026161194\n",
      "G loss: 0.35109826922416687\n",
      "Training Model  ...\n",
      "E loss:  0.6073724627494812\n",
      "G loss: 0.3369431793689728\n",
      "E loss:  0.5978963971138\n",
      "G loss: 0.3772439956665039\n",
      "E loss:  0.591832160949707\n",
      "G loss: 0.3182261884212494\n",
      "E loss:  0.5948309898376465\n",
      "G loss: 0.32223251461982727\n",
      "E loss:  0.5970590114593506\n",
      "G loss: 0.323709636926651\n",
      "Training Model  ...\n",
      "E loss:  0.6501001715660095\n",
      "G loss: 0.30478063225746155\n",
      "E loss:  0.6529408693313599\n",
      "G loss: 0.3108290433883667\n",
      "E loss:  0.6586734056472778\n",
      "G loss: 0.33850544691085815\n",
      "E loss:  0.6590355038642883\n",
      "G loss: 0.3325153887271881\n",
      "E loss:  0.6544598937034607\n",
      "G loss: 0.385037362575531\n",
      "Training Model  ...\n",
      "E loss:  0.5903245210647583\n",
      "G loss: 0.33760902285575867\n",
      "E loss:  0.5903619527816772\n",
      "G loss: 0.31910428404808044\n",
      "E loss:  0.5796442627906799\n",
      "G loss: 0.2873680591583252\n",
      "E loss:  0.577741801738739\n",
      "G loss: 0.258872389793396\n",
      "E loss:  0.5835297107696533\n",
      "G loss: 0.24697303771972656\n",
      "Training Model  ...\n",
      "E loss:  0.6164129972457886\n",
      "G loss: 0.28273874521255493\n",
      "E loss:  0.6100203990936279\n",
      "G loss: 0.32126402854919434\n",
      "E loss:  0.6035203337669373\n",
      "G loss: 0.3188067674636841\n",
      "E loss:  0.5848786234855652\n",
      "G loss: 0.3597974181175232\n",
      "E loss:  0.5698111653327942\n",
      "G loss: 0.367458701133728\n",
      "Training Model  ...\n",
      "E loss:  0.6010273694992065\n",
      "G loss: 0.3432060182094574\n",
      "E loss:  0.5922797918319702\n",
      "G loss: 0.328779399394989\n",
      "E loss:  0.5930148363113403\n",
      "G loss: 0.285300612449646\n",
      "E loss:  0.5944655537605286\n",
      "G loss: 0.2693188190460205\n",
      "E loss:  0.5939511656761169\n",
      "G loss: 0.256391316652298\n",
      "Training Model  ...\n",
      "E loss:  0.5740737915039062\n",
      "G loss: 0.2956100106239319\n",
      "E loss:  0.5702246427536011\n",
      "G loss: 0.27871108055114746\n",
      "E loss:  0.5632783770561218\n",
      "G loss: 0.2985602915287018\n",
      "E loss:  0.5463019609451294\n",
      "G loss: 0.37131884694099426\n",
      "E loss:  0.5450583696365356\n",
      "G loss: 0.36383056640625\n",
      "Training Model  ...\n",
      "E loss:  0.586046576499939\n",
      "G loss: 0.3404490351676941\n",
      "E loss:  0.5897708535194397\n",
      "G loss: 0.3061475157737732\n",
      "E loss:  0.5807809233665466\n",
      "G loss: 0.2969915270805359\n",
      "E loss:  0.5794205665588379\n",
      "G loss: 0.2642008662223816\n",
      "E loss:  0.5890196561813354\n",
      "G loss: 0.2714619040489197\n",
      "Training Model  ...\n",
      "E loss:  0.5554724335670471\n",
      "G loss: 0.27524930238723755\n",
      "E loss:  0.559384286403656\n",
      "G loss: 0.2806307077407837\n",
      "E loss:  0.5562131404876709\n",
      "G loss: 0.2909187078475952\n",
      "E loss:  0.5502092838287354\n",
      "G loss: 0.2951630651950836\n",
      "E loss:  0.556256115436554\n",
      "G loss: 0.28022682666778564\n",
      "Training Model  ...\n",
      "E loss:  0.5889026522636414\n",
      "G loss: 0.28963154554367065\n",
      "E loss:  0.5831413865089417\n",
      "G loss: 0.3389730155467987\n",
      "E loss:  0.5795367956161499\n",
      "G loss: 0.3493674695491791\n",
      "E loss:  0.5727596282958984\n",
      "G loss: 0.37025314569473267\n",
      "E loss:  0.5637279152870178\n",
      "G loss: 0.3986720144748688\n",
      "Training Model  ...\n",
      "E loss:  0.592783510684967\n",
      "G loss: 0.3728220462799072\n",
      "E loss:  0.58661949634552\n",
      "G loss: 0.3567715883255005\n",
      "E loss:  0.5732887983322144\n",
      "G loss: 0.2964361608028412\n",
      "E loss:  0.5783731937408447\n",
      "G loss: 0.2606322169303894\n",
      "E loss:  0.5818212628364563\n",
      "G loss: 0.25429385900497437\n",
      "Training Model  ...\n",
      "E loss:  0.6231241226196289\n",
      "G loss: 0.2669808268547058\n",
      "E loss:  0.6073428988456726\n",
      "G loss: 0.2816419303417206\n",
      "E loss:  0.6029779314994812\n",
      "G loss: 0.2813533842563629\n",
      "E loss:  0.6073836088180542\n",
      "G loss: 0.28810909390449524\n",
      "E loss:  0.6038427948951721\n",
      "G loss: 0.3503386974334717\n",
      "Training Model  ...\n",
      "E loss:  0.5791847109794617\n",
      "G loss: 0.33195289969444275\n",
      "E loss:  0.5722508430480957\n",
      "G loss: 0.31397566199302673\n",
      "E loss:  0.5744396448135376\n",
      "G loss: 0.29966267943382263\n",
      "E loss:  0.5635572671890259\n",
      "G loss: 0.3322944641113281\n",
      "E loss:  0.5400515794754028\n",
      "G loss: 0.29211264848709106\n",
      "Training Model  ...\n",
      "E loss:  0.5840059518814087\n",
      "G loss: 0.2968622148036957\n",
      "E loss:  0.5888882875442505\n",
      "G loss: 0.2932688295841217\n",
      "E loss:  0.5837860703468323\n",
      "G loss: 0.30294832587242126\n",
      "E loss:  0.5905629396438599\n",
      "G loss: 0.2815704643726349\n",
      "E loss:  0.5815473198890686\n",
      "G loss: 0.28573182225227356\n",
      "Training Model  ...\n",
      "E loss:  0.5973863005638123\n",
      "G loss: 0.29174700379371643\n",
      "E loss:  0.5959067940711975\n",
      "G loss: 0.3139876127243042\n",
      "E loss:  0.591157078742981\n",
      "G loss: 0.2971670925617218\n",
      "E loss:  0.592949390411377\n",
      "G loss: 0.32739487290382385\n",
      "E loss:  0.5928087830543518\n",
      "G loss: 0.3638228178024292\n",
      "Training Model  ...\n",
      "E loss:  0.607342004776001\n",
      "G loss: 0.33674106001853943\n",
      "E loss:  0.6009209752082825\n",
      "G loss: 0.31927919387817383\n",
      "E loss:  0.6013181805610657\n",
      "G loss: 0.273155152797699\n",
      "E loss:  0.5942075252532959\n",
      "G loss: 0.27868014574050903\n",
      "E loss:  0.5979071259498596\n",
      "G loss: 0.24467960000038147\n",
      "Training Model  ...\n",
      "E loss:  0.6261115670204163\n",
      "G loss: 0.24096675217151642\n",
      "E loss:  0.6092618107795715\n",
      "G loss: 0.30745115876197815\n",
      "E loss:  0.608727216720581\n",
      "G loss: 0.3463951647281647\n",
      "E loss:  0.6098544597625732\n",
      "G loss: 0.4130852520465851\n",
      "E loss:  0.6138443350791931\n",
      "G loss: 0.402458131313324\n",
      "Training Model  ...\n",
      "E loss:  0.6041929721832275\n",
      "G loss: 0.37462905049324036\n",
      "E loss:  0.599702000617981\n",
      "G loss: 0.3599257171154022\n",
      "E loss:  0.5917456746101379\n",
      "G loss: 0.34436270594596863\n",
      "E loss:  0.5799684524536133\n",
      "G loss: 0.32287412881851196\n",
      "E loss:  0.5854735970497131\n",
      "G loss: 0.3344574272632599\n",
      "Training Model  ...\n",
      "E loss:  0.6397363543510437\n",
      "G loss: 0.3260127902030945\n",
      "E loss:  0.6317917704582214\n",
      "G loss: 0.3125324547290802\n",
      "E loss:  0.6413501501083374\n",
      "G loss: 0.3034592270851135\n",
      "E loss:  0.6315892338752747\n",
      "G loss: 0.3414759337902069\n",
      "E loss:  0.6205230355262756\n",
      "G loss: 0.33181682229042053\n",
      "Training Model  ...\n",
      "E loss:  0.6227192878723145\n",
      "G loss: 0.3436732590198517\n",
      "E loss:  0.621825098991394\n",
      "G loss: 0.3191419839859009\n",
      "E loss:  0.6106773614883423\n",
      "G loss: 0.3131972551345825\n",
      "E loss:  0.6231234073638916\n",
      "G loss: 0.3045477271080017\n",
      "E loss:  0.6322067975997925\n",
      "G loss: 0.2955588400363922\n",
      "Training Model  ...\n",
      "E loss:  0.5645342469215393\n",
      "G loss: 0.2737235426902771\n",
      "E loss:  0.5722658038139343\n",
      "G loss: 0.3227193355560303\n",
      "E loss:  0.5696393251419067\n",
      "G loss: 0.3205548822879791\n",
      "E loss:  0.5756756663322449\n",
      "G loss: 0.3794754147529602\n",
      "E loss:  0.591928243637085\n",
      "G loss: 0.37823328375816345\n",
      "Training Model  ...\n",
      "E loss:  0.5799297094345093\n",
      "G loss: 0.3504638969898224\n",
      "E loss:  0.5800097584724426\n",
      "G loss: 0.3361661732196808\n",
      "E loss:  0.5641718506813049\n",
      "G loss: 0.34853336215019226\n",
      "E loss:  0.5751257538795471\n",
      "G loss: 0.2986646890640259\n",
      "E loss:  0.5737025737762451\n",
      "G loss: 0.36433765292167664\n",
      "Training Model  ...\n",
      "E loss:  0.5468834042549133\n",
      "G loss: 0.34190812706947327\n",
      "E loss:  0.536558210849762\n",
      "G loss: 0.35839182138442993\n",
      "E loss:  0.5358849763870239\n",
      "G loss: 0.31085431575775146\n",
      "E loss:  0.5477692484855652\n",
      "G loss: 0.32239240407943726\n",
      "E loss:  0.556498646736145\n",
      "G loss: 0.3010355830192566\n",
      "Training Model  ...\n",
      "E loss:  0.6292292475700378\n",
      "G loss: 0.3052193522453308\n",
      "E loss:  0.6320526003837585\n",
      "G loss: 0.36124321818351746\n",
      "E loss:  0.635642409324646\n",
      "G loss: 0.330745130777359\n",
      "E loss:  0.6152046918869019\n",
      "G loss: 0.38524729013442993\n",
      "E loss:  0.599113941192627\n",
      "G loss: 0.35350340604782104\n",
      "Training Model  ...\n",
      "E loss:  0.6331436038017273\n",
      "G loss: 0.38101744651794434\n",
      "E loss:  0.6311305165290833\n",
      "G loss: 0.3590022623538971\n",
      "E loss:  0.632739245891571\n",
      "G loss: 0.33643198013305664\n",
      "E loss:  0.6184703707695007\n",
      "G loss: 0.314459890127182\n",
      "E loss:  0.6169922947883606\n",
      "G loss: 0.3543887138366699\n",
      "Training Model  ...\n",
      "E loss:  0.6058036684989929\n",
      "G loss: 0.29511943459510803\n",
      "E loss:  0.5927811861038208\n",
      "G loss: 0.3343500792980194\n",
      "E loss:  0.5937501192092896\n",
      "G loss: 0.3413611948490143\n",
      "E loss:  0.6043921709060669\n",
      "G loss: 0.3683411180973053\n",
      "E loss:  0.6009739637374878\n",
      "G loss: 0.36026105284690857\n",
      "Training Model  ...\n",
      "E loss:  0.6263254880905151\n",
      "G loss: 0.3210068345069885\n",
      "E loss:  0.612623929977417\n",
      "G loss: 0.3078033924102783\n",
      "E loss:  0.6081485152244568\n",
      "G loss: 0.28146326541900635\n",
      "E loss:  0.619439959526062\n",
      "G loss: 0.2804420292377472\n",
      "E loss:  0.6115325093269348\n",
      "G loss: 0.22700315713882446\n",
      "Training Model  ...\n",
      "E loss:  0.5838205814361572\n",
      "G loss: 0.2732517719268799\n",
      "E loss:  0.5844764113426208\n",
      "G loss: 0.2797777056694031\n",
      "E loss:  0.5729296803474426\n",
      "G loss: 0.2773524522781372\n",
      "E loss:  0.580715000629425\n",
      "G loss: 0.3110073208808899\n",
      "E loss:  0.5780723690986633\n",
      "G loss: 0.31916266679763794\n",
      "Training Model  ...\n",
      "E loss:  0.5966736078262329\n",
      "G loss: 0.31269729137420654\n",
      "E loss:  0.5869593024253845\n",
      "G loss: 0.29836463928222656\n",
      "E loss:  0.5891468524932861\n",
      "G loss: 0.30815067887306213\n",
      "E loss:  0.5898115634918213\n",
      "G loss: 0.33929771184921265\n",
      "E loss:  0.5882083773612976\n",
      "G loss: 0.3279433250427246\n",
      "Training Model  ...\n",
      "E loss:  0.5723882913589478\n",
      "G loss: 0.34290313720703125\n",
      "E loss:  0.5682218670845032\n",
      "G loss: 0.360769659280777\n",
      "E loss:  0.5813456773757935\n",
      "G loss: 0.31724587082862854\n",
      "E loss:  0.580363392829895\n",
      "G loss: 0.33592283725738525\n",
      "E loss:  0.5860674977302551\n",
      "G loss: 0.35797253251075745\n",
      "Training Model  ...\n",
      "E loss:  0.629840075969696\n",
      "G loss: 0.3212500810623169\n",
      "E loss:  0.6351504921913147\n",
      "G loss: 0.32151225209236145\n",
      "E loss:  0.6397151350975037\n",
      "G loss: 0.2987230718135834\n",
      "E loss:  0.6380922794342041\n",
      "G loss: 0.2957562804222107\n",
      "E loss:  0.6429545879364014\n",
      "G loss: 0.2903539538383484\n",
      "Training Model  ...\n",
      "E loss:  0.6452481746673584\n",
      "G loss: 0.29064881801605225\n",
      "E loss:  0.6427140831947327\n",
      "G loss: 0.30573973059654236\n",
      "E loss:  0.6409410834312439\n",
      "G loss: 0.3160272240638733\n",
      "E loss:  0.6448603272438049\n",
      "G loss: 0.3280310332775116\n",
      "E loss:  0.6384557485580444\n",
      "G loss: 0.3562842905521393\n",
      "Training Model  ...\n",
      "E loss:  0.6008641123771667\n",
      "G loss: 0.3847157955169678\n",
      "E loss:  0.6032703518867493\n",
      "G loss: 0.3236813247203827\n",
      "E loss:  0.6084918975830078\n",
      "G loss: 0.3006398677825928\n",
      "E loss:  0.621722936630249\n",
      "G loss: 0.2546466886997223\n",
      "E loss:  0.625926673412323\n",
      "G loss: 0.22512197494506836\n",
      "Training Model  ...\n",
      "E loss:  0.5935408473014832\n",
      "G loss: 0.26283910870552063\n",
      "E loss:  0.5927179455757141\n",
      "G loss: 0.2700997591018677\n",
      "E loss:  0.596110463142395\n",
      "G loss: 0.2670522630214691\n",
      "E loss:  0.5830765962600708\n",
      "G loss: 0.25181251764297485\n",
      "E loss:  0.5841172337532043\n",
      "G loss: 0.28759151697158813\n",
      "Training Model  ...\n",
      "E loss:  0.5566957592964172\n",
      "G loss: 0.29198116064071655\n",
      "E loss:  0.5583672523498535\n",
      "G loss: 0.2982640266418457\n",
      "E loss:  0.555942714214325\n",
      "G loss: 0.29894641041755676\n",
      "E loss:  0.5608053207397461\n",
      "G loss: 0.30292341113090515\n",
      "E loss:  0.5627528429031372\n",
      "G loss: 0.34222179651260376\n",
      "Training Model  ...\n",
      "E loss:  0.5753494501113892\n",
      "G loss: 0.31651586294174194\n",
      "E loss:  0.5698989629745483\n",
      "G loss: 0.29168739914894104\n",
      "E loss:  0.5623905658721924\n",
      "G loss: 0.29420459270477295\n",
      "E loss:  0.5593603253364563\n",
      "G loss: 0.29959020018577576\n",
      "E loss:  0.5583166480064392\n",
      "G loss: 0.2959226071834564\n",
      "Training Model  ...\n",
      "E loss:  0.6625816822052002\n",
      "G loss: 0.3294476866722107\n",
      "E loss:  0.6449428200721741\n",
      "G loss: 0.331969290971756\n",
      "E loss:  0.654625654220581\n",
      "G loss: 0.3725447356700897\n",
      "E loss:  0.6610078811645508\n",
      "G loss: 0.38836780190467834\n",
      "E loss:  0.6548103094100952\n",
      "G loss: 0.43119925260543823\n",
      "Training Model  ...\n",
      "E loss:  0.6437259316444397\n",
      "G loss: 0.3920043706893921\n",
      "E loss:  0.6388941407203674\n",
      "G loss: 0.3454444110393524\n",
      "E loss:  0.6306370496749878\n",
      "G loss: 0.3156448006629944\n",
      "E loss:  0.623511791229248\n",
      "G loss: 0.29243922233581543\n",
      "E loss:  0.6244701147079468\n",
      "G loss: 0.27508944272994995\n",
      "Training Model  ...\n",
      "E loss:  0.5879967212677002\n",
      "G loss: 0.2764919400215149\n",
      "E loss:  0.586452841758728\n",
      "G loss: 0.29711857438087463\n",
      "E loss:  0.5812970399856567\n",
      "G loss: 0.2747756540775299\n",
      "E loss:  0.5877896547317505\n",
      "G loss: 0.2993485629558563\n",
      "E loss:  0.5774167776107788\n",
      "G loss: 0.3525530695915222\n",
      "Training Model  ...\n",
      "E loss:  0.6705501079559326\n",
      "G loss: 0.3292710781097412\n",
      "E loss:  0.6755582690238953\n",
      "G loss: 0.3308763802051544\n",
      "E loss:  0.6673522591590881\n",
      "G loss: 0.3270825743675232\n",
      "E loss:  0.6694566011428833\n",
      "G loss: 0.311362087726593\n",
      "E loss:  0.6611008644104004\n",
      "G loss: 0.28730660676956177\n",
      "Training Model  ...\n",
      "E loss:  0.5978726744651794\n",
      "G loss: 0.28935083746910095\n",
      "E loss:  0.6038408875465393\n",
      "G loss: 0.29052338004112244\n",
      "E loss:  0.6079134941101074\n",
      "G loss: 0.27254223823547363\n",
      "E loss:  0.5899906158447266\n",
      "G loss: 0.29100513458251953\n",
      "E loss:  0.5978397130966187\n",
      "G loss: 0.2730783522129059\n",
      "Training Model  ...\n",
      "E loss:  0.5667244791984558\n",
      "G loss: 0.29521581530570984\n",
      "E loss:  0.5751250386238098\n",
      "G loss: 0.2876804769039154\n",
      "E loss:  0.5653624534606934\n",
      "G loss: 0.2660265266895294\n",
      "E loss:  0.5622743964195251\n",
      "G loss: 0.2758832573890686\n",
      "E loss:  0.5651144981384277\n",
      "G loss: 0.26481711864471436\n",
      "Training Model  ...\n",
      "E loss:  0.5918973684310913\n",
      "G loss: 0.2855490446090698\n",
      "E loss:  0.5873867869377136\n",
      "G loss: 0.2763715386390686\n",
      "E loss:  0.5816640853881836\n",
      "G loss: 0.264128714799881\n",
      "E loss:  0.5804674029350281\n",
      "G loss: 0.24228134751319885\n",
      "E loss:  0.5721489787101746\n",
      "G loss: 0.2524176239967346\n",
      "Training Model  ...\n",
      "E loss:  0.5377800464630127\n",
      "G loss: 0.2630872130393982\n",
      "E loss:  0.5330600142478943\n",
      "G loss: 0.2783174216747284\n",
      "E loss:  0.52927565574646\n",
      "G loss: 0.2893933951854706\n",
      "E loss:  0.5358750224113464\n",
      "G loss: 0.30937522649765015\n",
      "E loss:  0.5399958491325378\n",
      "G loss: 0.34677988290786743\n",
      "Training Model  ...\n",
      "E loss:  0.5789608955383301\n",
      "G loss: 0.2866423726081848\n",
      "E loss:  0.5757768154144287\n",
      "G loss: 0.31191331148147583\n",
      "E loss:  0.5630455017089844\n",
      "G loss: 0.3109268844127655\n",
      "E loss:  0.5579676628112793\n",
      "G loss: 0.2778335511684418\n",
      "E loss:  0.5620677471160889\n",
      "G loss: 0.26767969131469727\n",
      "Training Model  ...\n",
      "E loss:  0.5664390325546265\n",
      "G loss: 0.30455267429351807\n",
      "E loss:  0.5679572224617004\n",
      "G loss: 0.33839160203933716\n",
      "E loss:  0.568292498588562\n",
      "G loss: 0.3210221529006958\n",
      "E loss:  0.5640951991081238\n",
      "G loss: 0.35819411277770996\n",
      "E loss:  0.5742409825325012\n",
      "G loss: 0.37741994857788086\n",
      "Training Model  ...\n",
      "E loss:  0.6236608028411865\n",
      "G loss: 0.3584338426589966\n",
      "E loss:  0.6236974000930786\n",
      "G loss: 0.3599295914173126\n",
      "E loss:  0.6302029490470886\n",
      "G loss: 0.3536885380744934\n",
      "E loss:  0.6250083446502686\n",
      "G loss: 0.37020784616470337\n",
      "E loss:  0.6186803579330444\n",
      "G loss: 0.3451373279094696\n",
      "Training Model  ...\n",
      "E loss:  0.5950461030006409\n",
      "G loss: 0.3182646930217743\n",
      "E loss:  0.5878695845603943\n",
      "G loss: 0.32080885767936707\n",
      "E loss:  0.5988560318946838\n",
      "G loss: 0.31056639552116394\n",
      "E loss:  0.5872297286987305\n",
      "G loss: 0.3008802533149719\n",
      "E loss:  0.5988914966583252\n",
      "G loss: 0.2997697591781616\n",
      "Training Model  ...\n",
      "E loss:  0.6151258945465088\n",
      "G loss: 0.2855362296104431\n",
      "E loss:  0.6250998973846436\n",
      "G loss: 0.28794461488723755\n",
      "E loss:  0.6140047907829285\n",
      "G loss: 0.3051462173461914\n",
      "E loss:  0.6229954957962036\n",
      "G loss: 0.35455793142318726\n",
      "E loss:  0.6196552515029907\n",
      "G loss: 0.31808096170425415\n",
      "Training Model  ...\n",
      "E loss:  0.5649623870849609\n",
      "G loss: 0.3179472088813782\n",
      "E loss:  0.5555812120437622\n",
      "G loss: 0.32350558042526245\n",
      "E loss:  0.5514667630195618\n",
      "G loss: 0.3496277928352356\n",
      "E loss:  0.5702226758003235\n",
      "G loss: 0.31369051337242126\n",
      "E loss:  0.572484016418457\n",
      "G loss: 0.33007824420928955\n",
      "Training Model  ...\n",
      "E loss:  0.6483439207077026\n",
      "G loss: 0.30196401476860046\n",
      "E loss:  0.6343769431114197\n",
      "G loss: 0.3333755135536194\n",
      "E loss:  0.6345001459121704\n",
      "G loss: 0.3114616274833679\n",
      "E loss:  0.6361582279205322\n",
      "G loss: 0.2614927291870117\n",
      "E loss:  0.6332369446754456\n",
      "G loss: 0.3030985891819\n",
      "Training Model  ...\n",
      "E loss:  0.5882531404495239\n",
      "G loss: 0.28105464577674866\n",
      "E loss:  0.5808305144309998\n",
      "G loss: 0.3024517595767975\n",
      "E loss:  0.5726230144500732\n",
      "G loss: 0.31676796078681946\n",
      "E loss:  0.5810062885284424\n",
      "G loss: 0.31622907519340515\n",
      "E loss:  0.5806726813316345\n",
      "G loss: 0.30739182233810425\n",
      "Training Model  ...\n",
      "E loss:  0.6588670611381531\n",
      "G loss: 0.29900795221328735\n",
      "E loss:  0.6546944975852966\n",
      "G loss: 0.3476307988166809\n",
      "E loss:  0.6555149555206299\n",
      "G loss: 0.3495447039604187\n",
      "E loss:  0.6496482491493225\n",
      "G loss: 0.3745829463005066\n",
      "E loss:  0.6467182040214539\n",
      "G loss: 0.35522735118865967\n",
      "Training Model  ...\n",
      "E loss:  0.5554308295249939\n",
      "G loss: 0.39186593890190125\n",
      "E loss:  0.5610594153404236\n",
      "G loss: 0.32417765259742737\n",
      "E loss:  0.5563234090805054\n",
      "G loss: 0.27302974462509155\n",
      "E loss:  0.5582089424133301\n",
      "G loss: 0.2408253252506256\n",
      "E loss:  0.5662816762924194\n",
      "G loss: 0.22785376012325287\n",
      "Training Model  ...\n",
      "E loss:  0.5360655188560486\n",
      "G loss: 0.2454618364572525\n",
      "E loss:  0.5398489832878113\n",
      "G loss: 0.27608516812324524\n",
      "E loss:  0.5300846099853516\n",
      "G loss: 0.2869214117527008\n",
      "E loss:  0.5310123562812805\n",
      "G loss: 0.3394136428833008\n",
      "E loss:  0.5313608050346375\n",
      "G loss: 0.3621762990951538\n",
      "Training Model  ...\n",
      "E loss:  0.6356090307235718\n",
      "G loss: 0.3192528188228607\n",
      "E loss:  0.6412720680236816\n",
      "G loss: 0.33106887340545654\n",
      "E loss:  0.6299266815185547\n",
      "G loss: 0.31531989574432373\n",
      "E loss:  0.6384836435317993\n",
      "G loss: 0.30943936109542847\n",
      "E loss:  0.6288021206855774\n",
      "G loss: 0.3175342082977295\n",
      "Training Model  ...\n",
      "E loss:  0.5981488227844238\n",
      "G loss: 0.31705743074417114\n",
      "E loss:  0.6057295799255371\n",
      "G loss: 0.32231849431991577\n",
      "E loss:  0.6013404130935669\n",
      "G loss: 0.3069310784339905\n",
      "E loss:  0.5943366289138794\n",
      "G loss: 0.2913822829723358\n",
      "E loss:  0.6042470932006836\n",
      "G loss: 0.30786770582199097\n",
      "Training Model  ...\n",
      "E loss:  0.5625190138816833\n",
      "G loss: 0.27911725640296936\n",
      "E loss:  0.5767340660095215\n",
      "G loss: 0.2721502482891083\n",
      "E loss:  0.5660044550895691\n",
      "G loss: 0.30890756845474243\n",
      "E loss:  0.5555080771446228\n",
      "G loss: 0.2602982819080353\n",
      "E loss:  0.562643826007843\n",
      "G loss: 0.2704538404941559\n",
      "Training Model  ...\n",
      "E loss:  0.5957605242729187\n",
      "G loss: 0.28862741589546204\n",
      "E loss:  0.6015050411224365\n",
      "G loss: 0.30469363927841187\n",
      "E loss:  0.6081821322441101\n",
      "G loss: 0.2871043384075165\n",
      "E loss:  0.608477771282196\n",
      "G loss: 0.28429684042930603\n",
      "E loss:  0.6088820099830627\n",
      "G loss: 0.3120134174823761\n",
      "Training Model  ...\n",
      "E loss:  0.5640968084335327\n",
      "G loss: 0.2707231640815735\n",
      "E loss:  0.5738664269447327\n",
      "G loss: 0.28852328658103943\n",
      "E loss:  0.5517297387123108\n",
      "G loss: 0.26222673058509827\n",
      "E loss:  0.5524042248725891\n",
      "G loss: 0.26711413264274597\n",
      "E loss:  0.545870304107666\n",
      "G loss: 0.24742305278778076\n",
      "Training Model  ...\n",
      "E loss:  0.5682038068771362\n",
      "G loss: 0.30054017901420593\n",
      "E loss:  0.571783721446991\n",
      "G loss: 0.3193233013153076\n",
      "E loss:  0.5851477980613708\n",
      "G loss: 0.34976381063461304\n",
      "E loss:  0.5764783620834351\n",
      "G loss: 0.35571038722991943\n",
      "E loss:  0.5639901757240295\n",
      "G loss: 0.3568103015422821\n",
      "Training Model  ...\n",
      "E loss:  0.6291314959526062\n",
      "G loss: 0.3592508137226105\n",
      "E loss:  0.6303386092185974\n",
      "G loss: 0.3098923861980438\n",
      "E loss:  0.6358039379119873\n",
      "G loss: 0.26949843764305115\n",
      "E loss:  0.6245497465133667\n",
      "G loss: 0.25626710057258606\n",
      "E loss:  0.6295841336250305\n",
      "G loss: 0.23999689519405365\n",
      "Training Model  ...\n",
      "E loss:  0.6214694976806641\n",
      "G loss: 0.22879871726036072\n",
      "E loss:  0.6129025220870972\n",
      "G loss: 0.3103593587875366\n",
      "E loss:  0.5985409021377563\n",
      "G loss: 0.35927289724349976\n",
      "E loss:  0.5890028476715088\n",
      "G loss: 0.43228578567504883\n",
      "E loss:  0.5946542024612427\n",
      "G loss: 0.4304053485393524\n",
      "Training Model  ...\n",
      "E loss:  0.6565197110176086\n",
      "G loss: 0.3796427249908447\n",
      "E loss:  0.6244676113128662\n",
      "G loss: 0.3385925889015198\n",
      "E loss:  0.6147314310073853\n",
      "G loss: 0.26337796449661255\n",
      "E loss:  0.6149829030036926\n",
      "G loss: 0.22583594918251038\n",
      "E loss:  0.6076691746711731\n",
      "G loss: 0.19638681411743164\n",
      "Training Model  ...\n",
      "E loss:  0.6389945149421692\n",
      "G loss: 0.19465091824531555\n",
      "E loss:  0.6308361291885376\n",
      "G loss: 0.25747230648994446\n",
      "E loss:  0.6083837747573853\n",
      "G loss: 0.2742084562778473\n",
      "E loss:  0.6173593401908875\n",
      "G loss: 0.2989581823348999\n",
      "E loss:  0.6202917695045471\n",
      "G loss: 0.35807761549949646\n",
      "Training Model  ...\n",
      "E loss:  0.5849326252937317\n",
      "G loss: 0.2933788299560547\n",
      "E loss:  0.5780162811279297\n",
      "G loss: 0.2876393496990204\n",
      "E loss:  0.5798981785774231\n",
      "G loss: 0.25401484966278076\n",
      "E loss:  0.5784704089164734\n",
      "G loss: 0.2269432544708252\n",
      "E loss:  0.5794677138328552\n",
      "G loss: 0.18669019639492035\n",
      "Training Model  ...\n",
      "E loss:  0.6098741888999939\n",
      "G loss: 0.21755695343017578\n",
      "E loss:  0.6050837635993958\n",
      "G loss: 0.29766610264778137\n",
      "E loss:  0.6035139560699463\n",
      "G loss: 0.3258511424064636\n",
      "E loss:  0.5962886214256287\n",
      "G loss: 0.36116576194763184\n",
      "E loss:  0.5971407890319824\n",
      "G loss: 0.3986809253692627\n",
      "Training Model  ...\n",
      "E loss:  0.6221860647201538\n",
      "G loss: 0.36478230357170105\n",
      "E loss:  0.5982050895690918\n",
      "G loss: 0.32442790269851685\n",
      "E loss:  0.5775018334388733\n",
      "G loss: 0.24036096036434174\n",
      "E loss:  0.5743246078491211\n",
      "G loss: 0.17618462443351746\n",
      "E loss:  0.5802631378173828\n",
      "G loss: 0.15916211903095245\n",
      "Training Model  ...\n",
      "E loss:  0.6282466053962708\n",
      "G loss: 0.18299230933189392\n",
      "E loss:  0.6200211048126221\n",
      "G loss: 0.26080331206321716\n",
      "E loss:  0.6030009388923645\n",
      "G loss: 0.29480814933776855\n",
      "E loss:  0.5987829566001892\n",
      "G loss: 0.34740063548088074\n",
      "E loss:  0.5989017486572266\n",
      "G loss: 0.41615957021713257\n",
      "Training Model  ...\n",
      "E loss:  0.6784716248512268\n",
      "G loss: 0.38437575101852417\n",
      "E loss:  0.6838485598564148\n",
      "G loss: 0.36832132935523987\n",
      "E loss:  0.6860910654067993\n",
      "G loss: 0.36569806933403015\n",
      "E loss:  0.6869632005691528\n",
      "G loss: 0.2988436222076416\n",
      "E loss:  0.687511682510376\n",
      "G loss: 0.3022390305995941\n",
      "Training Model  ...\n",
      "E loss:  0.5804946422576904\n",
      "G loss: 0.26717883348464966\n",
      "E loss:  0.571736216545105\n",
      "G loss: 0.29412326216697693\n",
      "E loss:  0.5826770067214966\n",
      "G loss: 0.2975878119468689\n",
      "E loss:  0.5781586170196533\n",
      "G loss: 0.30981308221817017\n",
      "E loss:  0.5729081034660339\n",
      "G loss: 0.28174951672554016\n",
      "Training Model  ...\n",
      "E loss:  0.6058950424194336\n",
      "G loss: 0.3067505359649658\n",
      "E loss:  0.6159012317657471\n",
      "G loss: 0.2874920964241028\n",
      "E loss:  0.6144790053367615\n",
      "G loss: 0.27347636222839355\n",
      "E loss:  0.6132122874259949\n",
      "G loss: 0.24441656470298767\n",
      "E loss:  0.6058406233787537\n",
      "G loss: 0.2228316068649292\n",
      "Training Model  ...\n",
      "E loss:  0.6015124320983887\n",
      "G loss: 0.23091042041778564\n",
      "E loss:  0.5978729724884033\n",
      "G loss: 0.23304817080497742\n",
      "E loss:  0.6063414812088013\n",
      "G loss: 0.22844569385051727\n",
      "E loss:  0.6062352657318115\n",
      "G loss: 0.24038314819335938\n",
      "E loss:  0.6031076908111572\n",
      "G loss: 0.24559587240219116\n",
      "Training Model  ...\n",
      "E loss:  0.6726412177085876\n",
      "G loss: 0.2630263566970825\n",
      "E loss:  0.6731715798377991\n",
      "G loss: 0.2685668170452118\n",
      "E loss:  0.6707810163497925\n",
      "G loss: 0.3703598380088806\n",
      "E loss:  0.6669827699661255\n",
      "G loss: 0.3876728415489197\n",
      "E loss:  0.6820852756500244\n",
      "G loss: 0.36614733934402466\n",
      "Training Model  ...\n",
      "E loss:  0.631494402885437\n",
      "G loss: 0.35907602310180664\n",
      "E loss:  0.6258732080459595\n",
      "G loss: 0.2705383598804474\n",
      "E loss:  0.6190643310546875\n",
      "G loss: 0.22442105412483215\n",
      "E loss:  0.6126464605331421\n",
      "G loss: 0.20042169094085693\n",
      "E loss:  0.6001062989234924\n",
      "G loss: 0.19854430854320526\n",
      "Training Model  ...\n",
      "E loss:  0.6616687178611755\n",
      "G loss: 0.2394121289253235\n",
      "E loss:  0.6451990008354187\n",
      "G loss: 0.22322040796279907\n",
      "E loss:  0.631057858467102\n",
      "G loss: 0.28752413392066956\n",
      "E loss:  0.6184089183807373\n",
      "G loss: 0.34441617131233215\n",
      "E loss:  0.6212908625602722\n",
      "G loss: 0.3409218192100525\n",
      "Training Model  ...\n",
      "E loss:  0.5882397890090942\n",
      "G loss: 0.3090217113494873\n",
      "E loss:  0.5966858267784119\n",
      "G loss: 0.3084787428379059\n",
      "E loss:  0.599305272102356\n",
      "G loss: 0.25749215483665466\n",
      "E loss:  0.6057213544845581\n",
      "G loss: 0.21552596986293793\n",
      "E loss:  0.6170902252197266\n",
      "G loss: 0.2218267321586609\n",
      "Training Model  ...\n",
      "E loss:  0.5887397527694702\n",
      "G loss: 0.23122476041316986\n",
      "E loss:  0.5800437331199646\n",
      "G loss: 0.23747675120830536\n",
      "E loss:  0.5731706023216248\n",
      "G loss: 0.2680880129337311\n",
      "E loss:  0.5639193654060364\n",
      "G loss: 0.3040720224380493\n",
      "E loss:  0.5566537380218506\n",
      "G loss: 0.3038519024848938\n",
      "Training Model  ...\n",
      "E loss:  0.5875308513641357\n",
      "G loss: 0.2960212826728821\n",
      "E loss:  0.577903151512146\n",
      "G loss: 0.2835342586040497\n",
      "E loss:  0.5841235518455505\n",
      "G loss: 0.25269201397895813\n",
      "E loss:  0.5801840424537659\n",
      "G loss: 0.2672690153121948\n",
      "E loss:  0.5742993950843811\n",
      "G loss: 0.2584880590438843\n",
      "Training Model  ...\n",
      "E loss:  0.5753575563430786\n",
      "G loss: 0.27862876653671265\n",
      "E loss:  0.5720518827438354\n",
      "G loss: 0.29898369312286377\n",
      "E loss:  0.558749794960022\n",
      "G loss: 0.3222818076610565\n",
      "E loss:  0.5690939426422119\n",
      "G loss: 0.32229387760162354\n",
      "E loss:  0.5623520612716675\n",
      "G loss: 0.3337499499320984\n",
      "Training Model  ...\n",
      "E loss:  0.5849388837814331\n",
      "G loss: 0.3207077085971832\n",
      "E loss:  0.5752976536750793\n",
      "G loss: 0.30317094922065735\n",
      "E loss:  0.5795190334320068\n",
      "G loss: 0.2550984025001526\n",
      "E loss:  0.5792537927627563\n",
      "G loss: 0.24062632024288177\n",
      "E loss:  0.5907593369483948\n",
      "G loss: 0.21643416583538055\n",
      "Training Model  ...\n",
      "E loss:  0.5883361101150513\n",
      "G loss: 0.24334324896335602\n",
      "E loss:  0.5748451352119446\n",
      "G loss: 0.27187955379486084\n",
      "E loss:  0.5794288516044617\n",
      "G loss: 0.26478832960128784\n",
      "E loss:  0.5752509832382202\n",
      "G loss: 0.29846838116645813\n",
      "E loss:  0.5869815945625305\n",
      "G loss: 0.2647564709186554\n",
      "Training Model  ...\n",
      "E loss:  0.5474683046340942\n",
      "G loss: 0.25288453698158264\n",
      "E loss:  0.5412459969520569\n",
      "G loss: 0.2605280876159668\n",
      "E loss:  0.5397136807441711\n",
      "G loss: 0.25898870825767517\n",
      "E loss:  0.5401571989059448\n",
      "G loss: 0.22175711393356323\n",
      "E loss:  0.5496509671211243\n",
      "G loss: 0.2513850927352905\n",
      "Training Model  ...\n",
      "E loss:  0.5966169238090515\n",
      "G loss: 0.2535821199417114\n",
      "E loss:  0.5888032913208008\n",
      "G loss: 0.2599412798881531\n",
      "E loss:  0.582802414894104\n",
      "G loss: 0.25201642513275146\n",
      "E loss:  0.5888760089874268\n",
      "G loss: 0.2421073317527771\n",
      "E loss:  0.5859723687171936\n",
      "G loss: 0.30868932604789734\n",
      "Training Model  ...\n",
      "E loss:  0.5682084560394287\n",
      "G loss: 0.2767036557197571\n",
      "E loss:  0.5719987154006958\n",
      "G loss: 0.2408655285835266\n",
      "E loss:  0.568084180355072\n",
      "G loss: 0.2583862245082855\n",
      "E loss:  0.5833776593208313\n",
      "G loss: 0.23066343367099762\n",
      "E loss:  0.5853806138038635\n",
      "G loss: 0.20083080232143402\n",
      "Training Model  ...\n",
      "E loss:  0.5291919708251953\n",
      "G loss: 0.22795554995536804\n",
      "E loss:  0.5462597012519836\n",
      "G loss: 0.23343698680400848\n",
      "E loss:  0.5405749082565308\n",
      "G loss: 0.2652944326400757\n",
      "E loss:  0.5396628379821777\n",
      "G loss: 0.2847602367401123\n",
      "E loss:  0.5501030683517456\n",
      "G loss: 0.3108217716217041\n",
      "Training Model  ...\n",
      "E loss:  0.584373950958252\n",
      "G loss: 0.2843032479286194\n",
      "E loss:  0.5829098224639893\n",
      "G loss: 0.29481303691864014\n",
      "E loss:  0.577339768409729\n",
      "G loss: 0.2615615129470825\n",
      "E loss:  0.582015335559845\n",
      "G loss: 0.28090494871139526\n",
      "E loss:  0.5742899179458618\n",
      "G loss: 0.28925925493240356\n",
      "Training Model  ...\n",
      "E loss:  0.5524781942367554\n",
      "G loss: 0.27921468019485474\n",
      "E loss:  0.5436405539512634\n",
      "G loss: 0.28301164507865906\n",
      "E loss:  0.5406887531280518\n",
      "G loss: 0.27572935819625854\n",
      "E loss:  0.5464197993278503\n",
      "G loss: 0.2747120261192322\n",
      "E loss:  0.5461512804031372\n",
      "G loss: 0.29128769040107727\n",
      "Training Model  ...\n",
      "E loss:  0.5531020164489746\n",
      "G loss: 0.2621278762817383\n",
      "E loss:  0.5576310157775879\n",
      "G loss: 0.2550216317176819\n",
      "E loss:  0.5112492442131042\n",
      "G loss: 0.2031434327363968\n",
      "E loss:  0.5063169598579407\n",
      "G loss: 0.19971562922000885\n",
      "E loss:  0.502582311630249\n",
      "G loss: 0.18275634944438934\n",
      "Training Model  ...\n",
      "E loss:  0.5476246476173401\n",
      "G loss: 0.2224847972393036\n",
      "E loss:  0.5385710000991821\n",
      "G loss: 0.2968766987323761\n",
      "E loss:  0.5292342305183411\n",
      "G loss: 0.28657323122024536\n",
      "E loss:  0.5258862972259521\n",
      "G loss: 0.336179256439209\n",
      "E loss:  0.5331218838691711\n",
      "G loss: 0.38686272501945496\n",
      "Training Model  ...\n",
      "E loss:  0.5360059142112732\n",
      "G loss: 0.3511749804019928\n",
      "E loss:  0.5397834777832031\n",
      "G loss: 0.3083055913448334\n",
      "E loss:  0.5320162773132324\n",
      "G loss: 0.2579129636287689\n",
      "E loss:  0.520118772983551\n",
      "G loss: 0.22149533033370972\n",
      "E loss:  0.5140029191970825\n",
      "G loss: 0.20654070377349854\n",
      "Training Model  ...\n",
      "E loss:  0.5388219356536865\n",
      "G loss: 0.22356727719306946\n",
      "E loss:  0.5379865169525146\n",
      "G loss: 0.2361687868833542\n",
      "E loss:  0.5227388739585876\n",
      "G loss: 0.23223836719989777\n",
      "E loss:  0.5318319797515869\n",
      "G loss: 0.20666776597499847\n",
      "E loss:  0.5313777923583984\n",
      "G loss: 0.2438260167837143\n",
      "Training Model  ...\n",
      "E loss:  0.6036490201950073\n",
      "G loss: 0.28205397725105286\n",
      "E loss:  0.5870307683944702\n",
      "G loss: 0.32064202427864075\n",
      "E loss:  0.5783593058586121\n",
      "G loss: 0.3446902334690094\n",
      "E loss:  0.5737157464027405\n",
      "G loss: 0.36728066205978394\n",
      "E loss:  0.5747772455215454\n",
      "G loss: 0.41284072399139404\n",
      "Training Model  ...\n",
      "E loss:  0.5718115568161011\n",
      "G loss: 0.3652571141719818\n",
      "E loss:  0.5645063519477844\n",
      "G loss: 0.30924180150032043\n",
      "E loss:  0.5390735268592834\n",
      "G loss: 0.2750673294067383\n",
      "E loss:  0.5500984191894531\n",
      "G loss: 0.20907756686210632\n",
      "E loss:  0.5550699830055237\n",
      "G loss: 0.19596442580223083\n",
      "Training Model  ...\n",
      "E loss:  0.5584611296653748\n",
      "G loss: 0.208492249250412\n",
      "E loss:  0.5403734445571899\n",
      "G loss: 0.2346384972333908\n",
      "E loss:  0.5469162464141846\n",
      "G loss: 0.282942533493042\n",
      "E loss:  0.5436924695968628\n",
      "G loss: 0.29598867893218994\n",
      "E loss:  0.5518205165863037\n",
      "G loss: 0.31379878520965576\n",
      "Training Model  ...\n",
      "E loss:  0.5008836984634399\n",
      "G loss: 0.2901632785797119\n",
      "E loss:  0.5044990181922913\n",
      "G loss: 0.275237500667572\n",
      "E loss:  0.49586403369903564\n",
      "G loss: 0.27703142166137695\n",
      "E loss:  0.5172607898712158\n",
      "G loss: 0.24468743801116943\n",
      "E loss:  0.5088509321212769\n",
      "G loss: 0.2300649732351303\n",
      "Training Model  ...\n",
      "E loss:  0.5823992490768433\n",
      "G loss: 0.26185938715934753\n",
      "E loss:  0.5748060345649719\n",
      "G loss: 0.22669561207294464\n",
      "E loss:  0.5774111747741699\n",
      "G loss: 0.23829418420791626\n",
      "E loss:  0.5805479884147644\n",
      "G loss: 0.23769760131835938\n",
      "E loss:  0.584713339805603\n",
      "G loss: 0.24590647220611572\n",
      "Training Model  ...\n",
      "E loss:  0.5379979014396667\n",
      "G loss: 0.24612869322299957\n",
      "E loss:  0.543122410774231\n",
      "G loss: 0.28336361050605774\n",
      "E loss:  0.5486974716186523\n",
      "G loss: 0.27483072876930237\n",
      "E loss:  0.5534152984619141\n",
      "G loss: 0.3054816424846649\n",
      "E loss:  0.5518264174461365\n",
      "G loss: 0.27306708693504333\n",
      "Training Model  ...\n",
      "E loss:  0.5923919677734375\n",
      "G loss: 0.28735679388046265\n",
      "E loss:  0.5736233592033386\n",
      "G loss: 0.2681189179420471\n",
      "E loss:  0.5702586770057678\n",
      "G loss: 0.2807936668395996\n",
      "E loss:  0.5739801526069641\n",
      "G loss: 0.27747640013694763\n",
      "E loss:  0.5705438852310181\n",
      "G loss: 0.2769886553287506\n",
      "Training Model  ...\n",
      "E loss:  0.5487203598022461\n",
      "G loss: 0.2666971981525421\n",
      "E loss:  0.5583586692810059\n",
      "G loss: 0.24479487538337708\n",
      "E loss:  0.5585445165634155\n",
      "G loss: 0.2772735357284546\n",
      "E loss:  0.5669227838516235\n",
      "G loss: 0.2660406231880188\n",
      "E loss:  0.5572025775909424\n",
      "G loss: 0.23082053661346436\n",
      "Training Model  ...\n",
      "E loss:  0.5557635426521301\n",
      "G loss: 0.2238568663597107\n",
      "E loss:  0.5418349504470825\n",
      "G loss: 0.24068018794059753\n",
      "E loss:  0.5445898175239563\n",
      "G loss: 0.2507021427154541\n",
      "E loss:  0.5307734608650208\n",
      "G loss: 0.2723984122276306\n",
      "E loss:  0.5276033878326416\n",
      "G loss: 0.25327953696250916\n",
      "Training Model  ...\n",
      "E loss:  0.6069521903991699\n",
      "G loss: 0.27843034267425537\n",
      "E loss:  0.6029101014137268\n",
      "G loss: 0.3120392858982086\n",
      "E loss:  0.6123665571212769\n",
      "G loss: 0.30685731768608093\n",
      "E loss:  0.6172926425933838\n",
      "G loss: 0.34609442949295044\n",
      "E loss:  0.6037479639053345\n",
      "G loss: 0.3297857642173767\n",
      "Training Model  ...\n",
      "E loss:  0.5877779722213745\n",
      "G loss: 0.3364374041557312\n",
      "E loss:  0.5773913860321045\n",
      "G loss: 0.2956286072731018\n",
      "E loss:  0.5645031929016113\n",
      "G loss: 0.24733439087867737\n",
      "E loss:  0.5588177442550659\n",
      "G loss: 0.23059530556201935\n",
      "E loss:  0.5591486692428589\n",
      "G loss: 0.21645107865333557\n",
      "Training Model  ...\n",
      "E loss:  0.5675689578056335\n",
      "G loss: 0.21170538663864136\n",
      "E loss:  0.5642186403274536\n",
      "G loss: 0.22610777616500854\n",
      "E loss:  0.5615330934524536\n",
      "G loss: 0.23870176076889038\n",
      "E loss:  0.5544700622558594\n",
      "G loss: 0.23697443306446075\n",
      "E loss:  0.5539107322692871\n",
      "G loss: 0.24889791011810303\n",
      "Training Model  ...\n",
      "E loss:  0.5676140785217285\n",
      "G loss: 0.258693128824234\n",
      "E loss:  0.5662604570388794\n",
      "G loss: 0.2773456871509552\n",
      "E loss:  0.5802053809165955\n",
      "G loss: 0.30709344148635864\n",
      "E loss:  0.5701761245727539\n",
      "G loss: 0.2808073163032532\n",
      "E loss:  0.5802092552185059\n",
      "G loss: 0.2964620888233185\n",
      "Training Model  ...\n",
      "E loss:  0.571773886680603\n",
      "G loss: 0.2442966103553772\n",
      "E loss:  0.5840478539466858\n",
      "G loss: 0.24383556842803955\n",
      "E loss:  0.5812686085700989\n",
      "G loss: 0.24447982013225555\n",
      "E loss:  0.5748138427734375\n",
      "G loss: 0.19854408502578735\n",
      "E loss:  0.5893516540527344\n",
      "G loss: 0.23214133083820343\n",
      "Training Model  ...\n",
      "E loss:  0.6027607917785645\n",
      "G loss: 0.2177761197090149\n",
      "E loss:  0.5973146557807922\n",
      "G loss: 0.2770620584487915\n",
      "E loss:  0.5908733606338501\n",
      "G loss: 0.3138359785079956\n",
      "E loss:  0.5773323178291321\n",
      "G loss: 0.33374127745628357\n",
      "E loss:  0.5687810778617859\n",
      "G loss: 0.32772618532180786\n",
      "Training Model  ...\n",
      "E loss:  0.5802614688873291\n",
      "G loss: 0.318797767162323\n",
      "E loss:  0.5762155652046204\n",
      "G loss: 0.29478639364242554\n",
      "E loss:  0.5608086585998535\n",
      "G loss: 0.24414792656898499\n",
      "E loss:  0.5617815256118774\n",
      "G loss: 0.1900092214345932\n",
      "E loss:  0.5663043856620789\n",
      "G loss: 0.16488796472549438\n",
      "Training Model  ...\n",
      "E loss:  0.6458125114440918\n",
      "G loss: 0.21982960402965546\n",
      "E loss:  0.6396456360816956\n",
      "G loss: 0.2811809480190277\n",
      "E loss:  0.6153645515441895\n",
      "G loss: 0.3520630896091461\n",
      "E loss:  0.6160761713981628\n",
      "G loss: 0.43690380454063416\n",
      "E loss:  0.63284832239151\n",
      "G loss: 0.4207923412322998\n",
      "Training Model  ...\n",
      "E loss:  0.6288431286811829\n",
      "G loss: 0.3667079210281372\n",
      "E loss:  0.6224451661109924\n",
      "G loss: 0.3322330415248871\n",
      "E loss:  0.594843864440918\n",
      "G loss: 0.2768799662590027\n",
      "E loss:  0.5841401219367981\n",
      "G loss: 0.19928047060966492\n",
      "E loss:  0.5835452079772949\n",
      "G loss: 0.18817037343978882\n",
      "Training Model  ...\n",
      "E loss:  0.5828354954719543\n",
      "G loss: 0.23939824104309082\n",
      "E loss:  0.5720382332801819\n",
      "G loss: 0.2933802008628845\n",
      "E loss:  0.5520254373550415\n",
      "G loss: 0.2901461124420166\n",
      "E loss:  0.5546550154685974\n",
      "G loss: 0.3369641602039337\n",
      "E loss:  0.5626612305641174\n",
      "G loss: 0.37529703974723816\n",
      "Training Model  ...\n",
      "E loss:  0.6125142574310303\n",
      "G loss: 0.3397941291332245\n",
      "E loss:  0.6159021854400635\n",
      "G loss: 0.30966517329216003\n",
      "E loss:  0.6122676730155945\n",
      "G loss: 0.2505480647087097\n",
      "E loss:  0.6233619451522827\n",
      "G loss: 0.23082277178764343\n",
      "E loss:  0.6281021237373352\n",
      "G loss: 0.23993051052093506\n",
      "Training Model  ...\n",
      "E loss:  0.5478726029396057\n",
      "G loss: 0.20515140891075134\n",
      "E loss:  0.5496025085449219\n",
      "G loss: 0.23161207139492035\n",
      "E loss:  0.546868085861206\n",
      "G loss: 0.24124133586883545\n",
      "E loss:  0.5608102679252625\n",
      "G loss: 0.22190101444721222\n",
      "E loss:  0.5693888664245605\n",
      "G loss: 0.22524812817573547\n",
      "Training Model  ...\n",
      "E loss:  0.5286770462989807\n",
      "G loss: 0.25059571862220764\n",
      "E loss:  0.5238781571388245\n",
      "G loss: 0.26796799898147583\n",
      "E loss:  0.5268083810806274\n",
      "G loss: 0.2827851474285126\n",
      "E loss:  0.526177167892456\n",
      "G loss: 0.27550721168518066\n",
      "E loss:  0.5290008187294006\n",
      "G loss: 0.2834281027317047\n",
      "Training Model  ...\n",
      "E loss:  0.5986256003379822\n",
      "G loss: 0.2702087163925171\n",
      "E loss:  0.5937145948410034\n",
      "G loss: 0.2724118232727051\n",
      "E loss:  0.5988338589668274\n",
      "G loss: 0.22192057967185974\n",
      "E loss:  0.6080092191696167\n",
      "G loss: 0.2475268840789795\n",
      "E loss:  0.6061590313911438\n",
      "G loss: 0.2190702259540558\n",
      "Training Model  ...\n",
      "E loss:  0.6066846251487732\n",
      "G loss: 0.25444433093070984\n",
      "E loss:  0.6014847755432129\n",
      "G loss: 0.2809522747993469\n",
      "E loss:  0.5948740839958191\n",
      "G loss: 0.3259926438331604\n",
      "E loss:  0.5991174578666687\n",
      "G loss: 0.33931389451026917\n",
      "E loss:  0.6024278998374939\n",
      "G loss: 0.358568400144577\n",
      "Training Model  ...\n",
      "E loss:  0.5466462969779968\n",
      "G loss: 0.2900242507457733\n",
      "E loss:  0.5439356565475464\n",
      "G loss: 0.2687416076660156\n",
      "E loss:  0.5247892737388611\n",
      "G loss: 0.20595020055770874\n",
      "E loss:  0.5347772240638733\n",
      "G loss: 0.18139289319515228\n",
      "E loss:  0.5407639741897583\n",
      "G loss: 0.17937226593494415\n",
      "Training Model  ...\n",
      "E loss:  0.6050324440002441\n",
      "G loss: 0.1891157180070877\n",
      "E loss:  0.5595443248748779\n",
      "G loss: 0.2559950053691864\n",
      "E loss:  0.5545952320098877\n",
      "G loss: 0.34402212500572205\n",
      "E loss:  0.5478936433792114\n",
      "G loss: 0.39628350734710693\n",
      "E loss:  0.563383936882019\n",
      "G loss: 0.43978551030158997\n",
      "Training Model  ...\n",
      "E loss:  0.6250171661376953\n",
      "G loss: 0.39235353469848633\n",
      "E loss:  0.6137558817863464\n",
      "G loss: 0.3327644467353821\n",
      "E loss:  0.5902118682861328\n",
      "G loss: 0.2436247169971466\n",
      "E loss:  0.5877853035926819\n",
      "G loss: 0.20482446253299713\n",
      "E loss:  0.5886759757995605\n",
      "G loss: 0.18520422279834747\n",
      "Training Model  ...\n",
      "E loss:  0.5705190300941467\n",
      "G loss: 0.19631315767765045\n",
      "E loss:  0.5418248176574707\n",
      "G loss: 0.23616769909858704\n",
      "E loss:  0.5340203046798706\n",
      "G loss: 0.294931560754776\n",
      "E loss:  0.5455923080444336\n",
      "G loss: 0.35199981927871704\n",
      "E loss:  0.5442835092544556\n",
      "G loss: 0.3877052962779999\n",
      "Training Model  ...\n",
      "E loss:  0.6177839636802673\n",
      "G loss: 0.3403744399547577\n",
      "E loss:  0.6139724254608154\n",
      "G loss: 0.3024173974990845\n",
      "E loss:  0.6179928183555603\n",
      "G loss: 0.29511672258377075\n",
      "E loss:  0.6001798510551453\n",
      "G loss: 0.22854487597942352\n",
      "E loss:  0.5958197116851807\n",
      "G loss: 0.23459121584892273\n",
      "Training Model  ...\n",
      "E loss:  0.6121657490730286\n",
      "G loss: 0.24581807851791382\n",
      "E loss:  0.6028972864151001\n",
      "G loss: 0.23913493752479553\n",
      "E loss:  0.6082490086555481\n",
      "G loss: 0.28009676933288574\n",
      "E loss:  0.6058253049850464\n",
      "G loss: 0.2748451232910156\n",
      "E loss:  0.5927106142044067\n",
      "G loss: 0.2952498197555542\n",
      "Training Model  ...\n",
      "E loss:  0.5797265768051147\n",
      "G loss: 0.28005826473236084\n",
      "E loss:  0.5649598836898804\n",
      "G loss: 0.2336782068014145\n",
      "E loss:  0.5607537031173706\n",
      "G loss: 0.2334902435541153\n",
      "E loss:  0.5618975162506104\n",
      "G loss: 0.19830697774887085\n",
      "E loss:  0.5691743493080139\n",
      "G loss: 0.19672472774982452\n",
      "Training Model  ...\n",
      "E loss:  0.5695312023162842\n",
      "G loss: 0.2092590034008026\n",
      "E loss:  0.5479500889778137\n",
      "G loss: 0.25914761424064636\n",
      "E loss:  0.5505797266960144\n",
      "G loss: 0.2812480926513672\n",
      "E loss:  0.5572174787521362\n",
      "G loss: 0.34222954511642456\n",
      "E loss:  0.5517261624336243\n",
      "G loss: 0.3096027076244354\n",
      "Training Model  ...\n",
      "E loss:  0.5690151453018188\n",
      "G loss: 0.3472326695919037\n",
      "E loss:  0.5775381326675415\n",
      "G loss: 0.337528258562088\n",
      "E loss:  0.5744026899337769\n",
      "G loss: 0.28279173374176025\n",
      "E loss:  0.5706474781036377\n",
      "G loss: 0.2632611095905304\n",
      "E loss:  0.5904170870780945\n",
      "G loss: 0.2800111174583435\n",
      "Training Model  ...\n",
      "E loss:  0.5197981595993042\n",
      "G loss: 0.2761783003807068\n",
      "E loss:  0.5264517068862915\n",
      "G loss: 0.26953279972076416\n",
      "E loss:  0.5245059728622437\n",
      "G loss: 0.23543784022331238\n",
      "E loss:  0.5168321132659912\n",
      "G loss: 0.2726440727710724\n",
      "E loss:  0.51857590675354\n",
      "G loss: 0.2516080141067505\n",
      "Training Model  ...\n",
      "E loss:  0.5533017516136169\n",
      "G loss: 0.244046151638031\n",
      "E loss:  0.5622592568397522\n",
      "G loss: 0.29514360427856445\n",
      "E loss:  0.5582972168922424\n",
      "G loss: 0.2666277289390564\n",
      "E loss:  0.5588770508766174\n",
      "G loss: 0.23177383840084076\n",
      "E loss:  0.5507250428199768\n",
      "G loss: 0.24477943778038025\n",
      "Training Model  ...\n",
      "E loss:  0.5729190707206726\n",
      "G loss: 0.24812528491020203\n",
      "E loss:  0.5517625212669373\n",
      "G loss: 0.22100023925304413\n",
      "E loss:  0.5552407503128052\n",
      "G loss: 0.22003817558288574\n",
      "E loss:  0.5566717386245728\n",
      "G loss: 0.23951642215251923\n",
      "E loss:  0.5524445176124573\n",
      "G loss: 0.23127201199531555\n",
      "Training Model  ...\n",
      "E loss:  0.5923766493797302\n",
      "G loss: 0.20880597829818726\n",
      "E loss:  0.5916182398796082\n",
      "G loss: 0.2278594821691513\n",
      "E loss:  0.5970941185951233\n",
      "G loss: 0.23367317020893097\n",
      "E loss:  0.593468189239502\n",
      "G loss: 0.28440073132514954\n",
      "E loss:  0.6013209819793701\n",
      "G loss: 0.257968008518219\n",
      "Training Model  ...\n",
      "E loss:  0.5435627102851868\n",
      "G loss: 0.23948627710342407\n",
      "E loss:  0.536454975605011\n",
      "G loss: 0.28271859884262085\n",
      "E loss:  0.5411412119865417\n",
      "G loss: 0.21250411868095398\n",
      "E loss:  0.5470252633094788\n",
      "G loss: 0.21346570551395416\n",
      "E loss:  0.5465665459632874\n",
      "G loss: 0.20212838053703308\n",
      "Training Model  ...\n",
      "E loss:  0.5439155697822571\n",
      "G loss: 0.19946637749671936\n",
      "E loss:  0.5434588193893433\n",
      "G loss: 0.21332310140132904\n",
      "E loss:  0.5545606017112732\n",
      "G loss: 0.17355534434318542\n",
      "E loss:  0.5583840012550354\n",
      "G loss: 0.22009313106536865\n",
      "E loss:  0.5691547989845276\n",
      "G loss: 0.2219962179660797\n",
      "Training Model  ...\n",
      "E loss:  0.5418365001678467\n",
      "G loss: 0.2540970742702484\n",
      "E loss:  0.550061821937561\n",
      "G loss: 0.25342586636543274\n",
      "E loss:  0.5456753969192505\n",
      "G loss: 0.25223901867866516\n",
      "E loss:  0.552985668182373\n",
      "G loss: 0.2823563814163208\n",
      "E loss:  0.5605391263961792\n",
      "G loss: 0.2605198919773102\n",
      "Training Model  ...\n",
      "E loss:  0.5431188344955444\n",
      "G loss: 0.2874412536621094\n",
      "E loss:  0.5423859357833862\n",
      "G loss: 0.2660728991031647\n",
      "E loss:  0.5392694473266602\n",
      "G loss: 0.2833022177219391\n",
      "E loss:  0.5282760262489319\n",
      "G loss: 0.2106461226940155\n",
      "E loss:  0.5333526134490967\n",
      "G loss: 0.19954103231430054\n",
      "Training Model  ...\n",
      "E loss:  0.5473340749740601\n",
      "G loss: 0.25819891691207886\n",
      "E loss:  0.5352203845977783\n",
      "G loss: 0.26529231667518616\n",
      "E loss:  0.5160827040672302\n",
      "G loss: 0.29038092494010925\n",
      "E loss:  0.520351231098175\n",
      "G loss: 0.31263795495033264\n",
      "E loss:  0.5270720720291138\n",
      "G loss: 0.2965577244758606\n",
      "Training Model  ...\n",
      "E loss:  0.5391397476196289\n",
      "G loss: 0.3159407675266266\n",
      "E loss:  0.5257859230041504\n",
      "G loss: 0.247177392244339\n",
      "E loss:  0.5146650671958923\n",
      "G loss: 0.2164885252714157\n",
      "E loss:  0.5151621103286743\n",
      "G loss: 0.21537332236766815\n",
      "E loss:  0.506746768951416\n",
      "G loss: 0.17683430016040802\n",
      "Training Model  ...\n",
      "E loss:  0.5659523606300354\n",
      "G loss: 0.18460479378700256\n",
      "E loss:  0.5466194152832031\n",
      "G loss: 0.21555396914482117\n",
      "E loss:  0.5457776784896851\n",
      "G loss: 0.26661327481269836\n",
      "E loss:  0.5523825287818909\n",
      "G loss: 0.2856190502643585\n",
      "E loss:  0.5521925091743469\n",
      "G loss: 0.2634086310863495\n",
      "Training Model  ...\n",
      "E loss:  0.5659235119819641\n",
      "G loss: 0.2727912664413452\n",
      "E loss:  0.555107831954956\n",
      "G loss: 0.22852978110313416\n",
      "E loss:  0.5606293082237244\n",
      "G loss: 0.22356100380420685\n",
      "E loss:  0.5701555609703064\n",
      "G loss: 0.19829490780830383\n",
      "E loss:  0.5673455595970154\n",
      "G loss: 0.19270311295986176\n",
      "Training Model  ...\n",
      "E loss:  0.5578630566596985\n",
      "G loss: 0.19050832092761993\n",
      "E loss:  0.5664997100830078\n",
      "G loss: 0.21616071462631226\n",
      "E loss:  0.5729518532752991\n",
      "G loss: 0.2208288311958313\n",
      "E loss:  0.5691508054733276\n",
      "G loss: 0.25914162397384644\n",
      "E loss:  0.577256977558136\n",
      "G loss: 0.2418183833360672\n",
      "Training Model  ...\n",
      "E loss:  0.5626125335693359\n",
      "G loss: 0.2287924885749817\n",
      "E loss:  0.5657259821891785\n",
      "G loss: 0.24157586693763733\n",
      "E loss:  0.5659124255180359\n",
      "G loss: 0.21143817901611328\n",
      "E loss:  0.5849283337593079\n",
      "G loss: 0.21415060758590698\n",
      "E loss:  0.5559074878692627\n",
      "G loss: 0.23371952772140503\n",
      "Training Model  ...\n",
      "E loss:  0.5500876903533936\n",
      "G loss: 0.21048283576965332\n",
      "E loss:  0.5475481152534485\n",
      "G loss: 0.24435456097126007\n",
      "E loss:  0.5545905232429504\n",
      "G loss: 0.25261035561561584\n",
      "E loss:  0.5692368745803833\n",
      "G loss: 0.29247400164604187\n",
      "E loss:  0.5591959953308105\n",
      "G loss: 0.2872368097305298\n",
      "Training Model  ...\n",
      "E loss:  0.574232280254364\n",
      "G loss: 0.24519065022468567\n",
      "E loss:  0.5711198449134827\n",
      "G loss: 0.268056184053421\n",
      "E loss:  0.5586322546005249\n",
      "G loss: 0.2380446493625641\n",
      "E loss:  0.5743930339813232\n",
      "G loss: 0.2548264265060425\n",
      "E loss:  0.5616511106491089\n",
      "G loss: 0.24203158915042877\n",
      "Training Model  ...\n",
      "E loss:  0.5704894661903381\n",
      "G loss: 0.26350584626197815\n",
      "E loss:  0.5596969723701477\n",
      "G loss: 0.2919207811355591\n",
      "E loss:  0.5680187940597534\n",
      "G loss: 0.28838714957237244\n",
      "E loss:  0.5534304976463318\n",
      "G loss: 0.26877057552337646\n",
      "E loss:  0.5377684831619263\n",
      "G loss: 0.3059242069721222\n",
      "Training Model  ...\n",
      "E loss:  0.5924513339996338\n",
      "G loss: 0.28527575731277466\n",
      "E loss:  0.591651439666748\n",
      "G loss: 0.22787311673164368\n",
      "E loss:  0.593522310256958\n",
      "G loss: 0.21375791728496552\n",
      "E loss:  0.5864453315734863\n",
      "G loss: 0.19812463223934174\n",
      "E loss:  0.5965733528137207\n",
      "G loss: 0.2197064906358719\n",
      "Training Model  ...\n",
      "E loss:  0.5580810308456421\n",
      "G loss: 0.17389357089996338\n",
      "E loss:  0.5468149185180664\n",
      "G loss: 0.24302229285240173\n",
      "E loss:  0.5622360706329346\n",
      "G loss: 0.2367750108242035\n",
      "E loss:  0.5526612997055054\n",
      "G loss: 0.2572340965270996\n",
      "E loss:  0.5548136830329895\n",
      "G loss: 0.27781641483306885\n",
      "Training Model  ...\n",
      "E loss:  0.5509364604949951\n",
      "G loss: 0.2525356411933899\n",
      "E loss:  0.5383756160736084\n",
      "G loss: 0.25687676668167114\n",
      "E loss:  0.5438449382781982\n",
      "G loss: 0.21110525727272034\n",
      "E loss:  0.560504674911499\n",
      "G loss: 0.28170719742774963\n",
      "E loss:  0.5624305605888367\n",
      "G loss: 0.2518502175807953\n",
      "Training Model  ...\n",
      "E loss:  0.541016697883606\n",
      "G loss: 0.2537398934364319\n",
      "E loss:  0.5213193893432617\n",
      "G loss: 0.2166953980922699\n",
      "E loss:  0.5200626850128174\n",
      "G loss: 0.18631228804588318\n",
      "E loss:  0.5181154012680054\n",
      "G loss: 0.1881680190563202\n",
      "E loss:  0.5187152624130249\n",
      "G loss: 0.17794588208198547\n",
      "Training Model  ...\n",
      "E loss:  0.5628427863121033\n",
      "G loss: 0.19427350163459778\n",
      "E loss:  0.5594596862792969\n",
      "G loss: 0.21976058185100555\n",
      "E loss:  0.5555487871170044\n",
      "G loss: 0.23975470662117004\n",
      "E loss:  0.5432279706001282\n",
      "G loss: 0.27865415811538696\n",
      "E loss:  0.5501953363418579\n",
      "G loss: 0.2633001208305359\n",
      "Training Model  ...\n",
      "E loss:  0.523545503616333\n",
      "G loss: 0.26577228307724\n",
      "E loss:  0.5247954726219177\n",
      "G loss: 0.23597045242786407\n",
      "E loss:  0.5097167491912842\n",
      "G loss: 0.16856685280799866\n",
      "E loss:  0.5120470523834229\n",
      "G loss: 0.208658367395401\n",
      "E loss:  0.5114877820014954\n",
      "G loss: 0.15966367721557617\n",
      "Training Model  ...\n",
      "E loss:  0.6779099702835083\n",
      "G loss: 0.20742374658584595\n",
      "E loss:  0.6560220718383789\n",
      "G loss: 0.24907255172729492\n",
      "E loss:  0.6518830060958862\n",
      "G loss: 0.33197686076164246\n",
      "E loss:  0.6504687070846558\n",
      "G loss: 0.36634987592697144\n",
      "E loss:  0.6558040976524353\n",
      "G loss: 0.356179416179657\n",
      "Training Model  ...\n",
      "E loss:  0.5593321323394775\n",
      "G loss: 0.3225943446159363\n",
      "E loss:  0.5566987991333008\n",
      "G loss: 0.26414743065834045\n",
      "E loss:  0.5604017376899719\n",
      "G loss: 0.24081259965896606\n",
      "E loss:  0.5662627816200256\n",
      "G loss: 0.19628478586673737\n",
      "E loss:  0.5707207322120667\n",
      "G loss: 0.19856300950050354\n",
      "Training Model  ...\n",
      "E loss:  0.5158698558807373\n",
      "G loss: 0.208886981010437\n",
      "E loss:  0.5206136703491211\n",
      "G loss: 0.19894695281982422\n",
      "E loss:  0.5127336978912354\n",
      "G loss: 0.22991886734962463\n",
      "E loss:  0.5154886245727539\n",
      "G loss: 0.22812217473983765\n",
      "E loss:  0.5039929747581482\n",
      "G loss: 0.2134152352809906\n",
      "Training Model  ...\n",
      "E loss:  0.5064032077789307\n",
      "G loss: 0.23411953449249268\n",
      "E loss:  0.5060217380523682\n",
      "G loss: 0.21391436457633972\n",
      "E loss:  0.5125293731689453\n",
      "G loss: 0.21271727979183197\n",
      "E loss:  0.4999155104160309\n",
      "G loss: 0.1904846876859665\n",
      "E loss:  0.5011730790138245\n",
      "G loss: 0.16864530742168427\n",
      "Training Model  ...\n",
      "E loss:  0.5894394516944885\n",
      "G loss: 0.22493498027324677\n",
      "E loss:  0.5656060576438904\n",
      "G loss: 0.23708394169807434\n",
      "E loss:  0.5745981931686401\n",
      "G loss: 0.2719779312610626\n",
      "E loss:  0.5627696514129639\n",
      "G loss: 0.2842434346675873\n",
      "E loss:  0.5592179298400879\n",
      "G loss: 0.2997535765171051\n",
      "Training Model  ...\n",
      "E loss:  0.525001049041748\n",
      "G loss: 0.2986351251602173\n",
      "E loss:  0.5214012265205383\n",
      "G loss: 0.23058421909809113\n",
      "E loss:  0.5152431726455688\n",
      "G loss: 0.23005737364292145\n",
      "E loss:  0.5133463144302368\n",
      "G loss: 0.20798787474632263\n",
      "E loss:  0.5171555876731873\n",
      "G loss: 0.19012147188186646\n",
      "Training Model  ...\n",
      "E loss:  0.5918663740158081\n",
      "G loss: 0.194097101688385\n",
      "E loss:  0.5872885584831238\n",
      "G loss: 0.22343268990516663\n",
      "E loss:  0.5797647833824158\n",
      "G loss: 0.27363160252571106\n",
      "E loss:  0.5671687126159668\n",
      "G loss: 0.25743451714515686\n",
      "E loss:  0.5670881867408752\n",
      "G loss: 0.2936635911464691\n",
      "Training Model  ...\n",
      "E loss:  0.5789715647697449\n",
      "G loss: 0.25978678464889526\n",
      "E loss:  0.5858380198478699\n",
      "G loss: 0.23062415421009064\n",
      "E loss:  0.5863881707191467\n",
      "G loss: 0.22793422639369965\n",
      "E loss:  0.5799407362937927\n",
      "G loss: 0.2156483381986618\n",
      "E loss:  0.5853790044784546\n",
      "G loss: 0.20694054663181305\n",
      "Training Model  ...\n",
      "E loss:  0.5466287136077881\n",
      "G loss: 0.2106473594903946\n",
      "E loss:  0.5447569489479065\n",
      "G loss: 0.23242156207561493\n",
      "E loss:  0.536447286605835\n",
      "G loss: 0.2661150395870209\n",
      "E loss:  0.5250301957130432\n",
      "G loss: 0.26781705021858215\n",
      "E loss:  0.5200225114822388\n",
      "G loss: 0.28697746992111206\n",
      "Training Model  ...\n",
      "E loss:  0.5193045735359192\n",
      "G loss: 0.29664158821105957\n",
      "E loss:  0.5104566812515259\n",
      "G loss: 0.28195542097091675\n",
      "E loss:  0.5220327377319336\n",
      "G loss: 0.26215916872024536\n",
      "E loss:  0.5161424875259399\n",
      "G loss: 0.260254442691803\n",
      "E loss:  0.522771954536438\n",
      "G loss: 0.24851661920547485\n",
      "Training Model  ...\n",
      "E loss:  0.5495412349700928\n",
      "G loss: 0.24665701389312744\n",
      "E loss:  0.5520099401473999\n",
      "G loss: 0.22534853219985962\n",
      "E loss:  0.5448997020721436\n",
      "G loss: 0.2404138296842575\n",
      "E loss:  0.5402030944824219\n",
      "G loss: 0.22256389260292053\n",
      "E loss:  0.5457121133804321\n",
      "G loss: 0.25565850734710693\n",
      "Training Model  ...\n",
      "E loss:  0.5823171734809875\n",
      "G loss: 0.2321639508008957\n",
      "E loss:  0.5671719312667847\n",
      "G loss: 0.21084637939929962\n",
      "E loss:  0.5662669539451599\n",
      "G loss: 0.22920912504196167\n",
      "E loss:  0.5617128014564514\n",
      "G loss: 0.19864493608474731\n",
      "E loss:  0.5610601902008057\n",
      "G loss: 0.17602147161960602\n",
      "Training Model  ...\n",
      "E loss:  0.578380286693573\n",
      "G loss: 0.20948120951652527\n",
      "E loss:  0.5517182350158691\n",
      "G loss: 0.22967197000980377\n",
      "E loss:  0.5360910892486572\n",
      "G loss: 0.24208220839500427\n",
      "E loss:  0.5372866988182068\n",
      "G loss: 0.30046916007995605\n",
      "E loss:  0.5343461632728577\n",
      "G loss: 0.3076571524143219\n",
      "Training Model  ...\n",
      "E loss:  0.5221222639083862\n",
      "G loss: 0.26710546016693115\n",
      "E loss:  0.5050384998321533\n",
      "G loss: 0.21508878469467163\n",
      "E loss:  0.4964003264904022\n",
      "G loss: 0.18596836924552917\n",
      "E loss:  0.494099497795105\n",
      "G loss: 0.13244542479515076\n",
      "E loss:  0.49687498807907104\n",
      "G loss: 0.12840192019939423\n",
      "Training Model  ...\n",
      "E loss:  0.5360254049301147\n",
      "G loss: 0.12411926686763763\n",
      "E loss:  0.5234500765800476\n",
      "G loss: 0.17404359579086304\n",
      "E loss:  0.5016018152236938\n",
      "G loss: 0.22175049781799316\n",
      "E loss:  0.4977414309978485\n",
      "G loss: 0.22860011458396912\n",
      "E loss:  0.511369526386261\n",
      "G loss: 0.2702016532421112\n",
      "Training Model  ...\n",
      "E loss:  0.5547703504562378\n",
      "G loss: 0.2519138753414154\n",
      "E loss:  0.5547913312911987\n",
      "G loss: 0.23709432780742645\n",
      "E loss:  0.5605179667472839\n",
      "G loss: 0.1945534497499466\n",
      "E loss:  0.5479849576950073\n",
      "G loss: 0.19367516040802002\n",
      "E loss:  0.5412007570266724\n",
      "G loss: 0.18358083069324493\n",
      "Training Model  ...\n",
      "E loss:  0.5744934678077698\n",
      "G loss: 0.2181382179260254\n",
      "E loss:  0.5712623596191406\n",
      "G loss: 0.2358701229095459\n",
      "E loss:  0.5670660734176636\n",
      "G loss: 0.2904987633228302\n",
      "E loss:  0.5619090795516968\n",
      "G loss: 0.26947450637817383\n",
      "E loss:  0.5551649332046509\n",
      "G loss: 0.3062600791454315\n",
      "Training Model  ...\n",
      "E loss:  0.5558285713195801\n",
      "G loss: 0.24245010316371918\n",
      "E loss:  0.5595723390579224\n",
      "G loss: 0.2641884684562683\n",
      "E loss:  0.5389367341995239\n",
      "G loss: 0.22564145922660828\n",
      "E loss:  0.5480234622955322\n",
      "G loss: 0.2338927537202835\n",
      "E loss:  0.5366386771202087\n",
      "G loss: 0.21968737244606018\n",
      "Training Model  ...\n",
      "E loss:  0.5659736394882202\n",
      "G loss: 0.23681841790676117\n",
      "E loss:  0.5566131472587585\n",
      "G loss: 0.2349112331867218\n",
      "E loss:  0.5708482265472412\n",
      "G loss: 0.26476046442985535\n",
      "E loss:  0.5442835092544556\n",
      "G loss: 0.259588360786438\n",
      "E loss:  0.5403975248336792\n",
      "G loss: 0.26429420709609985\n",
      "Training Model  ...\n",
      "E loss:  0.5686143040657043\n",
      "G loss: 0.2152644693851471\n",
      "E loss:  0.5752325654029846\n",
      "G loss: 0.2405444234609604\n",
      "E loss:  0.5740549564361572\n",
      "G loss: 0.22746095061302185\n",
      "E loss:  0.568457841873169\n",
      "G loss: 0.22842027246952057\n",
      "E loss:  0.5598689913749695\n",
      "G loss: 0.21741855144500732\n",
      "Training Model  ...\n",
      "E loss:  0.5303348302841187\n",
      "G loss: 0.18332357704639435\n",
      "E loss:  0.5284775495529175\n",
      "G loss: 0.20058920979499817\n",
      "E loss:  0.53313148021698\n",
      "G loss: 0.19605909287929535\n",
      "E loss:  0.5317015647888184\n",
      "G loss: 0.2107280194759369\n",
      "E loss:  0.5310698747634888\n",
      "G loss: 0.20796778798103333\n",
      "Training Model  ...\n",
      "E loss:  0.47612354159355164\n",
      "G loss: 0.19632521271705627\n",
      "E loss:  0.4815230369567871\n",
      "G loss: 0.22079303860664368\n",
      "E loss:  0.4907057285308838\n",
      "G loss: 0.19517122209072113\n",
      "E loss:  0.477701336145401\n",
      "G loss: 0.2100340873003006\n",
      "E loss:  0.4695552587509155\n",
      "G loss: 0.18101057410240173\n",
      "Training Model  ...\n",
      "E loss:  0.5804443955421448\n",
      "G loss: 0.2197798192501068\n",
      "E loss:  0.5740229487419128\n",
      "G loss: 0.2237093299627304\n",
      "E loss:  0.5889525413513184\n",
      "G loss: 0.21525827050209045\n",
      "E loss:  0.589726448059082\n",
      "G loss: 0.22564907371997833\n",
      "E loss:  0.5899927616119385\n",
      "G loss: 0.2143653929233551\n",
      "Training Model  ...\n",
      "E loss:  0.5839243531227112\n",
      "G loss: 0.20608487725257874\n",
      "E loss:  0.5875621438026428\n",
      "G loss: 0.21201056241989136\n",
      "E loss:  0.5798426866531372\n",
      "G loss: 0.22634969651699066\n",
      "E loss:  0.5775015950202942\n",
      "G loss: 0.22281844913959503\n",
      "E loss:  0.5833595395088196\n",
      "G loss: 0.23183399438858032\n",
      "Training Model  ...\n",
      "E loss:  0.5111032128334045\n",
      "G loss: 0.1936616450548172\n",
      "E loss:  0.518158495426178\n",
      "G loss: 0.18177618086338043\n",
      "E loss:  0.5143818855285645\n",
      "G loss: 0.1642204076051712\n",
      "E loss:  0.5019587278366089\n",
      "G loss: 0.15685990452766418\n",
      "E loss:  0.510004997253418\n",
      "G loss: 0.17304407060146332\n",
      "Training Model  ...\n",
      "E loss:  0.5217229127883911\n",
      "G loss: 0.18630382418632507\n",
      "E loss:  0.5188618302345276\n",
      "G loss: 0.20789313316345215\n",
      "E loss:  0.5228736996650696\n",
      "G loss: 0.2286871373653412\n",
      "E loss:  0.5292633771896362\n",
      "G loss: 0.24578192830085754\n",
      "E loss:  0.5225350260734558\n",
      "G loss: 0.28646790981292725\n",
      "Training Model  ...\n",
      "E loss:  0.502159595489502\n",
      "G loss: 0.2516409754753113\n",
      "E loss:  0.49749627709388733\n",
      "G loss: 0.18821033835411072\n",
      "E loss:  0.49224650859832764\n",
      "G loss: 0.18148945271968842\n",
      "E loss:  0.4962986707687378\n",
      "G loss: 0.15417331457138062\n",
      "E loss:  0.506629228591919\n",
      "G loss: 0.1541990339756012\n",
      "Training Model  ...\n",
      "E loss:  0.49178436398506165\n",
      "G loss: 0.1650121659040451\n",
      "E loss:  0.4872155487537384\n",
      "G loss: 0.19447758793830872\n",
      "E loss:  0.4787684977054596\n",
      "G loss: 0.24991753697395325\n",
      "E loss:  0.4839215576648712\n",
      "G loss: 0.2614525854587555\n",
      "E loss:  0.4850528836250305\n",
      "G loss: 0.24376554787158966\n",
      "Training Model  ...\n",
      "E loss:  0.5441886186599731\n",
      "G loss: 0.22308243811130524\n",
      "E loss:  0.5411028265953064\n",
      "G loss: 0.19684730470180511\n",
      "E loss:  0.5336673259735107\n",
      "G loss: 0.19778305292129517\n",
      "E loss:  0.529850959777832\n",
      "G loss: 0.1766175478696823\n",
      "E loss:  0.5281217098236084\n",
      "G loss: 0.18838205933570862\n",
      "Training Model  ...\n",
      "E loss:  0.5119025707244873\n",
      "G loss: 0.2136412262916565\n",
      "E loss:  0.515361487865448\n",
      "G loss: 0.2112700641155243\n",
      "E loss:  0.5222864747047424\n",
      "G loss: 0.19133754074573517\n",
      "E loss:  0.5194579362869263\n",
      "G loss: 0.210869699716568\n",
      "E loss:  0.5163295269012451\n",
      "G loss: 0.21857057511806488\n",
      "Training Model  ...\n",
      "E loss:  0.5385429263114929\n",
      "G loss: 0.20552551746368408\n",
      "E loss:  0.5424591898918152\n",
      "G loss: 0.2162245810031891\n",
      "E loss:  0.5383032560348511\n",
      "G loss: 0.21710482239723206\n",
      "E loss:  0.5310164093971252\n",
      "G loss: 0.22600671648979187\n",
      "E loss:  0.536531925201416\n",
      "G loss: 0.19618728756904602\n",
      "Training Model  ...\n",
      "E loss:  0.5658937096595764\n",
      "G loss: 0.21365909278392792\n",
      "E loss:  0.5606777667999268\n",
      "G loss: 0.2102402299642563\n",
      "E loss:  0.5775847434997559\n",
      "G loss: 0.23868224024772644\n",
      "E loss:  0.5798534750938416\n",
      "G loss: 0.29992833733558655\n",
      "E loss:  0.5681415796279907\n",
      "G loss: 0.26928144693374634\n",
      "Training Model  ...\n",
      "E loss:  0.5223813056945801\n",
      "G loss: 0.27648311853408813\n",
      "E loss:  0.5216928720474243\n",
      "G loss: 0.2273469716310501\n",
      "E loss:  0.5244364738464355\n",
      "G loss: 0.18710967898368835\n",
      "E loss:  0.541832447052002\n",
      "G loss: 0.20174461603164673\n",
      "E loss:  0.5377476811408997\n",
      "G loss: 0.17060460150241852\n",
      "Training Model  ...\n",
      "E loss:  0.5435857772827148\n",
      "G loss: 0.17062383890151978\n",
      "E loss:  0.5454384088516235\n",
      "G loss: 0.20175281167030334\n",
      "E loss:  0.540733814239502\n",
      "G loss: 0.2379707545042038\n",
      "E loss:  0.5384193062782288\n",
      "G loss: 0.24284246563911438\n",
      "E loss:  0.5345649123191833\n",
      "G loss: 0.22093139588832855\n",
      "Training Model  ...\n",
      "E loss:  0.5530111789703369\n",
      "G loss: 0.22951781749725342\n",
      "E loss:  0.5403251051902771\n",
      "G loss: 0.21453553438186646\n",
      "E loss:  0.5417969226837158\n",
      "G loss: 0.1876383274793625\n",
      "E loss:  0.5446639657020569\n",
      "G loss: 0.21387699246406555\n",
      "E loss:  0.5499750375747681\n",
      "G loss: 0.21788200736045837\n",
      "Training Model  ...\n",
      "E loss:  0.5579831600189209\n",
      "G loss: 0.21887928247451782\n",
      "E loss:  0.5585234761238098\n",
      "G loss: 0.2648581862449646\n",
      "E loss:  0.5564943552017212\n",
      "G loss: 0.28749656677246094\n",
      "E loss:  0.5641130208969116\n",
      "G loss: 0.323974609375\n",
      "E loss:  0.5757026672363281\n",
      "G loss: 0.3166252076625824\n",
      "Training Model  ...\n",
      "E loss:  0.6207775473594666\n",
      "G loss: 0.28784823417663574\n",
      "E loss:  0.6167007684707642\n",
      "G loss: 0.26089581847190857\n",
      "E loss:  0.6329872608184814\n",
      "G loss: 0.19182023406028748\n",
      "E loss:  0.6294465661048889\n",
      "G loss: 0.21537011861801147\n",
      "E loss:  0.6057829260826111\n",
      "G loss: 0.22914594411849976\n",
      "Training Model  ...\n",
      "E loss:  0.48665380477905273\n",
      "G loss: 0.24027156829833984\n",
      "E loss:  0.47043687105178833\n",
      "G loss: 0.24267014861106873\n",
      "E loss:  0.45657509565353394\n",
      "G loss: 0.2614079713821411\n",
      "E loss:  0.45492249727249146\n",
      "G loss: 0.23371483385562897\n",
      "E loss:  0.46628448367118835\n",
      "G loss: 0.2857709228992462\n",
      "Training Model  ...\n",
      "E loss:  0.5496945977210999\n",
      "G loss: 0.22400368750095367\n",
      "E loss:  0.5393253564834595\n",
      "G loss: 0.22581934928894043\n",
      "E loss:  0.5416208505630493\n",
      "G loss: 0.2231726050376892\n",
      "E loss:  0.5408464670181274\n",
      "G loss: 0.18180179595947266\n",
      "E loss:  0.5392221212387085\n",
      "G loss: 0.1958974003791809\n",
      "Training Model  ...\n",
      "E loss:  0.5142909288406372\n",
      "G loss: 0.2037978321313858\n",
      "E loss:  0.5089927315711975\n",
      "G loss: 0.22089490294456482\n",
      "E loss:  0.4999992549419403\n",
      "G loss: 0.2494889497756958\n",
      "E loss:  0.5070900321006775\n",
      "G loss: 0.2670699656009674\n",
      "E loss:  0.5116101503372192\n",
      "G loss: 0.27468305826187134\n",
      "Training Model  ...\n",
      "E loss:  0.532839834690094\n",
      "G loss: 0.269123911857605\n",
      "E loss:  0.5328555107116699\n",
      "G loss: 0.2652015686035156\n",
      "E loss:  0.5324219465255737\n",
      "G loss: 0.25258326530456543\n",
      "E loss:  0.53841632604599\n",
      "G loss: 0.25528401136398315\n",
      "E loss:  0.5484591126441956\n",
      "G loss: 0.25991424918174744\n",
      "Training Model  ...\n",
      "E loss:  0.5254594087600708\n",
      "G loss: 0.25321048498153687\n",
      "E loss:  0.5272601842880249\n",
      "G loss: 0.293136864900589\n",
      "E loss:  0.5277422666549683\n",
      "G loss: 0.2861766517162323\n",
      "E loss:  0.5271152257919312\n",
      "G loss: 0.28221094608306885\n",
      "E loss:  0.532543420791626\n",
      "G loss: 0.2864193916320801\n",
      "Training Model  ...\n",
      "E loss:  0.5403773784637451\n",
      "G loss: 0.26405662298202515\n",
      "E loss:  0.5278644561767578\n",
      "G loss: 0.2298746258020401\n",
      "E loss:  0.533806562423706\n",
      "G loss: 0.20067095756530762\n",
      "E loss:  0.5273029804229736\n",
      "G loss: 0.2292133867740631\n",
      "E loss:  0.5267006754875183\n",
      "G loss: 0.20151939988136292\n",
      "Training Model  ...\n",
      "E loss:  0.625126302242279\n",
      "G loss: 0.20918069779872894\n",
      "E loss:  0.607895016670227\n",
      "G loss: 0.2685214579105377\n",
      "E loss:  0.5955303311347961\n",
      "G loss: 0.33987662196159363\n",
      "E loss:  0.5897915363311768\n",
      "G loss: 0.36142095923423767\n",
      "E loss:  0.5898316502571106\n",
      "G loss: 0.36153608560562134\n",
      "Training Model  ...\n",
      "E loss:  0.5102538466453552\n",
      "G loss: 0.2863403260707855\n",
      "E loss:  0.511539101600647\n",
      "G loss: 0.27957814931869507\n",
      "E loss:  0.5023624300956726\n",
      "G loss: 0.19270431995391846\n",
      "E loss:  0.49390774965286255\n",
      "G loss: 0.19684328138828278\n",
      "E loss:  0.5101468563079834\n",
      "G loss: 0.19287170469760895\n",
      "Training Model  ...\n",
      "E loss:  0.4633823037147522\n",
      "G loss: 0.17988911271095276\n",
      "E loss:  0.4792516827583313\n",
      "G loss: 0.20966340601444244\n",
      "E loss:  0.492834210395813\n",
      "G loss: 0.19910870492458344\n",
      "E loss:  0.4946824908256531\n",
      "G loss: 0.20411163568496704\n",
      "E loss:  0.4907275438308716\n",
      "G loss: 0.2136438488960266\n",
      "Training Model  ...\n",
      "E loss:  0.5168108344078064\n",
      "G loss: 0.25901949405670166\n",
      "E loss:  0.5115473866462708\n",
      "G loss: 0.2436952143907547\n",
      "E loss:  0.5069785118103027\n",
      "G loss: 0.27356502413749695\n",
      "E loss:  0.502001941204071\n",
      "G loss: 0.3084321618080139\n",
      "E loss:  0.5179075598716736\n",
      "G loss: 0.3380190134048462\n",
      "Training Model  ...\n",
      "E loss:  0.520259439945221\n",
      "G loss: 0.2910361886024475\n",
      "E loss:  0.5160092711448669\n",
      "G loss: 0.2746114134788513\n",
      "E loss:  0.5103103518486023\n",
      "G loss: 0.2618655264377594\n",
      "E loss:  0.5170549750328064\n",
      "G loss: 0.26720255613327026\n",
      "E loss:  0.5152339935302734\n",
      "G loss: 0.2581671178340912\n",
      "Training Model  ...\n",
      "E loss:  0.5160619616508484\n",
      "G loss: 0.23878440260887146\n",
      "E loss:  0.5146717429161072\n",
      "G loss: 0.21537408232688904\n",
      "E loss:  0.5102970600128174\n",
      "G loss: 0.21846507489681244\n",
      "E loss:  0.5200857520103455\n",
      "G loss: 0.20413050055503845\n",
      "E loss:  0.5224604606628418\n",
      "G loss: 0.21637998521327972\n",
      "Training Model  ...\n",
      "E loss:  0.5415903329849243\n",
      "G loss: 0.20447508990764618\n",
      "E loss:  0.516510546207428\n",
      "G loss: 0.23324836790561676\n",
      "E loss:  0.5223576426506042\n",
      "G loss: 0.31109803915023804\n",
      "E loss:  0.5236961245536804\n",
      "G loss: 0.29122114181518555\n",
      "E loss:  0.5193955898284912\n",
      "G loss: 0.28046295046806335\n",
      "Training Model  ...\n",
      "E loss:  0.5061737298965454\n",
      "G loss: 0.2669408917427063\n",
      "E loss:  0.5171043276786804\n",
      "G loss: 0.2378200888633728\n",
      "E loss:  0.5097339153289795\n",
      "G loss: 0.18632562458515167\n",
      "E loss:  0.5075432658195496\n",
      "G loss: 0.20852839946746826\n",
      "E loss:  0.5078323483467102\n",
      "G loss: 0.19189630448818207\n",
      "Training Model  ...\n",
      "E loss:  0.5410224199295044\n",
      "G loss: 0.20154143869876862\n",
      "E loss:  0.5255081057548523\n",
      "G loss: 0.24885588884353638\n",
      "E loss:  0.5307605266571045\n",
      "G loss: 0.27840685844421387\n",
      "E loss:  0.536113977432251\n",
      "G loss: 0.2550738453865051\n",
      "E loss:  0.5315784215927124\n",
      "G loss: 0.26541754603385925\n",
      "Training Model  ...\n",
      "E loss:  0.5189816355705261\n",
      "G loss: 0.2714043855667114\n",
      "E loss:  0.5158416628837585\n",
      "G loss: 0.251494437456131\n",
      "E loss:  0.5176020264625549\n",
      "G loss: 0.2650262713432312\n",
      "E loss:  0.5103621482849121\n",
      "G loss: 0.25530168414115906\n",
      "E loss:  0.5041428208351135\n",
      "G loss: 0.24433061480522156\n",
      "Training Model  ...\n",
      "E loss:  0.49508315324783325\n",
      "G loss: 0.2398230880498886\n",
      "E loss:  0.4994133710861206\n",
      "G loss: 0.24845126271247864\n",
      "E loss:  0.5020393133163452\n",
      "G loss: 0.22726435959339142\n",
      "E loss:  0.5282740592956543\n",
      "G loss: 0.205712229013443\n",
      "E loss:  0.5235017538070679\n",
      "G loss: 0.19985365867614746\n",
      "Training Model  ...\n",
      "E loss:  0.5223370790481567\n",
      "G loss: 0.22942711412906647\n",
      "E loss:  0.5264067649841309\n",
      "G loss: 0.19765296578407288\n",
      "E loss:  0.5288488864898682\n",
      "G loss: 0.2258577048778534\n",
      "E loss:  0.5333348512649536\n",
      "G loss: 0.2250853180885315\n",
      "E loss:  0.5252149701118469\n",
      "G loss: 0.2256316989660263\n",
      "Training Model  ...\n",
      "E loss:  0.5120869874954224\n",
      "G loss: 0.2049807757139206\n",
      "E loss:  0.5088598132133484\n",
      "G loss: 0.2006111592054367\n",
      "E loss:  0.5202606916427612\n",
      "G loss: 0.2118586152791977\n",
      "E loss:  0.5180474519729614\n",
      "G loss: 0.20987604558467865\n",
      "E loss:  0.5046190023422241\n",
      "G loss: 0.18651257455348969\n",
      "Training Model  ...\n",
      "E loss:  0.5934046506881714\n",
      "G loss: 0.2109389454126358\n",
      "E loss:  0.5890803337097168\n",
      "G loss: 0.23918671905994415\n",
      "E loss:  0.5864484310150146\n",
      "G loss: 0.2606361508369446\n",
      "E loss:  0.5893153548240662\n",
      "G loss: 0.22696749866008759\n",
      "E loss:  0.5719097852706909\n",
      "G loss: 0.28402042388916016\n",
      "Training Model  ...\n",
      "E loss:  0.5530849099159241\n",
      "G loss: 0.22192047536373138\n",
      "E loss:  0.5379745960235596\n",
      "G loss: 0.18698158860206604\n",
      "E loss:  0.5359073281288147\n",
      "G loss: 0.17084240913391113\n",
      "E loss:  0.5366615056991577\n",
      "G loss: 0.14802362024784088\n",
      "E loss:  0.5262550711631775\n",
      "G loss: 0.14542633295059204\n",
      "Training Model  ...\n",
      "E loss:  0.556870698928833\n",
      "G loss: 0.16555926203727722\n",
      "E loss:  0.543953001499176\n",
      "G loss: 0.23773325979709625\n",
      "E loss:  0.5320199728012085\n",
      "G loss: 0.2978675663471222\n",
      "E loss:  0.5385929346084595\n",
      "G loss: 0.38377052545547485\n",
      "E loss:  0.5557252168655396\n",
      "G loss: 0.32567429542541504\n",
      "Training Model  ...\n",
      "E loss:  0.5600061416625977\n",
      "G loss: 0.2808876037597656\n",
      "E loss:  0.5461930632591248\n",
      "G loss: 0.22228272259235382\n",
      "E loss:  0.5491334795951843\n",
      "G loss: 0.18137259781360626\n",
      "E loss:  0.5418304800987244\n",
      "G loss: 0.16370683908462524\n",
      "E loss:  0.5194176435470581\n",
      "G loss: 0.17851825058460236\n",
      "Training Model  ...\n",
      "E loss:  0.5547134280204773\n",
      "G loss: 0.17341816425323486\n",
      "E loss:  0.552216649055481\n",
      "G loss: 0.2397322952747345\n",
      "E loss:  0.5333864688873291\n",
      "G loss: 0.28630033135414124\n",
      "E loss:  0.5373741388320923\n",
      "G loss: 0.2838202118873596\n",
      "E loss:  0.520049512386322\n",
      "G loss: 0.3199363946914673\n",
      "Training Model  ...\n",
      "E loss:  0.5254647731781006\n",
      "G loss: 0.28073650598526\n",
      "E loss:  0.5190655589103699\n",
      "G loss: 0.21176643669605255\n",
      "E loss:  0.5017520189285278\n",
      "G loss: 0.173887699842453\n",
      "E loss:  0.4997251629829407\n",
      "G loss: 0.19109031558036804\n",
      "E loss:  0.5090462565422058\n",
      "G loss: 0.1748708039522171\n",
      "Training Model  ...\n",
      "E loss:  0.4963988661766052\n",
      "G loss: 0.17033985257148743\n",
      "E loss:  0.49229609966278076\n",
      "G loss: 0.2015206217765808\n",
      "E loss:  0.4934339225292206\n",
      "G loss: 0.20245963335037231\n",
      "E loss:  0.5002123117446899\n",
      "G loss: 0.23497740924358368\n",
      "E loss:  0.5031928420066833\n",
      "G loss: 0.2519609034061432\n",
      "Training Model  ...\n",
      "E loss:  0.46996498107910156\n",
      "G loss: 0.2113288938999176\n",
      "E loss:  0.46573606133461\n",
      "G loss: 0.19497159123420715\n",
      "E loss:  0.46381238102912903\n",
      "G loss: 0.1779363751411438\n",
      "E loss:  0.45705515146255493\n",
      "G loss: 0.1552380621433258\n",
      "E loss:  0.46642613410949707\n",
      "G loss: 0.17634597420692444\n",
      "Training Model  ...\n",
      "E loss:  0.5916999578475952\n",
      "G loss: 0.2195187211036682\n",
      "E loss:  0.5793619751930237\n",
      "G loss: 0.23976828157901764\n",
      "E loss:  0.572565495967865\n",
      "G loss: 0.26491281390190125\n",
      "E loss:  0.5775488018989563\n",
      "G loss: 0.32900911569595337\n",
      "E loss:  0.5815989971160889\n",
      "G loss: 0.2938553988933563\n",
      "Training Model  ...\n",
      "E loss:  0.5100486278533936\n",
      "G loss: 0.2634735107421875\n",
      "E loss:  0.4963555932044983\n",
      "G loss: 0.2243337333202362\n",
      "E loss:  0.4872308373451233\n",
      "G loss: 0.15893612802028656\n",
      "E loss:  0.500076413154602\n",
      "G loss: 0.14391380548477173\n",
      "E loss:  0.4868144094944\n",
      "G loss: 0.1417541354894638\n",
      "Training Model  ...\n",
      "E loss:  0.5086895227432251\n",
      "G loss: 0.16598963737487793\n",
      "E loss:  0.5028054714202881\n",
      "G loss: 0.1811124086380005\n",
      "E loss:  0.5043793320655823\n",
      "G loss: 0.18974652886390686\n",
      "E loss:  0.504490852355957\n",
      "G loss: 0.19264905154705048\n",
      "E loss:  0.5061210989952087\n",
      "G loss: 0.24563997983932495\n",
      "Training Model  ...\n",
      "E loss:  0.5479274988174438\n",
      "G loss: 0.20592622458934784\n",
      "E loss:  0.5582780838012695\n",
      "G loss: 0.25855910778045654\n",
      "E loss:  0.5588815808296204\n",
      "G loss: 0.22610144317150116\n",
      "E loss:  0.5653549432754517\n",
      "G loss: 0.19369956851005554\n",
      "E loss:  0.5585872530937195\n",
      "G loss: 0.19816315174102783\n",
      "Training Model  ...\n",
      "E loss:  0.532515287399292\n",
      "G loss: 0.20131754875183105\n",
      "E loss:  0.5408931374549866\n",
      "G loss: 0.19231261312961578\n",
      "E loss:  0.533832311630249\n",
      "G loss: 0.19479823112487793\n",
      "E loss:  0.5322359800338745\n",
      "G loss: 0.19612058997154236\n",
      "E loss:  0.5381309390068054\n",
      "G loss: 0.19823183119297028\n",
      "Training Model  ...\n",
      "E loss:  0.5411157011985779\n",
      "G loss: 0.2227335423231125\n",
      "E loss:  0.5243949890136719\n",
      "G loss: 0.23172348737716675\n",
      "E loss:  0.5106647610664368\n",
      "G loss: 0.24510034918785095\n",
      "E loss:  0.5113451480865479\n",
      "G loss: 0.3021736145019531\n",
      "E loss:  0.48069196939468384\n",
      "G loss: 0.24774383008480072\n",
      "Training Model  ...\n",
      "E loss:  0.5092944502830505\n",
      "G loss: 0.24746190011501312\n",
      "E loss:  0.5050095319747925\n",
      "G loss: 0.19743353128433228\n",
      "E loss:  0.50738525390625\n",
      "G loss: 0.15077435970306396\n",
      "E loss:  0.5080501437187195\n",
      "G loss: 0.12857183814048767\n",
      "E loss:  0.5222840905189514\n",
      "G loss: 0.13913317024707794\n",
      "Training Model  ...\n",
      "E loss:  0.5113230347633362\n",
      "G loss: 0.18375164270401\n",
      "E loss:  0.5060554146766663\n",
      "G loss: 0.22474095225334167\n",
      "E loss:  0.4906097948551178\n",
      "G loss: 0.26813429594039917\n",
      "E loss:  0.5044518709182739\n",
      "G loss: 0.30953657627105713\n",
      "E loss:  0.5069344639778137\n",
      "G loss: 0.2859842777252197\n",
      "Training Model  ...\n",
      "E loss:  0.5556260943412781\n",
      "G loss: 0.297096848487854\n",
      "E loss:  0.5296642184257507\n",
      "G loss: 0.23791465163230896\n",
      "E loss:  0.528701901435852\n",
      "G loss: 0.2045503705739975\n",
      "E loss:  0.5182618498802185\n",
      "G loss: 0.16776539385318756\n",
      "E loss:  0.5296672582626343\n",
      "G loss: 0.1641782820224762\n",
      "Training Model  ...\n",
      "E loss:  0.5725927352905273\n",
      "G loss: 0.20192191004753113\n",
      "E loss:  0.5590101480484009\n",
      "G loss: 0.22360524535179138\n",
      "E loss:  0.555875837802887\n",
      "G loss: 0.24174995720386505\n",
      "E loss:  0.5586059093475342\n",
      "G loss: 0.27288636565208435\n",
      "E loss:  0.5445441603660583\n",
      "G loss: 0.2697450816631317\n",
      "Training Model  ...\n",
      "E loss:  0.5514153242111206\n",
      "G loss: 0.23387768864631653\n",
      "E loss:  0.5418127775192261\n",
      "G loss: 0.22414334118366241\n",
      "E loss:  0.5346920490264893\n",
      "G loss: 0.17168159782886505\n",
      "E loss:  0.5506653785705566\n",
      "G loss: 0.17168028652668\n",
      "E loss:  0.554398775100708\n",
      "G loss: 0.19140693545341492\n",
      "Training Model  ...\n",
      "E loss:  0.5306413769721985\n",
      "G loss: 0.2108796238899231\n",
      "E loss:  0.5256676077842712\n",
      "G loss: 0.25438541173934937\n",
      "E loss:  0.5183765888214111\n",
      "G loss: 0.28863367438316345\n",
      "E loss:  0.5345858931541443\n",
      "G loss: 0.32604289054870605\n",
      "E loss:  0.5311105847358704\n",
      "G loss: 0.35002100467681885\n",
      "Training Model  ...\n",
      "E loss:  0.5308414101600647\n",
      "G loss: 0.30458003282546997\n",
      "E loss:  0.5194582343101501\n",
      "G loss: 0.2599278688430786\n",
      "E loss:  0.5042557120323181\n",
      "G loss: 0.20411871373653412\n",
      "E loss:  0.5068233609199524\n",
      "G loss: 0.22437039017677307\n",
      "E loss:  0.5123805403709412\n",
      "G loss: 0.18698330223560333\n",
      "Training Model  ...\n",
      "E loss:  0.46934568881988525\n",
      "G loss: 0.1989852786064148\n",
      "E loss:  0.4674043655395508\n",
      "G loss: 0.21361365914344788\n",
      "E loss:  0.46020814776420593\n",
      "G loss: 0.19053931534290314\n",
      "E loss:  0.46463751792907715\n",
      "G loss: 0.22015556693077087\n",
      "E loss:  0.46480563282966614\n",
      "G loss: 0.21793757379055023\n",
      "Training Model  ...\n",
      "E loss:  0.5712127685546875\n",
      "G loss: 0.25881320238113403\n",
      "E loss:  0.5670615434646606\n",
      "G loss: 0.24115589261054993\n",
      "E loss:  0.5567841529846191\n",
      "G loss: 0.2943670153617859\n",
      "E loss:  0.5594887137413025\n",
      "G loss: 0.2997826933860779\n",
      "E loss:  0.5587449669837952\n",
      "G loss: 0.26849114894866943\n",
      "Training Model  ...\n",
      "E loss:  0.4807743728160858\n",
      "G loss: 0.2695610225200653\n",
      "E loss:  0.4580530822277069\n",
      "G loss: 0.22524109482765198\n",
      "E loss:  0.4456012547016144\n",
      "G loss: 0.20961087942123413\n",
      "E loss:  0.4484875500202179\n",
      "G loss: 0.18631388247013092\n",
      "E loss:  0.4683571457862854\n",
      "G loss: 0.18624287843704224\n",
      "Training Model  ...\n",
      "E loss:  0.5272747874259949\n",
      "G loss: 0.20983487367630005\n",
      "E loss:  0.5349339246749878\n",
      "G loss: 0.2479027509689331\n",
      "E loss:  0.5362425446510315\n",
      "G loss: 0.263774573802948\n",
      "E loss:  0.5337838530540466\n",
      "G loss: 0.270027220249176\n",
      "E loss:  0.5408474802970886\n",
      "G loss: 0.2646748721599579\n",
      "Training Model  ...\n",
      "E loss:  0.5702097415924072\n",
      "G loss: 0.24901802837848663\n",
      "E loss:  0.5627345442771912\n",
      "G loss: 0.2496642768383026\n",
      "E loss:  0.5685810446739197\n",
      "G loss: 0.19025790691375732\n",
      "E loss:  0.5722897052764893\n",
      "G loss: 0.18763118982315063\n",
      "E loss:  0.5713117718696594\n",
      "G loss: 0.22112637758255005\n",
      "Training Model  ...\n",
      "E loss:  0.5375176072120667\n",
      "G loss: 0.2137269377708435\n",
      "E loss:  0.5500712394714355\n",
      "G loss: 0.2370336353778839\n",
      "E loss:  0.542851448059082\n",
      "G loss: 0.23284384608268738\n",
      "E loss:  0.5384499430656433\n",
      "G loss: 0.20751692354679108\n",
      "E loss:  0.5382565259933472\n",
      "G loss: 0.20732438564300537\n",
      "Training Model  ...\n",
      "E loss:  0.4768523573875427\n",
      "G loss: 0.2245994210243225\n",
      "E loss:  0.4787493348121643\n",
      "G loss: 0.2386636734008789\n",
      "E loss:  0.49068450927734375\n",
      "G loss: 0.21298444271087646\n",
      "E loss:  0.49764707684516907\n",
      "G loss: 0.2111784964799881\n",
      "E loss:  0.5035669803619385\n",
      "G loss: 0.2149522304534912\n",
      "Training Model  ...\n",
      "E loss:  0.47657400369644165\n",
      "G loss: 0.22592663764953613\n",
      "E loss:  0.4837045669555664\n",
      "G loss: 0.2790847718715668\n",
      "E loss:  0.4857572913169861\n",
      "G loss: 0.27447521686553955\n",
      "E loss:  0.48470938205718994\n",
      "G loss: 0.25839757919311523\n",
      "E loss:  0.49365633726119995\n",
      "G loss: 0.2567150294780731\n",
      "Training Model  ...\n",
      "E loss:  0.5292012691497803\n",
      "G loss: 0.26291826367378235\n",
      "E loss:  0.5151008367538452\n",
      "G loss: 0.25557559728622437\n",
      "E loss:  0.5142914056777954\n",
      "G loss: 0.2413085550069809\n",
      "E loss:  0.5133865475654602\n",
      "G loss: 0.18937574326992035\n",
      "E loss:  0.49629366397857666\n",
      "G loss: 0.21715132892131805\n",
      "Training Model  ...\n",
      "E loss:  0.5396418571472168\n",
      "G loss: 0.2112770676612854\n",
      "E loss:  0.5438507199287415\n",
      "G loss: 0.21284174919128418\n",
      "E loss:  0.5447511076927185\n",
      "G loss: 0.23576287925243378\n",
      "E loss:  0.5536989569664001\n",
      "G loss: 0.22849273681640625\n",
      "E loss:  0.5512211322784424\n",
      "G loss: 0.22305560111999512\n",
      "Training Model  ...\n",
      "E loss:  0.4544427990913391\n",
      "G loss: 0.2283458113670349\n",
      "E loss:  0.43886280059814453\n",
      "G loss: 0.24188536405563354\n",
      "E loss:  0.43322545289993286\n",
      "G loss: 0.22368058562278748\n",
      "E loss:  0.43325310945510864\n",
      "G loss: 0.2295052856206894\n",
      "E loss:  0.4305627942085266\n",
      "G loss: 0.24836809933185577\n",
      "Training Model  ...\n",
      "E loss:  0.5034871697425842\n",
      "G loss: 0.20898273587226868\n",
      "E loss:  0.5079531669616699\n",
      "G loss: 0.2231164276599884\n",
      "E loss:  0.49947187304496765\n",
      "G loss: 0.2292921096086502\n",
      "E loss:  0.49390655755996704\n",
      "G loss: 0.202373206615448\n",
      "E loss:  0.5044081211090088\n",
      "G loss: 0.2206754982471466\n",
      "Training Model  ...\n",
      "E loss:  0.5028371810913086\n",
      "G loss: 0.2075732946395874\n",
      "E loss:  0.5046983957290649\n",
      "G loss: 0.18451832234859467\n",
      "E loss:  0.5052876472473145\n",
      "G loss: 0.2062085121870041\n",
      "E loss:  0.500728964805603\n",
      "G loss: 0.20488515496253967\n",
      "E loss:  0.49648380279541016\n",
      "G loss: 0.18650148808956146\n",
      "Training Model  ...\n",
      "E loss:  0.5457839369773865\n",
      "G loss: 0.20881202816963196\n",
      "E loss:  0.5344312191009521\n",
      "G loss: 0.20392824709415436\n",
      "E loss:  0.5369700789451599\n",
      "G loss: 0.19982944428920746\n",
      "E loss:  0.5305562615394592\n",
      "G loss: 0.21449626982212067\n",
      "E loss:  0.5141226053237915\n",
      "G loss: 0.2170082926750183\n",
      "Training Model  ...\n",
      "E loss:  0.49687740206718445\n",
      "G loss: 0.23397579789161682\n",
      "E loss:  0.514508843421936\n",
      "G loss: 0.2584526240825653\n",
      "E loss:  0.5049543380737305\n",
      "G loss: 0.23822873830795288\n",
      "E loss:  0.5038352012634277\n",
      "G loss: 0.24542613327503204\n",
      "E loss:  0.499594509601593\n",
      "G loss: 0.24000078439712524\n",
      "Training Model  ...\n",
      "E loss:  0.4564873278141022\n",
      "G loss: 0.21847602725028992\n",
      "E loss:  0.4540342092514038\n",
      "G loss: 0.18198786675930023\n",
      "E loss:  0.44512927532196045\n",
      "G loss: 0.18625375628471375\n",
      "E loss:  0.43926623463630676\n",
      "G loss: 0.18337087333202362\n",
      "E loss:  0.44146478176116943\n",
      "G loss: 0.15059234201908112\n",
      "Training Model  ...\n",
      "E loss:  0.4817144274711609\n",
      "G loss: 0.14676599204540253\n",
      "E loss:  0.4820582866668701\n",
      "G loss: 0.18386083841323853\n",
      "E loss:  0.47238263487815857\n",
      "G loss: 0.22817876935005188\n",
      "E loss:  0.4752158522605896\n",
      "G loss: 0.2501521706581116\n",
      "E loss:  0.48392197489738464\n",
      "G loss: 0.21876715123653412\n",
      "Training Model  ...\n",
      "E loss:  0.5014169812202454\n",
      "G loss: 0.22017285227775574\n",
      "E loss:  0.4913763999938965\n",
      "G loss: 0.16924744844436646\n",
      "E loss:  0.48670801520347595\n",
      "G loss: 0.15267151594161987\n",
      "E loss:  0.48481082916259766\n",
      "G loss: 0.1453380137681961\n",
      "E loss:  0.493727445602417\n",
      "G loss: 0.14212527871131897\n",
      "Training Model  ...\n",
      "E loss:  0.5603861808776855\n",
      "G loss: 0.1618250012397766\n",
      "E loss:  0.5328070521354675\n",
      "G loss: 0.19095027446746826\n",
      "E loss:  0.5288363099098206\n",
      "G loss: 0.2677714228630066\n",
      "E loss:  0.5351022481918335\n",
      "G loss: 0.3201497197151184\n",
      "E loss:  0.5422973036766052\n",
      "G loss: 0.2669958770275116\n",
      "Training Model  ...\n",
      "E loss:  0.5630230903625488\n",
      "G loss: 0.25448909401893616\n",
      "E loss:  0.5566872358322144\n",
      "G loss: 0.247031107544899\n",
      "E loss:  0.5620811581611633\n",
      "G loss: 0.22932618856430054\n",
      "E loss:  0.556841254234314\n",
      "G loss: 0.20260992646217346\n",
      "E loss:  0.5520632863044739\n",
      "G loss: 0.2275981605052948\n",
      "Training Model  ...\n",
      "E loss:  0.5277490615844727\n",
      "G loss: 0.22933661937713623\n",
      "E loss:  0.5232821702957153\n",
      "G loss: 0.1976723074913025\n",
      "E loss:  0.5253958702087402\n",
      "G loss: 0.21533183753490448\n",
      "E loss:  0.5357116460800171\n",
      "G loss: 0.21029441058635712\n",
      "E loss:  0.5255250930786133\n",
      "G loss: 0.2078699767589569\n",
      "Training Model  ...\n",
      "E loss:  0.5514498353004456\n",
      "G loss: 0.22312529385089874\n",
      "E loss:  0.540562629699707\n",
      "G loss: 0.1528071165084839\n",
      "E loss:  0.5366414785385132\n",
      "G loss: 0.17908211052417755\n",
      "E loss:  0.5242649912834167\n",
      "G loss: 0.17687886953353882\n",
      "E loss:  0.5373382568359375\n",
      "G loss: 0.20106597244739532\n",
      "Training Model  ...\n",
      "E loss:  0.48200443387031555\n",
      "G loss: 0.20090337097644806\n",
      "E loss:  0.4749975800514221\n",
      "G loss: 0.19547581672668457\n",
      "E loss:  0.4695611596107483\n",
      "G loss: 0.24901406466960907\n",
      "E loss:  0.46881937980651855\n",
      "G loss: 0.270596444606781\n",
      "E loss:  0.47968870401382446\n",
      "G loss: 0.24492579698562622\n",
      "Training Model  ...\n",
      "E loss:  0.5049304366111755\n",
      "G loss: 0.2374894618988037\n",
      "E loss:  0.49352943897247314\n",
      "G loss: 0.18803489208221436\n",
      "E loss:  0.4896025061607361\n",
      "G loss: 0.15057659149169922\n",
      "E loss:  0.47724857926368713\n",
      "G loss: 0.1528647094964981\n",
      "E loss:  0.47817081212997437\n",
      "G loss: 0.1516726315021515\n",
      "Training Model  ...\n",
      "E loss:  0.596295952796936\n",
      "G loss: 0.1808110624551773\n",
      "E loss:  0.5863608121871948\n",
      "G loss: 0.2021396905183792\n",
      "E loss:  0.5910451412200928\n",
      "G loss: 0.24638479948043823\n",
      "E loss:  0.5839051008224487\n",
      "G loss: 0.21630913019180298\n",
      "E loss:  0.5876066088676453\n",
      "G loss: 0.23711863160133362\n",
      "Training Model  ...\n",
      "E loss:  0.46096229553222656\n",
      "G loss: 0.23660925030708313\n",
      "E loss:  0.45929086208343506\n",
      "G loss: 0.21940955519676208\n",
      "E loss:  0.45385754108428955\n",
      "G loss: 0.24067780375480652\n",
      "E loss:  0.4531104862689972\n",
      "G loss: 0.25393712520599365\n",
      "E loss:  0.4569614827632904\n",
      "G loss: 0.21961550414562225\n",
      "Training Model  ...\n",
      "E loss:  0.5422093272209167\n",
      "G loss: 0.23064297437667847\n",
      "E loss:  0.5446504354476929\n",
      "G loss: 0.20374174416065216\n",
      "E loss:  0.5416555404663086\n",
      "G loss: 0.22182825207710266\n",
      "E loss:  0.5406151413917542\n",
      "G loss: 0.18648213148117065\n",
      "E loss:  0.536146879196167\n",
      "G loss: 0.24667799472808838\n",
      "Training Model  ...\n",
      "E loss:  0.5181600451469421\n",
      "G loss: 0.2061905711889267\n",
      "E loss:  0.5214710235595703\n",
      "G loss: 0.18529097735881805\n",
      "E loss:  0.5149122476577759\n",
      "G loss: 0.18882527947425842\n",
      "E loss:  0.5207988023757935\n",
      "G loss: 0.16562803089618683\n",
      "E loss:  0.5223415493965149\n",
      "G loss: 0.16434931755065918\n",
      "Training Model  ...\n",
      "E loss:  0.4870937764644623\n",
      "G loss: 0.19616523385047913\n",
      "E loss:  0.4897812008857727\n",
      "G loss: 0.19222138822078705\n",
      "E loss:  0.48783546686172485\n",
      "G loss: 0.20352032780647278\n",
      "E loss:  0.49224457144737244\n",
      "G loss: 0.2469428926706314\n",
      "E loss:  0.4917590618133545\n",
      "G loss: 0.20123355090618134\n",
      "Training Model  ...\n",
      "E loss:  0.5133975744247437\n",
      "G loss: 0.24171659350395203\n",
      "E loss:  0.5193654298782349\n",
      "G loss: 0.20406264066696167\n",
      "E loss:  0.5111544728279114\n",
      "G loss: 0.21222957968711853\n",
      "E loss:  0.5075435042381287\n",
      "G loss: 0.1900947093963623\n",
      "E loss:  0.5116707682609558\n",
      "G loss: 0.22590991854667664\n",
      "Training Model  ...\n",
      "E loss:  0.5055734515190125\n",
      "G loss: 0.21609920263290405\n",
      "E loss:  0.5110807418823242\n",
      "G loss: 0.20770126581192017\n",
      "E loss:  0.5102548003196716\n",
      "G loss: 0.2070557177066803\n",
      "E loss:  0.5084671974182129\n",
      "G loss: 0.22010484337806702\n",
      "E loss:  0.50790935754776\n",
      "G loss: 0.2414828985929489\n",
      "Training Model  ...\n",
      "E loss:  0.45276230573654175\n",
      "G loss: 0.19736430048942566\n",
      "E loss:  0.46065619587898254\n",
      "G loss: 0.21592482924461365\n",
      "E loss:  0.44114556908607483\n",
      "G loss: 0.2084435522556305\n",
      "E loss:  0.45382577180862427\n",
      "G loss: 0.20360668003559113\n",
      "E loss:  0.4487107992172241\n",
      "G loss: 0.20059561729431152\n",
      "Training Model  ...\n",
      "E loss:  0.4437467157840729\n",
      "G loss: 0.21814274787902832\n",
      "E loss:  0.4460830092430115\n",
      "G loss: 0.23114126920700073\n",
      "E loss:  0.4480414092540741\n",
      "G loss: 0.21209591627120972\n",
      "E loss:  0.429481565952301\n",
      "G loss: 0.23221927881240845\n",
      "E loss:  0.42468610405921936\n",
      "G loss: 0.2236449271440506\n",
      "Training Model  ...\n",
      "E loss:  0.5281822085380554\n",
      "G loss: 0.23393377661705017\n",
      "E loss:  0.5203766822814941\n",
      "G loss: 0.22504502534866333\n",
      "E loss:  0.5206834673881531\n",
      "G loss: 0.23118102550506592\n",
      "E loss:  0.5163346529006958\n",
      "G loss: 0.22242827713489532\n",
      "E loss:  0.5094106197357178\n",
      "G loss: 0.2559206783771515\n",
      "Training Model  ...\n",
      "E loss:  0.5096142888069153\n",
      "G loss: 0.28360775113105774\n",
      "E loss:  0.5128281116485596\n",
      "G loss: 0.27558720111846924\n",
      "E loss:  0.49706974625587463\n",
      "G loss: 0.3159767687320709\n",
      "E loss:  0.5104474425315857\n",
      "G loss: 0.32428061962127686\n",
      "E loss:  0.49465012550354004\n",
      "G loss: 0.31335270404815674\n",
      "Training Model  ...\n",
      "E loss:  0.5324024558067322\n",
      "G loss: 0.2788158059120178\n",
      "E loss:  0.51598060131073\n",
      "G loss: 0.2289983481168747\n",
      "E loss:  0.4998778998851776\n",
      "G loss: 0.21242867410182953\n",
      "E loss:  0.5044541358947754\n",
      "G loss: 0.17359201610088348\n",
      "E loss:  0.512392520904541\n",
      "G loss: 0.2242816686630249\n",
      "Training Model  ...\n",
      "E loss:  0.4988238215446472\n",
      "G loss: 0.204515278339386\n",
      "E loss:  0.5051974058151245\n",
      "G loss: 0.22255268692970276\n",
      "E loss:  0.5115907788276672\n",
      "G loss: 0.22588738799095154\n",
      "E loss:  0.5117312669754028\n",
      "G loss: 0.22217130661010742\n",
      "E loss:  0.5150502324104309\n",
      "G loss: 0.2201191484928131\n",
      "Training Model  ...\n",
      "E loss:  0.5124287009239197\n",
      "G loss: 0.24495604634284973\n",
      "E loss:  0.5218051671981812\n",
      "G loss: 0.24709941446781158\n",
      "E loss:  0.5094318389892578\n",
      "G loss: 0.2753092050552368\n",
      "E loss:  0.5072873830795288\n",
      "G loss: 0.298341304063797\n",
      "E loss:  0.4965362548828125\n",
      "G loss: 0.2687023878097534\n",
      "Training Model  ...\n",
      "E loss:  0.4873393177986145\n",
      "G loss: 0.26849499344825745\n",
      "E loss:  0.47303900122642517\n",
      "G loss: 0.23885147273540497\n",
      "E loss:  0.46657508611679077\n",
      "G loss: 0.19747459888458252\n",
      "E loss:  0.4696364104747772\n",
      "G loss: 0.19629895687103271\n",
      "E loss:  0.4692014753818512\n",
      "G loss: 0.16905902326107025\n",
      "Training Model  ...\n",
      "E loss:  0.5263047814369202\n",
      "G loss: 0.22398057579994202\n",
      "E loss:  0.5174760818481445\n",
      "G loss: 0.23341000080108643\n",
      "E loss:  0.502762496471405\n",
      "G loss: 0.2617608904838562\n",
      "E loss:  0.49870356917381287\n",
      "G loss: 0.26543229818344116\n",
      "E loss:  0.4838019609451294\n",
      "G loss: 0.2848554253578186\n",
      "Training Model  ...\n",
      "E loss:  0.532544732093811\n",
      "G loss: 0.2555711269378662\n",
      "E loss:  0.5272516012191772\n",
      "G loss: 0.22170545160770416\n",
      "E loss:  0.5182850360870361\n",
      "G loss: 0.18057562410831451\n",
      "E loss:  0.5171900987625122\n",
      "G loss: 0.1614031344652176\n",
      "E loss:  0.5202777981758118\n",
      "G loss: 0.2056376338005066\n",
      "Training Model  ...\n",
      "E loss:  0.551222562789917\n",
      "G loss: 0.2223576009273529\n",
      "E loss:  0.545129656791687\n",
      "G loss: 0.21555662155151367\n",
      "E loss:  0.5442644357681274\n",
      "G loss: 0.2601235806941986\n",
      "E loss:  0.5442477464675903\n",
      "G loss: 0.2728458344936371\n",
      "E loss:  0.5461814999580383\n",
      "G loss: 0.2781873345375061\n",
      "Training Model  ...\n",
      "E loss:  0.49022364616394043\n",
      "G loss: 0.24326685070991516\n",
      "E loss:  0.4880205988883972\n",
      "G loss: 0.19020506739616394\n",
      "E loss:  0.47609031200408936\n",
      "G loss: 0.16960662603378296\n",
      "E loss:  0.46856796741485596\n",
      "G loss: 0.12523256242275238\n",
      "E loss:  0.4873719811439514\n",
      "G loss: 0.14403200149536133\n",
      "Training Model  ...\n",
      "E loss:  0.4596596658229828\n",
      "G loss: 0.17115060985088348\n",
      "E loss:  0.44207289814949036\n",
      "G loss: 0.20098067820072174\n",
      "E loss:  0.4491897523403168\n",
      "G loss: 0.23816800117492676\n",
      "E loss:  0.45365846157073975\n",
      "G loss: 0.22680358588695526\n",
      "E loss:  0.45208585262298584\n",
      "G loss: 0.21956640481948853\n",
      "Training Model  ...\n",
      "E loss:  0.513390302658081\n",
      "G loss: 0.22493243217468262\n",
      "E loss:  0.5118108987808228\n",
      "G loss: 0.23649922013282776\n",
      "E loss:  0.5106801986694336\n",
      "G loss: 0.2184484750032425\n",
      "E loss:  0.5032767057418823\n",
      "G loss: 0.23038250207901\n",
      "E loss:  0.514258086681366\n",
      "G loss: 0.2275225818157196\n",
      "Training Model  ...\n",
      "E loss:  0.4758617877960205\n",
      "G loss: 0.2016104906797409\n",
      "E loss:  0.4698314070701599\n",
      "G loss: 0.22874850034713745\n",
      "E loss:  0.46477776765823364\n",
      "G loss: 0.24047204852104187\n",
      "E loss:  0.45517435669898987\n",
      "G loss: 0.2610987424850464\n",
      "E loss:  0.4523617923259735\n",
      "G loss: 0.2752148509025574\n",
      "Training Model  ...\n",
      "E loss:  0.5137071013450623\n",
      "G loss: 0.2347857505083084\n",
      "E loss:  0.5156676769256592\n",
      "G loss: 0.18722978234291077\n",
      "E loss:  0.5220143795013428\n",
      "G loss: 0.16104072332382202\n",
      "E loss:  0.5214776396751404\n",
      "G loss: 0.16640575230121613\n",
      "E loss:  0.5197145342826843\n",
      "G loss: 0.19968490302562714\n",
      "Training Model  ...\n",
      "E loss:  0.5270888209342957\n",
      "G loss: 0.20863424241542816\n",
      "E loss:  0.5159568190574646\n",
      "G loss: 0.2510463297367096\n",
      "E loss:  0.5148087739944458\n",
      "G loss: 0.2887803614139557\n",
      "E loss:  0.516691267490387\n",
      "G loss: 0.32800793647766113\n",
      "E loss:  0.5082751512527466\n",
      "G loss: 0.2610210180282593\n",
      "Training Model  ...\n",
      "E loss:  0.503461480140686\n",
      "G loss: 0.23864513635635376\n",
      "E loss:  0.49888482689857483\n",
      "G loss: 0.21338695287704468\n",
      "E loss:  0.4922693371772766\n",
      "G loss: 0.2244686782360077\n",
      "E loss:  0.5017677545547485\n",
      "G loss: 0.20531748235225677\n",
      "E loss:  0.4976559281349182\n",
      "G loss: 0.21418188512325287\n",
      "Training Model  ...\n",
      "E loss:  0.4612921476364136\n",
      "G loss: 0.24450932443141937\n",
      "E loss:  0.4548022449016571\n",
      "G loss: 0.2199568748474121\n",
      "E loss:  0.45644426345825195\n",
      "G loss: 0.1926535665988922\n",
      "E loss:  0.4458881616592407\n",
      "G loss: 0.21388068795204163\n",
      "E loss:  0.449612557888031\n",
      "G loss: 0.2212826907634735\n",
      "Training Model  ...\n",
      "E loss:  0.49691054224967957\n",
      "G loss: 0.2173411101102829\n",
      "E loss:  0.49647650122642517\n",
      "G loss: 0.2342393845319748\n",
      "E loss:  0.49407607316970825\n",
      "G loss: 0.24905547499656677\n",
      "E loss:  0.4919257164001465\n",
      "G loss: 0.21139755845069885\n",
      "E loss:  0.4846590757369995\n",
      "G loss: 0.22587627172470093\n",
      "Training Model  ...\n",
      "E loss:  0.5004606246948242\n",
      "G loss: 0.21627949178218842\n",
      "E loss:  0.5024127960205078\n",
      "G loss: 0.2195354551076889\n",
      "E loss:  0.49739331007003784\n",
      "G loss: 0.18545439839363098\n",
      "E loss:  0.4842170476913452\n",
      "G loss: 0.1749129444360733\n",
      "E loss:  0.48405662178993225\n",
      "G loss: 0.18869510293006897\n",
      "Training Model  ...\n",
      "E loss:  0.5241938233375549\n",
      "G loss: 0.2088260054588318\n",
      "E loss:  0.5087623000144958\n",
      "G loss: 0.25454986095428467\n",
      "E loss:  0.5188626646995544\n",
      "G loss: 0.24032148718833923\n",
      "E loss:  0.5261277556419373\n",
      "G loss: 0.3198983371257782\n",
      "E loss:  0.5271480083465576\n",
      "G loss: 0.2982015609741211\n",
      "Training Model  ...\n",
      "E loss:  0.5015906691551208\n",
      "G loss: 0.27305030822753906\n",
      "E loss:  0.5027034282684326\n",
      "G loss: 0.22719916701316833\n",
      "E loss:  0.48377352952957153\n",
      "G loss: 0.24151986837387085\n",
      "E loss:  0.5019832849502563\n",
      "G loss: 0.23036246001720428\n",
      "E loss:  0.5009806156158447\n",
      "G loss: 0.234463170170784\n",
      "Training Model  ...\n",
      "E loss:  0.5005794167518616\n",
      "G loss: 0.2232343852519989\n",
      "E loss:  0.501594603061676\n",
      "G loss: 0.23874923586845398\n",
      "E loss:  0.493105947971344\n",
      "G loss: 0.17556019127368927\n",
      "E loss:  0.4671556353569031\n",
      "G loss: 0.19767525792121887\n",
      "E loss:  0.4672631323337555\n",
      "G loss: 0.22948923707008362\n",
      "Training Model  ...\n",
      "E loss:  0.5146524310112\n",
      "G loss: 0.21255071461200714\n",
      "E loss:  0.5238919258117676\n",
      "G loss: 0.21730923652648926\n",
      "E loss:  0.5163211822509766\n",
      "G loss: 0.21667709946632385\n",
      "E loss:  0.5084578394889832\n",
      "G loss: 0.22583359479904175\n",
      "E loss:  0.5033103823661804\n",
      "G loss: 0.254559725522995\n",
      "Training Model  ...\n",
      "E loss:  0.4624972343444824\n",
      "G loss: 0.19347134232521057\n",
      "E loss:  0.4505939781665802\n",
      "G loss: 0.2046915888786316\n",
      "E loss:  0.45348280668258667\n",
      "G loss: 0.1992921531200409\n",
      "E loss:  0.44987428188323975\n",
      "G loss: 0.21013778448104858\n",
      "E loss:  0.44035717844963074\n",
      "G loss: 0.21535933017730713\n",
      "Training Model  ...\n",
      "E loss:  0.4887753129005432\n",
      "G loss: 0.18804840743541718\n",
      "E loss:  0.49365413188934326\n",
      "G loss: 0.2324419617652893\n",
      "E loss:  0.486908495426178\n",
      "G loss: 0.21384204924106598\n",
      "E loss:  0.4845508933067322\n",
      "G loss: 0.20139232277870178\n",
      "E loss:  0.49508869647979736\n",
      "G loss: 0.20581389963626862\n",
      "Training Model  ...\n",
      "E loss:  0.5154032707214355\n",
      "G loss: 0.22757457196712494\n",
      "E loss:  0.5260964035987854\n",
      "G loss: 0.22207854688167572\n",
      "E loss:  0.5251366496086121\n",
      "G loss: 0.2545897364616394\n",
      "E loss:  0.5383923649787903\n",
      "G loss: 0.2806498408317566\n",
      "E loss:  0.5357544422149658\n",
      "G loss: 0.2387148141860962\n",
      "Training Model  ...\n",
      "E loss:  0.4784015715122223\n",
      "G loss: 0.22383564710617065\n",
      "E loss:  0.4706445336341858\n",
      "G loss: 0.17864185571670532\n",
      "E loss:  0.47759881615638733\n",
      "G loss: 0.17799802124500275\n",
      "E loss:  0.47649624943733215\n",
      "G loss: 0.17290164530277252\n",
      "E loss:  0.4773273169994354\n",
      "G loss: 0.18161925673484802\n",
      "Training Model  ...\n",
      "E loss:  0.5192314386367798\n",
      "G loss: 0.18805600702762604\n",
      "E loss:  0.509934663772583\n",
      "G loss: 0.20709410309791565\n",
      "E loss:  0.5004909634590149\n",
      "G loss: 0.25745993852615356\n",
      "E loss:  0.502293586730957\n",
      "G loss: 0.2623540759086609\n",
      "E loss:  0.5079152584075928\n",
      "G loss: 0.21768194437026978\n",
      "Training Model  ...\n",
      "E loss:  0.5472283363342285\n",
      "G loss: 0.21636760234832764\n",
      "E loss:  0.5324336886405945\n",
      "G loss: 0.16650232672691345\n",
      "E loss:  0.5155702233314514\n",
      "G loss: 0.1704513281583786\n",
      "E loss:  0.5205776691436768\n",
      "G loss: 0.1831243336200714\n",
      "E loss:  0.517377495765686\n",
      "G loss: 0.20876501500606537\n",
      "Training Model  ...\n",
      "E loss:  0.4262956976890564\n",
      "G loss: 0.1628127247095108\n",
      "E loss:  0.4175310730934143\n",
      "G loss: 0.20537175238132477\n",
      "E loss:  0.4176979660987854\n",
      "G loss: 0.2373037040233612\n",
      "E loss:  0.42058074474334717\n",
      "G loss: 0.25798267126083374\n",
      "E loss:  0.4215174913406372\n",
      "G loss: 0.22954632341861725\n",
      "Training Model  ...\n",
      "E loss:  0.4794035851955414\n",
      "G loss: 0.22289282083511353\n",
      "E loss:  0.4813932776451111\n",
      "G loss: 0.21887199580669403\n",
      "E loss:  0.47802573442459106\n",
      "G loss: 0.20203298330307007\n",
      "E loss:  0.47918760776519775\n",
      "G loss: 0.18952986598014832\n",
      "E loss:  0.4722144305706024\n",
      "G loss: 0.18844562768936157\n",
      "Training Model  ...\n",
      "E loss:  0.511542022228241\n",
      "G loss: 0.2610364854335785\n",
      "E loss:  0.5088326930999756\n",
      "G loss: 0.25646722316741943\n",
      "E loss:  0.5059463977813721\n",
      "G loss: 0.27174046635627747\n",
      "E loss:  0.5135714411735535\n",
      "G loss: 0.31889405846595764\n",
      "E loss:  0.5197657942771912\n",
      "G loss: 0.2835662066936493\n",
      "Training Model  ...\n",
      "E loss:  0.5267190337181091\n",
      "G loss: 0.26984456181526184\n",
      "E loss:  0.5170396566390991\n",
      "G loss: 0.20658627152442932\n",
      "E loss:  0.5213256478309631\n",
      "G loss: 0.17567110061645508\n",
      "E loss:  0.5452996492385864\n",
      "G loss: 0.17292995750904083\n",
      "E loss:  0.5467153787612915\n",
      "G loss: 0.19974713027477264\n",
      "Training Model  ...\n",
      "E loss:  0.49323517084121704\n",
      "G loss: 0.1963401436805725\n",
      "E loss:  0.5010899901390076\n",
      "G loss: 0.21214205026626587\n",
      "E loss:  0.49395066499710083\n",
      "G loss: 0.2656119465827942\n",
      "E loss:  0.49813854694366455\n",
      "G loss: 0.2634977698326111\n",
      "E loss:  0.49945539236068726\n",
      "G loss: 0.2165840119123459\n",
      "Training Model  ...\n",
      "E loss:  0.5293902158737183\n",
      "G loss: 0.24209536612033844\n",
      "E loss:  0.5337508320808411\n",
      "G loss: 0.24271631240844727\n",
      "E loss:  0.5410261154174805\n",
      "G loss: 0.2310914695262909\n",
      "E loss:  0.5399497747421265\n",
      "G loss: 0.26349782943725586\n",
      "E loss:  0.5370147228240967\n",
      "G loss: 0.23181313276290894\n",
      "Training Model  ...\n",
      "E loss:  0.49839791655540466\n",
      "G loss: 0.24465511739253998\n",
      "E loss:  0.49384596943855286\n",
      "G loss: 0.20161844789981842\n",
      "E loss:  0.4931253492832184\n",
      "G loss: 0.22252999246120453\n",
      "E loss:  0.4897022247314453\n",
      "G loss: 0.2106708288192749\n",
      "E loss:  0.48840683698654175\n",
      "G loss: 0.21473081409931183\n",
      "Training Model  ...\n",
      "E loss:  0.4984630346298218\n",
      "G loss: 0.1967850625514984\n",
      "E loss:  0.5019776821136475\n",
      "G loss: 0.1946079134941101\n",
      "E loss:  0.49598243832588196\n",
      "G loss: 0.22613389790058136\n",
      "E loss:  0.5039797425270081\n",
      "G loss: 0.20251329243183136\n",
      "E loss:  0.5023932456970215\n",
      "G loss: 0.163700133562088\n",
      "Training Model  ...\n",
      "E loss:  0.42088884115219116\n",
      "G loss: 0.17202405631542206\n",
      "E loss:  0.41664764285087585\n",
      "G loss: 0.18690094351768494\n",
      "E loss:  0.42654451727867126\n",
      "G loss: 0.20984870195388794\n",
      "E loss:  0.4268887937068939\n",
      "G loss: 0.16470476984977722\n",
      "E loss:  0.4341951012611389\n",
      "G loss: 0.2073250263929367\n",
      "Training Model  ...\n",
      "E loss:  0.4856005311012268\n",
      "G loss: 0.20134186744689941\n",
      "E loss:  0.48937544226646423\n",
      "G loss: 0.23303578794002533\n",
      "E loss:  0.5020548105239868\n",
      "G loss: 0.22647996246814728\n",
      "E loss:  0.503206193447113\n",
      "G loss: 0.22047390043735504\n",
      "E loss:  0.4986926019191742\n",
      "G loss: 0.21095043420791626\n",
      "Training Model  ...\n",
      "E loss:  0.4817388951778412\n",
      "G loss: 0.24767768383026123\n",
      "E loss:  0.47586140036582947\n",
      "G loss: 0.20752450823783875\n",
      "E loss:  0.47683146595954895\n",
      "G loss: 0.22449062764644623\n",
      "E loss:  0.47818005084991455\n",
      "G loss: 0.20582835376262665\n",
      "E loss:  0.4709353446960449\n",
      "G loss: 0.22413289546966553\n",
      "Training Model  ...\n",
      "E loss:  0.5511882305145264\n",
      "G loss: 0.2381785660982132\n",
      "E loss:  0.5500166416168213\n",
      "G loss: 0.23911698162555695\n",
      "E loss:  0.5469372868537903\n",
      "G loss: 0.2501630187034607\n",
      "E loss:  0.548778235912323\n",
      "G loss: 0.2630184590816498\n",
      "E loss:  0.5536485314369202\n",
      "G loss: 0.2286076843738556\n",
      "Training Model  ...\n",
      "E loss:  0.47747907042503357\n",
      "G loss: 0.26184991002082825\n",
      "E loss:  0.48176684975624084\n",
      "G loss: 0.22100220620632172\n",
      "E loss:  0.48214319348335266\n",
      "G loss: 0.22493082284927368\n",
      "E loss:  0.47887012362480164\n",
      "G loss: 0.23577658832073212\n",
      "E loss:  0.48833030462265015\n",
      "G loss: 0.2377227544784546\n",
      "Training Model  ...\n",
      "E loss:  0.5010919570922852\n",
      "G loss: 0.19141033291816711\n",
      "E loss:  0.5101282000541687\n",
      "G loss: 0.24235652387142181\n",
      "E loss:  0.5117345452308655\n",
      "G loss: 0.23225830495357513\n",
      "E loss:  0.5068888664245605\n",
      "G loss: 0.200506329536438\n",
      "E loss:  0.511684775352478\n",
      "G loss: 0.21895456314086914\n",
      "Training Model  ...\n",
      "E loss:  0.4775882363319397\n",
      "G loss: 0.2377450168132782\n",
      "E loss:  0.4790268540382385\n",
      "G loss: 0.2275935411453247\n",
      "E loss:  0.4800286591053009\n",
      "G loss: 0.2390010505914688\n",
      "E loss:  0.48408401012420654\n",
      "G loss: 0.2238529473543167\n",
      "E loss:  0.4936786890029907\n",
      "G loss: 0.21505331993103027\n",
      "Training Model  ...\n",
      "E loss:  0.4702463448047638\n",
      "G loss: 0.2232784926891327\n",
      "E loss:  0.4688679277896881\n",
      "G loss: 0.20331063866615295\n",
      "E loss:  0.4656655788421631\n",
      "G loss: 0.23451033234596252\n",
      "E loss:  0.46802765130996704\n",
      "G loss: 0.20213648676872253\n",
      "E loss:  0.4725317060947418\n",
      "G loss: 0.21950939297676086\n",
      "Training Model  ...\n",
      "E loss:  0.4645707607269287\n",
      "G loss: 0.20821866393089294\n",
      "E loss:  0.458271861076355\n",
      "G loss: 0.20972342789173126\n",
      "E loss:  0.4516947865486145\n",
      "G loss: 0.22720232605934143\n",
      "E loss:  0.45962873101234436\n",
      "G loss: 0.18620483577251434\n",
      "E loss:  0.4602752923965454\n",
      "G loss: 0.19682003557682037\n",
      "Training Model  ...\n",
      "E loss:  0.489931583404541\n",
      "G loss: 0.24007609486579895\n",
      "E loss:  0.4717448055744171\n",
      "G loss: 0.26200971007347107\n",
      "E loss:  0.46542274951934814\n",
      "G loss: 0.29999616742134094\n",
      "E loss:  0.46823006868362427\n",
      "G loss: 0.25207021832466125\n",
      "E loss:  0.4741740822792053\n",
      "G loss: 0.3054477274417877\n",
      "Training Model  ...\n",
      "E loss:  0.5382226705551147\n",
      "G loss: 0.23171161115169525\n",
      "E loss:  0.5241422057151794\n",
      "G loss: 0.22903254628181458\n",
      "E loss:  0.5294883847236633\n",
      "G loss: 0.20665642619132996\n",
      "E loss:  0.538942277431488\n",
      "G loss: 0.20798204839229584\n",
      "E loss:  0.536638617515564\n",
      "G loss: 0.2396206110715866\n",
      "Training Model  ...\n",
      "E loss:  0.4899219572544098\n",
      "G loss: 0.20030467212200165\n",
      "E loss:  0.48349305987358093\n",
      "G loss: 0.20736660063266754\n",
      "E loss:  0.4887489974498749\n",
      "G loss: 0.21846510469913483\n",
      "E loss:  0.4936554729938507\n",
      "G loss: 0.21676105260849\n",
      "E loss:  0.4989657700061798\n",
      "G loss: 0.23213934898376465\n",
      "Training Model  ...\n",
      "E loss:  0.5228977799415588\n",
      "G loss: 0.20042601227760315\n",
      "E loss:  0.5281954407691956\n",
      "G loss: 0.21295279264450073\n",
      "E loss:  0.5225809812545776\n",
      "G loss: 0.18099741637706757\n",
      "E loss:  0.5277537703514099\n",
      "G loss: 0.17487049102783203\n",
      "E loss:  0.529414176940918\n",
      "G loss: 0.19825683534145355\n",
      "Training Model  ...\n",
      "E loss:  0.5558822154998779\n",
      "G loss: 0.1845051348209381\n",
      "E loss:  0.5587594509124756\n",
      "G loss: 0.26439371705055237\n",
      "E loss:  0.5590315461158752\n",
      "G loss: 0.3127226233482361\n",
      "E loss:  0.5695849657058716\n",
      "G loss: 0.253755658864975\n",
      "E loss:  0.5669306516647339\n",
      "G loss: 0.2516487240791321\n",
      "Training Model  ...\n",
      "E loss:  0.5141668915748596\n",
      "G loss: 0.23530203104019165\n",
      "E loss:  0.5017716884613037\n",
      "G loss: 0.21590328216552734\n",
      "E loss:  0.5052347183227539\n",
      "G loss: 0.2048090696334839\n",
      "E loss:  0.49643853306770325\n",
      "G loss: 0.16619360446929932\n",
      "E loss:  0.49429309368133545\n",
      "G loss: 0.1865568310022354\n",
      "Training Model  ...\n",
      "E loss:  0.5642282366752625\n",
      "G loss: 0.20274688303470612\n",
      "E loss:  0.5542045831680298\n",
      "G loss: 0.2700914144515991\n",
      "E loss:  0.556079626083374\n",
      "G loss: 0.31898993253707886\n",
      "E loss:  0.5616707801818848\n",
      "G loss: 0.28797483444213867\n",
      "E loss:  0.5568785071372986\n",
      "G loss: 0.25577491521835327\n",
      "Training Model  ...\n",
      "E loss:  0.455921471118927\n",
      "G loss: 0.24338677525520325\n",
      "E loss:  0.4554000794887543\n",
      "G loss: 0.2312682569026947\n",
      "E loss:  0.4510696232318878\n",
      "G loss: 0.21929071843624115\n",
      "E loss:  0.45575863122940063\n",
      "G loss: 0.23614105582237244\n",
      "E loss:  0.45084911584854126\n",
      "G loss: 0.2349255532026291\n",
      "Training Model  ...\n",
      "E loss:  0.438890278339386\n",
      "G loss: 0.261639803647995\n",
      "E loss:  0.44397008419036865\n",
      "G loss: 0.210245281457901\n",
      "E loss:  0.4553505778312683\n",
      "G loss: 0.21501681208610535\n",
      "E loss:  0.460359126329422\n",
      "G loss: 0.2172166258096695\n",
      "E loss:  0.47078201174736023\n",
      "G loss: 0.22199133038520813\n",
      "Training Model  ...\n",
      "E loss:  0.4548419713973999\n",
      "G loss: 0.18878431618213654\n",
      "E loss:  0.4637574553489685\n",
      "G loss: 0.18909533321857452\n",
      "E loss:  0.4767662286758423\n",
      "G loss: 0.20607006549835205\n",
      "E loss:  0.4729744493961334\n",
      "G loss: 0.2013576477766037\n",
      "E loss:  0.47509628534317017\n",
      "G loss: 0.2124391794204712\n",
      "Training Model  ...\n",
      "E loss:  0.4839358925819397\n",
      "G loss: 0.211173415184021\n",
      "E loss:  0.46838659048080444\n",
      "G loss: 0.21690621972084045\n",
      "E loss:  0.46381106972694397\n",
      "G loss: 0.21133074164390564\n",
      "E loss:  0.466724693775177\n",
      "G loss: 0.22522442042827606\n",
      "E loss:  0.46271491050720215\n",
      "G loss: 0.2008494734764099\n",
      "Training Model  ...\n",
      "E loss:  0.4646214246749878\n",
      "G loss: 0.19329489767551422\n",
      "E loss:  0.45998644828796387\n",
      "G loss: 0.1518402099609375\n",
      "E loss:  0.4731086194515228\n",
      "G loss: 0.18627844750881195\n",
      "E loss:  0.4811092019081116\n",
      "G loss: 0.1777275800704956\n",
      "E loss:  0.4813152551651001\n",
      "G loss: 0.19214266538619995\n",
      "Training Model  ...\n",
      "E loss:  0.515192449092865\n",
      "G loss: 0.18371322751045227\n",
      "E loss:  0.5066654086112976\n",
      "G loss: 0.21631555259227753\n",
      "E loss:  0.5061774253845215\n",
      "G loss: 0.21976597607135773\n",
      "E loss:  0.5078639388084412\n",
      "G loss: 0.2282549887895584\n",
      "E loss:  0.509665310382843\n",
      "G loss: 0.2231179028749466\n",
      "Training Model  ...\n",
      "E loss:  0.4912172555923462\n",
      "G loss: 0.22236177325248718\n",
      "E loss:  0.49574071168899536\n",
      "G loss: 0.21319687366485596\n",
      "E loss:  0.48419830203056335\n",
      "G loss: 0.23570752143859863\n",
      "E loss:  0.4990121126174927\n",
      "G loss: 0.2346380203962326\n",
      "E loss:  0.49185940623283386\n",
      "G loss: 0.2041313499212265\n",
      "Training Model  ...\n",
      "E loss:  0.4989193379878998\n",
      "G loss: 0.18150882422924042\n",
      "E loss:  0.5013692378997803\n",
      "G loss: 0.18693889677524567\n",
      "E loss:  0.4824703633785248\n",
      "G loss: 0.17749032378196716\n",
      "E loss:  0.4690653681755066\n",
      "G loss: 0.14816704392433167\n",
      "E loss:  0.46759235858917236\n",
      "G loss: 0.19832313060760498\n",
      "Training Model  ...\n",
      "E loss:  0.5060215592384338\n",
      "G loss: 0.19191819429397583\n",
      "E loss:  0.4897882640361786\n",
      "G loss: 0.20062412321567535\n",
      "E loss:  0.4795018434524536\n",
      "G loss: 0.24167576432228088\n",
      "E loss:  0.5081959962844849\n",
      "G loss: 0.23762518167495728\n",
      "E loss:  0.49575164914131165\n",
      "G loss: 0.22531990706920624\n",
      "Training Model  ...\n",
      "E loss:  0.49212783575057983\n",
      "G loss: 0.21225860714912415\n",
      "E loss:  0.4880622923374176\n",
      "G loss: 0.21204452216625214\n",
      "E loss:  0.4875229001045227\n",
      "G loss: 0.1818477362394333\n",
      "E loss:  0.48356249928474426\n",
      "G loss: 0.18892821669578552\n",
      "E loss:  0.47241824865341187\n",
      "G loss: 0.2430400401353836\n",
      "Training Model  ...\n",
      "E loss:  0.477064311504364\n",
      "G loss: 0.191087007522583\n",
      "E loss:  0.4857504963874817\n",
      "G loss: 0.17393773794174194\n",
      "E loss:  0.4761868715286255\n",
      "G loss: 0.21259373426437378\n",
      "E loss:  0.4719424843788147\n",
      "G loss: 0.18335582315921783\n",
      "E loss:  0.4831015169620514\n",
      "G loss: 0.18331782519817352\n",
      "Training Model  ...\n",
      "E loss:  0.431622713804245\n",
      "G loss: 0.18644371628761292\n",
      "E loss:  0.4326654076576233\n",
      "G loss: 0.19439533352851868\n",
      "E loss:  0.42299070954322815\n",
      "G loss: 0.18218228220939636\n",
      "E loss:  0.4338492751121521\n",
      "G loss: 0.23191240429878235\n",
      "E loss:  0.4429129958152771\n",
      "G loss: 0.1935388445854187\n",
      "Training Model  ...\n",
      "E loss:  0.4515003263950348\n",
      "G loss: 0.1863413155078888\n",
      "E loss:  0.46800777316093445\n",
      "G loss: 0.20936527848243713\n",
      "E loss:  0.46078842878341675\n",
      "G loss: 0.21019214391708374\n",
      "E loss:  0.4598537087440491\n",
      "G loss: 0.1924707442522049\n",
      "E loss:  0.4638058543205261\n",
      "G loss: 0.17995990812778473\n",
      "Training Model  ...\n",
      "E loss:  0.4562954902648926\n",
      "G loss: 0.20257769525051117\n",
      "E loss:  0.45128709077835083\n",
      "G loss: 0.2268836498260498\n",
      "E loss:  0.46335533261299133\n",
      "G loss: 0.22475576400756836\n",
      "E loss:  0.45541948080062866\n",
      "G loss: 0.2298343926668167\n",
      "E loss:  0.4569440484046936\n",
      "G loss: 0.2260187715291977\n",
      "Training Model  ...\n",
      "E loss:  0.43111056089401245\n",
      "G loss: 0.21508021652698517\n",
      "E loss:  0.43383538722991943\n",
      "G loss: 0.24618273973464966\n",
      "E loss:  0.42932969331741333\n",
      "G loss: 0.2117813229560852\n",
      "E loss:  0.4286329746246338\n",
      "G loss: 0.20419354736804962\n",
      "E loss:  0.4323020577430725\n",
      "G loss: 0.22417736053466797\n",
      "Training Model  ...\n",
      "E loss:  0.47801637649536133\n",
      "G loss: 0.21272262930870056\n",
      "E loss:  0.4778560996055603\n",
      "G loss: 0.21743075549602509\n",
      "E loss:  0.47647151350975037\n",
      "G loss: 0.22296756505966187\n",
      "E loss:  0.48389768600463867\n",
      "G loss: 0.22470897436141968\n",
      "E loss:  0.481528103351593\n",
      "G loss: 0.20591914653778076\n",
      "Training Model  ...\n",
      "E loss:  0.48640257120132446\n",
      "G loss: 0.20192833244800568\n",
      "E loss:  0.48011863231658936\n",
      "G loss: 0.17227302491664886\n",
      "E loss:  0.4841915965080261\n",
      "G loss: 0.20037037134170532\n",
      "E loss:  0.4705659747123718\n",
      "G loss: 0.18372298777103424\n",
      "E loss:  0.470691978931427\n",
      "G loss: 0.18018101155757904\n",
      "Training Model  ...\n",
      "E loss:  0.44829389452934265\n",
      "G loss: 0.1650848686695099\n",
      "E loss:  0.44280949234962463\n",
      "G loss: 0.2115025520324707\n",
      "E loss:  0.44327983260154724\n",
      "G loss: 0.1734054535627365\n",
      "E loss:  0.4419254660606384\n",
      "G loss: 0.20567023754119873\n",
      "E loss:  0.439242422580719\n",
      "G loss: 0.1962513029575348\n",
      "Training Model  ...\n",
      "E loss:  0.48743149638175964\n",
      "G loss: 0.19119437038898468\n",
      "E loss:  0.48369812965393066\n",
      "G loss: 0.1743185967206955\n",
      "E loss:  0.47848671674728394\n",
      "G loss: 0.20485477149486542\n",
      "E loss:  0.4772947430610657\n",
      "G loss: 0.19865812361240387\n",
      "E loss:  0.4662846624851227\n",
      "G loss: 0.18530313670635223\n",
      "Training Model  ...\n",
      "E loss:  0.5273165702819824\n",
      "G loss: 0.2270127832889557\n",
      "E loss:  0.5346962809562683\n",
      "G loss: 0.23038649559020996\n",
      "E loss:  0.5197805166244507\n",
      "G loss: 0.2820463478565216\n",
      "E loss:  0.5175447463989258\n",
      "G loss: 0.285327285528183\n",
      "E loss:  0.5116590261459351\n",
      "G loss: 0.2278110533952713\n",
      "Training Model  ...\n",
      "E loss:  0.5109508037567139\n",
      "G loss: 0.2448996752500534\n",
      "E loss:  0.5138815641403198\n",
      "G loss: 0.20652298629283905\n",
      "E loss:  0.5146337747573853\n",
      "G loss: 0.17512978613376617\n",
      "E loss:  0.5153135061264038\n",
      "G loss: 0.21776410937309265\n",
      "E loss:  0.5097366571426392\n",
      "G loss: 0.2226942926645279\n",
      "Training Model  ...\n",
      "E loss:  0.41996580362319946\n",
      "G loss: 0.17511604726314545\n",
      "E loss:  0.4234747290611267\n",
      "G loss: 0.17844022810459137\n",
      "E loss:  0.40975213050842285\n",
      "G loss: 0.17937377095222473\n",
      "E loss:  0.41200244426727295\n",
      "G loss: 0.16057607531547546\n",
      "E loss:  0.4264095723628998\n",
      "G loss: 0.19076937437057495\n",
      "Training Model  ...\n",
      "E loss:  0.496320515871048\n",
      "G loss: 0.19299033284187317\n",
      "E loss:  0.49185633659362793\n",
      "G loss: 0.22060424089431763\n",
      "E loss:  0.4981997609138489\n",
      "G loss: 0.2869604229927063\n",
      "E loss:  0.5030765533447266\n",
      "G loss: 0.24359196424484253\n",
      "E loss:  0.5101942420005798\n",
      "G loss: 0.28221240639686584\n",
      "Training Model  ...\n",
      "E loss:  0.48610323667526245\n",
      "G loss: 0.20524962246418\n",
      "E loss:  0.4667890965938568\n",
      "G loss: 0.197478786110878\n",
      "E loss:  0.45987072587013245\n",
      "G loss: 0.17385423183441162\n",
      "E loss:  0.46961989998817444\n",
      "G loss: 0.16264566779136658\n",
      "E loss:  0.4818401336669922\n",
      "G loss: 0.19353583455085754\n",
      "Training Model  ...\n",
      "E loss:  0.4576243460178375\n",
      "G loss: 0.19001159071922302\n",
      "E loss:  0.46585017442703247\n",
      "G loss: 0.23467567563056946\n",
      "E loss:  0.4673770070075989\n",
      "G loss: 0.2156473845243454\n",
      "E loss:  0.48321473598480225\n",
      "G loss: 0.280460923910141\n",
      "E loss:  0.4787617027759552\n",
      "G loss: 0.21956776082515717\n",
      "Training Model  ...\n",
      "E loss:  0.46006011962890625\n",
      "G loss: 0.18526799976825714\n",
      "E loss:  0.45138823986053467\n",
      "G loss: 0.16926322877407074\n",
      "E loss:  0.45503705739974976\n",
      "G loss: 0.16628123819828033\n",
      "E loss:  0.46454548835754395\n",
      "G loss: 0.13339973986148834\n",
      "E loss:  0.4704381823539734\n",
      "G loss: 0.15671990811824799\n",
      "Training Model  ...\n",
      "E loss:  0.5050215721130371\n",
      "G loss: 0.1890265792608261\n",
      "E loss:  0.5023953914642334\n",
      "G loss: 0.20581825077533722\n",
      "E loss:  0.4970163404941559\n",
      "G loss: 0.2153947502374649\n",
      "E loss:  0.4883200228214264\n",
      "G loss: 0.2566719353199005\n",
      "E loss:  0.49438855051994324\n",
      "G loss: 0.24411210417747498\n",
      "Training Model  ...\n",
      "E loss:  0.4462117850780487\n",
      "G loss: 0.23359712958335876\n",
      "E loss:  0.44160979986190796\n",
      "G loss: 0.23039215803146362\n",
      "E loss:  0.4291899502277374\n",
      "G loss: 0.1914244145154953\n",
      "E loss:  0.4304754137992859\n",
      "G loss: 0.1874937266111374\n",
      "E loss:  0.45168712735176086\n",
      "G loss: 0.20125338435173035\n",
      "Training Model  ...\n",
      "E loss:  0.4502532184123993\n",
      "G loss: 0.2438114881515503\n",
      "E loss:  0.4558359384536743\n",
      "G loss: 0.20439966022968292\n",
      "E loss:  0.44527196884155273\n",
      "G loss: 0.19696149230003357\n",
      "E loss:  0.43205496668815613\n",
      "G loss: 0.25711262226104736\n",
      "E loss:  0.4296673536300659\n",
      "G loss: 0.2109583020210266\n",
      "Training Model  ...\n",
      "E loss:  0.4803732633590698\n",
      "G loss: 0.23548120260238647\n",
      "E loss:  0.4828205108642578\n",
      "G loss: 0.223563089966774\n",
      "E loss:  0.4697319269180298\n",
      "G loss: 0.24581515789031982\n",
      "E loss:  0.4742124676704407\n",
      "G loss: 0.2664567232131958\n",
      "E loss:  0.4677509069442749\n",
      "G loss: 0.27014511823654175\n",
      "Training Model  ...\n",
      "E loss:  0.4312886893749237\n",
      "G loss: 0.2199729084968567\n",
      "E loss:  0.4283459782600403\n",
      "G loss: 0.19998136162757874\n",
      "E loss:  0.4239479899406433\n",
      "G loss: 0.17457762360572815\n",
      "E loss:  0.42737624049186707\n",
      "G loss: 0.20678290724754333\n",
      "E loss:  0.4261244833469391\n",
      "G loss: 0.17186897993087769\n",
      "Training Model  ...\n",
      "E loss:  0.46999219059944153\n",
      "G loss: 0.18235939741134644\n",
      "E loss:  0.463477224111557\n",
      "G loss: 0.19645634293556213\n",
      "E loss:  0.4677671194076538\n",
      "G loss: 0.22639703750610352\n",
      "E loss:  0.45555517077445984\n",
      "G loss: 0.19704961776733398\n",
      "E loss:  0.4555400609970093\n",
      "G loss: 0.22162693738937378\n",
      "Training Model  ...\n",
      "E loss:  0.48954400420188904\n",
      "G loss: 0.21662327647209167\n",
      "E loss:  0.4808945655822754\n",
      "G loss: 0.19639746844768524\n",
      "E loss:  0.4789142608642578\n",
      "G loss: 0.18919047713279724\n",
      "E loss:  0.47931161522865295\n",
      "G loss: 0.17680038511753082\n",
      "E loss:  0.4694795608520508\n",
      "G loss: 0.18535944819450378\n",
      "Training Model  ...\n",
      "E loss:  0.5106344223022461\n",
      "G loss: 0.2149205356836319\n",
      "E loss:  0.5174087285995483\n",
      "G loss: 0.2692481577396393\n",
      "E loss:  0.5009954571723938\n",
      "G loss: 0.3028208911418915\n",
      "E loss:  0.5054184794425964\n",
      "G loss: 0.30901646614074707\n",
      "E loss:  0.5022180676460266\n",
      "G loss: 0.2764474153518677\n",
      "Training Model  ...\n",
      "E loss:  0.4138331711292267\n",
      "G loss: 0.2651326656341553\n",
      "E loss:  0.4131174087524414\n",
      "G loss: 0.22093242406845093\n",
      "E loss:  0.4006224572658539\n",
      "G loss: 0.19869059324264526\n",
      "E loss:  0.40748724341392517\n",
      "G loss: 0.21646356582641602\n",
      "E loss:  0.41355907917022705\n",
      "G loss: 0.23409324884414673\n",
      "Training Model  ...\n",
      "E loss:  0.40386784076690674\n",
      "G loss: 0.23906897008419037\n",
      "E loss:  0.412714421749115\n",
      "G loss: 0.19452333450317383\n",
      "E loss:  0.41866177320480347\n",
      "G loss: 0.23568692803382874\n",
      "E loss:  0.4198218286037445\n",
      "G loss: 0.20902113616466522\n",
      "E loss:  0.416057288646698\n",
      "G loss: 0.21979016065597534\n",
      "Training Model  ...\n",
      "E loss:  0.44776731729507446\n",
      "G loss: 0.18604955077171326\n",
      "E loss:  0.4489995837211609\n",
      "G loss: 0.18990276753902435\n",
      "E loss:  0.435921311378479\n",
      "G loss: 0.18624450266361237\n",
      "E loss:  0.439364492893219\n",
      "G loss: 0.1617657095193863\n",
      "E loss:  0.4379727244377136\n",
      "G loss: 0.16597139835357666\n",
      "Training Model  ...\n",
      "E loss:  0.487704336643219\n",
      "G loss: 0.19205760955810547\n",
      "E loss:  0.48224693536758423\n",
      "G loss: 0.23678474128246307\n",
      "E loss:  0.4938884973526001\n",
      "G loss: 0.30202633142471313\n",
      "E loss:  0.5038332343101501\n",
      "G loss: 0.30945152044296265\n",
      "E loss:  0.5184817314147949\n",
      "G loss: 0.3111512064933777\n",
      "Training Model  ...\n",
      "E loss:  0.46919259428977966\n",
      "G loss: 0.25288641452789307\n",
      "E loss:  0.4620775878429413\n",
      "G loss: 0.23415908217430115\n",
      "E loss:  0.4620678424835205\n",
      "G loss: 0.2030419409275055\n",
      "E loss:  0.46875208616256714\n",
      "G loss: 0.20858582854270935\n",
      "E loss:  0.466621458530426\n",
      "G loss: 0.20728449523448944\n",
      "Training Model  ...\n",
      "E loss:  0.4848463237285614\n",
      "G loss: 0.21707014739513397\n",
      "E loss:  0.46064242720603943\n",
      "G loss: 0.21382683515548706\n",
      "E loss:  0.4651252031326294\n",
      "G loss: 0.20482489466667175\n",
      "E loss:  0.47001808881759644\n",
      "G loss: 0.21543322503566742\n",
      "E loss:  0.4514276087284088\n",
      "G loss: 0.2133604735136032\n",
      "Training Model  ...\n",
      "E loss:  0.46907109022140503\n",
      "G loss: 0.21637582778930664\n",
      "E loss:  0.4667041301727295\n",
      "G loss: 0.21435236930847168\n",
      "E loss:  0.4699988067150116\n",
      "G loss: 0.17706994712352753\n",
      "E loss:  0.46685096621513367\n",
      "G loss: 0.2271697223186493\n",
      "E loss:  0.46754026412963867\n",
      "G loss: 0.20622938871383667\n",
      "Training Model  ...\n",
      "E loss:  0.4495488107204437\n",
      "G loss: 0.23250989615917206\n",
      "E loss:  0.45614421367645264\n",
      "G loss: 0.21726016700267792\n",
      "E loss:  0.4510883688926697\n",
      "G loss: 0.26651230454444885\n",
      "E loss:  0.43428361415863037\n",
      "G loss: 0.2401486337184906\n",
      "E loss:  0.4326581060886383\n",
      "G loss: 0.23809576034545898\n",
      "Training Model  ...\n",
      "E loss:  0.479580819606781\n",
      "G loss: 0.21129168570041656\n",
      "E loss:  0.47743016481399536\n",
      "G loss: 0.231045201420784\n",
      "E loss:  0.48154217004776\n",
      "G loss: 0.2264547348022461\n",
      "E loss:  0.47602590918540955\n",
      "G loss: 0.20603060722351074\n",
      "E loss:  0.47194331884384155\n",
      "G loss: 0.21630732715129852\n",
      "Training Model  ...\n",
      "E loss:  0.4488752484321594\n",
      "G loss: 0.21430639922618866\n",
      "E loss:  0.4401668906211853\n",
      "G loss: 0.19829906523227692\n",
      "E loss:  0.4460109770298004\n",
      "G loss: 0.22638502717018127\n",
      "E loss:  0.43172305822372437\n",
      "G loss: 0.19371667504310608\n",
      "E loss:  0.4285397231578827\n",
      "G loss: 0.20306359231472015\n",
      "Training Model  ...\n",
      "E loss:  0.45188552141189575\n",
      "G loss: 0.20809686183929443\n",
      "E loss:  0.44552379846572876\n",
      "G loss: 0.2117835283279419\n",
      "E loss:  0.45375576615333557\n",
      "G loss: 0.21494834125041962\n",
      "E loss:  0.4484245181083679\n",
      "G loss: 0.20485734939575195\n",
      "E loss:  0.4375862181186676\n",
      "G loss: 0.2308947890996933\n",
      "Training Model  ...\n",
      "E loss:  0.47082728147506714\n",
      "G loss: 0.22017399966716766\n",
      "E loss:  0.46112051606178284\n",
      "G loss: 0.2495507001876831\n",
      "E loss:  0.4656989872455597\n",
      "G loss: 0.22919252514839172\n",
      "E loss:  0.45695561170578003\n",
      "G loss: 0.2329288125038147\n",
      "E loss:  0.45224910974502563\n",
      "G loss: 0.2292519062757492\n",
      "Training Model  ...\n",
      "E loss:  0.4567872881889343\n",
      "G loss: 0.24045996367931366\n",
      "E loss:  0.4356611371040344\n",
      "G loss: 0.24518150091171265\n",
      "E loss:  0.45022138953208923\n",
      "G loss: 0.18574640154838562\n",
      "E loss:  0.453286349773407\n",
      "G loss: 0.1852765828371048\n",
      "E loss:  0.43259677290916443\n",
      "G loss: 0.19315111637115479\n",
      "Training Model  ...\n",
      "E loss:  0.48145902156829834\n",
      "G loss: 0.20872682332992554\n",
      "E loss:  0.4998791217803955\n",
      "G loss: 0.19051097333431244\n",
      "E loss:  0.5088005661964417\n",
      "G loss: 0.2208656370639801\n",
      "E loss:  0.5061568021774292\n",
      "G loss: 0.23372484743595123\n",
      "E loss:  0.5059654116630554\n",
      "G loss: 0.22473379969596863\n",
      "Training Model  ...\n",
      "E loss:  0.4457661807537079\n",
      "G loss: 0.2506624162197113\n",
      "E loss:  0.4397059977054596\n",
      "G loss: 0.2594967186450958\n",
      "E loss:  0.44531652331352234\n",
      "G loss: 0.23558644950389862\n",
      "E loss:  0.4313177168369293\n",
      "G loss: 0.23981577157974243\n",
      "E loss:  0.43503475189208984\n",
      "G loss: 0.22459456324577332\n",
      "Training Model  ...\n",
      "E loss:  0.560667872428894\n",
      "G loss: 0.18966494500637054\n",
      "E loss:  0.5517241358757019\n",
      "G loss: 0.1895855963230133\n",
      "E loss:  0.5288382172584534\n",
      "G loss: 0.18023744225502014\n",
      "E loss:  0.5294198989868164\n",
      "G loss: 0.1694130003452301\n",
      "E loss:  0.533042311668396\n",
      "G loss: 0.21883147954940796\n",
      "Training Model  ...\n",
      "E loss:  0.4307953417301178\n",
      "G loss: 0.23596137762069702\n",
      "E loss:  0.43369245529174805\n",
      "G loss: 0.24666251242160797\n",
      "E loss:  0.4312087297439575\n",
      "G loss: 0.26386427879333496\n",
      "E loss:  0.43963080644607544\n",
      "G loss: 0.24450799822807312\n",
      "E loss:  0.441254198551178\n",
      "G loss: 0.21194866299629211\n",
      "Training Model  ...\n",
      "E loss:  0.44137218594551086\n",
      "G loss: 0.2120099663734436\n",
      "E loss:  0.44456204771995544\n",
      "G loss: 0.22328561544418335\n",
      "E loss:  0.43754610419273376\n",
      "G loss: 0.20406943559646606\n",
      "E loss:  0.4410293698310852\n",
      "G loss: 0.20198646187782288\n",
      "E loss:  0.44352230429649353\n",
      "G loss: 0.21348699927330017\n",
      "Training Model  ...\n",
      "E loss:  0.4627119302749634\n",
      "G loss: 0.2236381620168686\n",
      "E loss:  0.4536451995372772\n",
      "G loss: 0.261620432138443\n",
      "E loss:  0.4389899671077728\n",
      "G loss: 0.27531498670578003\n",
      "E loss:  0.44537875056266785\n",
      "G loss: 0.2604106068611145\n",
      "E loss:  0.4432918429374695\n",
      "G loss: 0.2698008716106415\n",
      "Training Model  ...\n",
      "E loss:  0.5200979113578796\n",
      "G loss: 0.21832683682441711\n",
      "E loss:  0.5178359150886536\n",
      "G loss: 0.1997608095407486\n",
      "E loss:  0.5010722875595093\n",
      "G loss: 0.1759635955095291\n",
      "E loss:  0.494624525308609\n",
      "G loss: 0.17758938670158386\n",
      "E loss:  0.49772247672080994\n",
      "G loss: 0.19574342668056488\n",
      "Training Model  ...\n",
      "E loss:  0.48730501532554626\n",
      "G loss: 0.23710349202156067\n",
      "E loss:  0.4810093641281128\n",
      "G loss: 0.31369709968566895\n",
      "E loss:  0.4778585433959961\n",
      "G loss: 0.332925021648407\n",
      "E loss:  0.4702589809894562\n",
      "G loss: 0.31661367416381836\n",
      "E loss:  0.47077906131744385\n",
      "G loss: 0.2739983797073364\n",
      "Training Model  ...\n",
      "E loss:  0.5305070281028748\n",
      "G loss: 0.22669439017772675\n",
      "E loss:  0.5237264633178711\n",
      "G loss: 0.22054722905158997\n",
      "E loss:  0.5194448232650757\n",
      "G loss: 0.17464162409305573\n",
      "E loss:  0.5180327892303467\n",
      "G loss: 0.1782950460910797\n",
      "E loss:  0.5104822516441345\n",
      "G loss: 0.20908445119857788\n",
      "Training Model  ...\n",
      "E loss:  0.4521262049674988\n",
      "G loss: 0.2179187387228012\n",
      "E loss:  0.4546510577201843\n",
      "G loss: 0.22746685147285461\n",
      "E loss:  0.4549180567264557\n",
      "G loss: 0.2618475556373596\n",
      "E loss:  0.4637487232685089\n",
      "G loss: 0.2336948662996292\n",
      "E loss:  0.4582763612270355\n",
      "G loss: 0.21638956665992737\n",
      "Training Model  ...\n",
      "E loss:  0.4896393120288849\n",
      "G loss: 0.24236412346363068\n",
      "E loss:  0.46195676922798157\n",
      "G loss: 0.3052715063095093\n",
      "E loss:  0.46556076407432556\n",
      "G loss: 0.2599976658821106\n",
      "E loss:  0.4780022203922272\n",
      "G loss: 0.27203482389450073\n",
      "E loss:  0.46761661767959595\n",
      "G loss: 0.23228715360164642\n",
      "Training Model  ...\n",
      "E loss:  0.4543759822845459\n",
      "G loss: 0.2591899037361145\n",
      "E loss:  0.44145604968070984\n",
      "G loss: 0.21241842210292816\n",
      "E loss:  0.4356483519077301\n",
      "G loss: 0.2130546271800995\n",
      "E loss:  0.4352014660835266\n",
      "G loss: 0.20844438672065735\n",
      "E loss:  0.4397042393684387\n",
      "G loss: 0.235963374376297\n",
      "Training Model  ...\n",
      "E loss:  0.5021714568138123\n",
      "G loss: 0.2242026925086975\n",
      "E loss:  0.5089039206504822\n",
      "G loss: 0.2429507076740265\n",
      "E loss:  0.5071420073509216\n",
      "G loss: 0.23420697450637817\n",
      "E loss:  0.506912887096405\n",
      "G loss: 0.21895936131477356\n",
      "E loss:  0.521161675453186\n",
      "G loss: 0.24400284886360168\n",
      "Training Model  ...\n",
      "E loss:  0.43061336874961853\n",
      "G loss: 0.2193286120891571\n",
      "E loss:  0.4334632158279419\n",
      "G loss: 0.1985403597354889\n",
      "E loss:  0.43095168471336365\n",
      "G loss: 0.25158819556236267\n",
      "E loss:  0.43181079626083374\n",
      "G loss: 0.22401989996433258\n",
      "E loss:  0.43658047914505005\n",
      "G loss: 0.25059524178504944\n",
      "Training Model  ...\n",
      "E loss:  0.44400617480278015\n",
      "G loss: 0.23660586774349213\n",
      "E loss:  0.43544483184814453\n",
      "G loss: 0.2311522364616394\n",
      "E loss:  0.4289686679840088\n",
      "G loss: 0.25673073530197144\n",
      "E loss:  0.42955970764160156\n",
      "G loss: 0.22592225670814514\n",
      "E loss:  0.42760246992111206\n",
      "G loss: 0.23024895787239075\n",
      "Training Model  ...\n",
      "E loss:  0.47587794065475464\n",
      "G loss: 0.2536394000053406\n",
      "E loss:  0.48659566044807434\n",
      "G loss: 0.2693594992160797\n",
      "E loss:  0.4988299310207367\n",
      "G loss: 0.2505417466163635\n",
      "E loss:  0.5056652426719666\n",
      "G loss: 0.28822171688079834\n",
      "E loss:  0.5007975101470947\n",
      "G loss: 0.2828550338745117\n",
      "Training Model  ...\n",
      "E loss:  0.46406883001327515\n",
      "G loss: 0.2295723557472229\n",
      "E loss:  0.459871768951416\n",
      "G loss: 0.19211210310459137\n",
      "E loss:  0.4582855701446533\n",
      "G loss: 0.18204253911972046\n",
      "E loss:  0.4539700150489807\n",
      "G loss: 0.18027791380882263\n",
      "E loss:  0.4550451934337616\n",
      "G loss: 0.20100031793117523\n",
      "Training Model  ...\n",
      "E loss:  0.4588310718536377\n",
      "G loss: 0.18823407590389252\n",
      "E loss:  0.4521624743938446\n",
      "G loss: 0.22393648326396942\n",
      "E loss:  0.45504412055015564\n",
      "G loss: 0.2275971919298172\n",
      "E loss:  0.44449150562286377\n",
      "G loss: 0.2790129780769348\n",
      "E loss:  0.4537040889263153\n",
      "G loss: 0.2179623693227768\n",
      "Training Model  ...\n",
      "E loss:  0.5294500589370728\n",
      "G loss: 0.24509994685649872\n",
      "E loss:  0.5227274298667908\n",
      "G loss: 0.23373691737651825\n",
      "E loss:  0.5089182257652283\n",
      "G loss: 0.19910016655921936\n",
      "E loss:  0.5154416561126709\n",
      "G loss: 0.21961192786693573\n",
      "E loss:  0.5246036648750305\n",
      "G loss: 0.24723683297634125\n",
      "Training Model  ...\n",
      "E loss:  0.475983202457428\n",
      "G loss: 0.24029678106307983\n",
      "E loss:  0.4661443531513214\n",
      "G loss: 0.3028813898563385\n",
      "E loss:  0.4684925675392151\n",
      "G loss: 0.3046414852142334\n",
      "E loss:  0.4612695574760437\n",
      "G loss: 0.32244873046875\n",
      "E loss:  0.460720419883728\n",
      "G loss: 0.28930357098579407\n",
      "Training Model  ...\n",
      "E loss:  0.5141626000404358\n",
      "G loss: 0.24302145838737488\n",
      "E loss:  0.5072686672210693\n",
      "G loss: 0.20607922971248627\n",
      "E loss:  0.5033586025238037\n",
      "G loss: 0.19769881665706635\n",
      "E loss:  0.5119748115539551\n",
      "G loss: 0.1831488013267517\n",
      "E loss:  0.5291171669960022\n",
      "G loss: 0.21661259233951569\n",
      "Training Model  ...\n",
      "E loss:  0.46693435311317444\n",
      "G loss: 0.21409055590629578\n",
      "E loss:  0.45531123876571655\n",
      "G loss: 0.2321314960718155\n",
      "E loss:  0.4565514922142029\n",
      "G loss: 0.24778114259243011\n",
      "E loss:  0.4498996436595917\n",
      "G loss: 0.2193942368030548\n",
      "E loss:  0.4426892101764679\n",
      "G loss: 0.21211373805999756\n",
      "Training Model  ...\n",
      "E loss:  0.4463028907775879\n",
      "G loss: 0.25776875019073486\n",
      "E loss:  0.43320977687835693\n",
      "G loss: 0.28148573637008667\n",
      "E loss:  0.44245511293411255\n",
      "G loss: 0.25579679012298584\n",
      "E loss:  0.4559492766857147\n",
      "G loss: 0.2862684428691864\n",
      "E loss:  0.4557451605796814\n",
      "G loss: 0.26108917593955994\n",
      "Training Model  ...\n",
      "E loss:  0.4907677173614502\n",
      "G loss: 0.2344408631324768\n",
      "E loss:  0.48377418518066406\n",
      "G loss: 0.1887144297361374\n",
      "E loss:  0.47241145372390747\n",
      "G loss: 0.17821002006530762\n",
      "E loss:  0.4662171006202698\n",
      "G loss: 0.17200122773647308\n",
      "E loss:  0.46777334809303284\n",
      "G loss: 0.2017068862915039\n",
      "Training Model  ...\n",
      "E loss:  0.3998638093471527\n",
      "G loss: 0.2179425060749054\n",
      "E loss:  0.3984726071357727\n",
      "G loss: 0.2378549575805664\n",
      "E loss:  0.41624534130096436\n",
      "G loss: 0.2837377190589905\n",
      "E loss:  0.4097360074520111\n",
      "G loss: 0.2586846947669983\n",
      "E loss:  0.40540915727615356\n",
      "G loss: 0.25046342611312866\n",
      "Training Model  ...\n",
      "E loss:  0.4429122805595398\n",
      "G loss: 0.25011271238327026\n",
      "E loss:  0.43845731019973755\n",
      "G loss: 0.21749161183834076\n",
      "E loss:  0.43792960047721863\n",
      "G loss: 0.21968957781791687\n",
      "E loss:  0.44632256031036377\n",
      "G loss: 0.20325149595737457\n",
      "E loss:  0.4453798532485962\n",
      "G loss: 0.19409304857254028\n",
      "Training Model  ...\n",
      "E loss:  0.5056896805763245\n",
      "G loss: 0.2194063514471054\n",
      "E loss:  0.492943674325943\n",
      "G loss: 0.23329460620880127\n",
      "E loss:  0.4859750270843506\n",
      "G loss: 0.22995968163013458\n",
      "E loss:  0.478647381067276\n",
      "G loss: 0.2219887673854828\n",
      "E loss:  0.47896403074264526\n",
      "G loss: 0.2773299515247345\n",
      "Training Model  ...\n",
      "E loss:  0.46988093852996826\n",
      "G loss: 0.22911888360977173\n",
      "E loss:  0.4664471447467804\n",
      "G loss: 0.2474205195903778\n",
      "E loss:  0.460006982088089\n",
      "G loss: 0.2493262141942978\n",
      "E loss:  0.4578258693218231\n",
      "G loss: 0.25582462549209595\n",
      "E loss:  0.4468379020690918\n",
      "G loss: 0.20710957050323486\n",
      "Training Model  ...\n",
      "E loss:  0.4988178014755249\n",
      "G loss: 0.20619060099124908\n",
      "E loss:  0.49432769417762756\n",
      "G loss: 0.17896147072315216\n",
      "E loss:  0.49846968054771423\n",
      "G loss: 0.18425612151622772\n",
      "E loss:  0.4967280626296997\n",
      "G loss: 0.18092264235019684\n",
      "E loss:  0.4905844032764435\n",
      "G loss: 0.19637367129325867\n",
      "Training Model  ...\n",
      "E loss:  0.462310254573822\n",
      "G loss: 0.2209518551826477\n",
      "E loss:  0.47503530979156494\n",
      "G loss: 0.2038385570049286\n",
      "E loss:  0.4686868488788605\n",
      "G loss: 0.24043288826942444\n",
      "E loss:  0.4862029552459717\n",
      "G loss: 0.22890715301036835\n",
      "E loss:  0.5002375841140747\n",
      "G loss: 0.2190445363521576\n",
      "Training Model  ...\n",
      "E loss:  0.4353879988193512\n",
      "G loss: 0.24783125519752502\n",
      "E loss:  0.4430510699748993\n",
      "G loss: 0.22225818037986755\n",
      "E loss:  0.44158363342285156\n",
      "G loss: 0.2228531539440155\n",
      "E loss:  0.4437951147556305\n",
      "G loss: 0.2602452039718628\n",
      "E loss:  0.43811798095703125\n",
      "G loss: 0.2384124994277954\n",
      "Training Model  ...\n",
      "E loss:  0.4867703914642334\n",
      "G loss: 0.2114051878452301\n",
      "E loss:  0.48415708541870117\n",
      "G loss: 0.18212904036045074\n",
      "E loss:  0.47211796045303345\n",
      "G loss: 0.18717530369758606\n",
      "E loss:  0.477882981300354\n",
      "G loss: 0.1841028928756714\n",
      "E loss:  0.46912533044815063\n",
      "G loss: 0.23038196563720703\n",
      "Training Model  ...\n",
      "E loss:  0.47272637486457825\n",
      "G loss: 0.24293489754199982\n",
      "E loss:  0.460080623626709\n",
      "G loss: 0.26577556133270264\n",
      "E loss:  0.46743980050086975\n",
      "G loss: 0.31165793538093567\n",
      "E loss:  0.47563958168029785\n",
      "G loss: 0.3051682114601135\n",
      "E loss:  0.47178179025650024\n",
      "G loss: 0.2594187557697296\n",
      "Training Model  ...\n",
      "E loss:  0.43226441740989685\n",
      "G loss: 0.25393441319465637\n",
      "E loss:  0.4220200181007385\n",
      "G loss: 0.22377368807792664\n",
      "E loss:  0.4305061101913452\n",
      "G loss: 0.1742088794708252\n",
      "E loss:  0.4227460026741028\n",
      "G loss: 0.17684200406074524\n",
      "E loss:  0.4156308174133301\n",
      "G loss: 0.25827088952064514\n",
      "Training Model  ...\n",
      "E loss:  0.4636252224445343\n",
      "G loss: 0.22255435585975647\n",
      "E loss:  0.4654269516468048\n",
      "G loss: 0.19630250334739685\n",
      "E loss:  0.4668227434158325\n",
      "G loss: 0.1888294219970703\n",
      "E loss:  0.45889028906822205\n",
      "G loss: 0.1863529086112976\n",
      "E loss:  0.45544731616973877\n",
      "G loss: 0.20014560222625732\n",
      "Training Model  ...\n",
      "E loss:  0.4585478603839874\n",
      "G loss: 0.1860659122467041\n",
      "E loss:  0.4513500928878784\n",
      "G loss: 0.2193223536014557\n",
      "E loss:  0.45200204849243164\n",
      "G loss: 0.23724938929080963\n",
      "E loss:  0.4551678001880646\n",
      "G loss: 0.2176666259765625\n",
      "E loss:  0.46248629689216614\n",
      "G loss: 0.2292654812335968\n",
      "Training Model  ...\n",
      "E loss:  0.47826316952705383\n",
      "G loss: 0.21133849024772644\n",
      "E loss:  0.4633762836456299\n",
      "G loss: 0.2039414644241333\n",
      "E loss:  0.46880850195884705\n",
      "G loss: 0.19094225764274597\n",
      "E loss:  0.4679323136806488\n",
      "G loss: 0.21309894323349\n",
      "E loss:  0.4767014682292938\n",
      "G loss: 0.22111181914806366\n",
      "Training Model  ...\n",
      "E loss:  0.44014352560043335\n",
      "G loss: 0.19922581315040588\n",
      "E loss:  0.4405917823314667\n",
      "G loss: 0.20965704321861267\n",
      "E loss:  0.4464055001735687\n",
      "G loss: 0.20315377414226532\n",
      "E loss:  0.4525117874145508\n",
      "G loss: 0.19495822489261627\n",
      "E loss:  0.4605673849582672\n",
      "G loss: 0.20612916350364685\n",
      "Training Model  ...\n",
      "E loss:  0.3909275233745575\n",
      "G loss: 0.21016494929790497\n",
      "E loss:  0.3912394046783447\n",
      "G loss: 0.2284374237060547\n",
      "E loss:  0.39046069979667664\n",
      "G loss: 0.24098631739616394\n",
      "E loss:  0.39941608905792236\n",
      "G loss: 0.23353037238121033\n",
      "E loss:  0.3925530016422272\n",
      "G loss: 0.23657788336277008\n",
      "Training Model  ...\n",
      "E loss:  0.41217485070228577\n",
      "G loss: 0.22759383916854858\n",
      "E loss:  0.41457098722457886\n",
      "G loss: 0.22428953647613525\n",
      "E loss:  0.4073789715766907\n",
      "G loss: 0.21128861606121063\n",
      "E loss:  0.4038768410682678\n",
      "G loss: 0.2575298845767975\n",
      "E loss:  0.4051826000213623\n",
      "G loss: 0.2564258575439453\n",
      "Training Model  ...\n",
      "E loss:  0.4550623893737793\n",
      "G loss: 0.21544887125492096\n",
      "E loss:  0.4441309869289398\n",
      "G loss: 0.17289681732654572\n",
      "E loss:  0.4471735954284668\n",
      "G loss: 0.1886860430240631\n",
      "E loss:  0.45174968242645264\n",
      "G loss: 0.1779157519340515\n",
      "E loss:  0.4673161208629608\n",
      "G loss: 0.14971458911895752\n",
      "Training Model  ...\n",
      "E loss:  0.4468822479248047\n",
      "G loss: 0.19072860479354858\n",
      "E loss:  0.43746429681777954\n",
      "G loss: 0.2566044330596924\n",
      "E loss:  0.43231379985809326\n",
      "G loss: 0.2965048551559448\n",
      "E loss:  0.44458216428756714\n",
      "G loss: 0.3118748664855957\n",
      "E loss:  0.4394623637199402\n",
      "G loss: 0.2145051509141922\n",
      "Training Model  ...\n",
      "E loss:  0.4180462956428528\n",
      "G loss: 0.21490596234798431\n",
      "E loss:  0.4191153943538666\n",
      "G loss: 0.2340143471956253\n",
      "E loss:  0.42023175954818726\n",
      "G loss: 0.21501070261001587\n",
      "E loss:  0.4219296872615814\n",
      "G loss: 0.2205314040184021\n",
      "E loss:  0.4341587722301483\n",
      "G loss: 0.2014445662498474\n",
      "Training Model  ...\n",
      "E loss:  0.46530038118362427\n",
      "G loss: 0.24275171756744385\n",
      "E loss:  0.4683339595794678\n",
      "G loss: 0.20613697171211243\n",
      "E loss:  0.4633706510066986\n",
      "G loss: 0.22521568834781647\n",
      "E loss:  0.452987402677536\n",
      "G loss: 0.24705754220485687\n",
      "E loss:  0.45075488090515137\n",
      "G loss: 0.22102341055870056\n",
      "Training Model  ...\n",
      "E loss:  0.48044097423553467\n",
      "G loss: 0.1896384358406067\n",
      "E loss:  0.48480191826820374\n",
      "G loss: 0.22394154965877533\n",
      "E loss:  0.4851369857788086\n",
      "G loss: 0.22431215643882751\n",
      "E loss:  0.47274520993232727\n",
      "G loss: 0.23172518610954285\n",
      "E loss:  0.478365033864975\n",
      "G loss: 0.2235500067472458\n",
      "Training Model  ...\n",
      "E loss:  0.42525455355644226\n",
      "G loss: 0.21166262030601501\n",
      "E loss:  0.42022693157196045\n",
      "G loss: 0.2042320966720581\n",
      "E loss:  0.420746773481369\n",
      "G loss: 0.2126425802707672\n",
      "E loss:  0.4113435745239258\n",
      "G loss: 0.21788369119167328\n",
      "E loss:  0.4157804250717163\n",
      "G loss: 0.1876828521490097\n",
      "Training Model  ...\n",
      "E loss:  0.4505605697631836\n",
      "G loss: 0.2109277844429016\n",
      "E loss:  0.4475782811641693\n",
      "G loss: 0.2302331030368805\n",
      "E loss:  0.437095582485199\n",
      "G loss: 0.222926065325737\n",
      "E loss:  0.42615169286727905\n",
      "G loss: 0.24456578493118286\n",
      "E loss:  0.42991459369659424\n",
      "G loss: 0.19767950475215912\n",
      "Training Model  ...\n",
      "E loss:  0.47416093945503235\n",
      "G loss: 0.23706188797950745\n",
      "E loss:  0.4778406322002411\n",
      "G loss: 0.24188165366649628\n",
      "E loss:  0.4758394658565521\n",
      "G loss: 0.2404187172651291\n",
      "E loss:  0.4821006655693054\n",
      "G loss: 0.23756274580955505\n",
      "E loss:  0.4923422336578369\n",
      "G loss: 0.22227531671524048\n",
      "Training Model  ...\n",
      "E loss:  0.4686436951160431\n",
      "G loss: 0.2055635303258896\n",
      "E loss:  0.46020200848579407\n",
      "G loss: 0.18209928274154663\n",
      "E loss:  0.47604209184646606\n",
      "G loss: 0.18983541429042816\n",
      "E loss:  0.46827104687690735\n",
      "G loss: 0.16694845259189606\n",
      "E loss:  0.46377667784690857\n",
      "G loss: 0.213018000125885\n",
      "Training Model  ...\n",
      "E loss:  0.42714402079582214\n",
      "G loss: 0.2076488733291626\n",
      "E loss:  0.4213551878929138\n",
      "G loss: 0.195377379655838\n",
      "E loss:  0.43813133239746094\n",
      "G loss: 0.21750089526176453\n",
      "E loss:  0.4401639401912689\n",
      "G loss: 0.2222394049167633\n",
      "E loss:  0.43515804409980774\n",
      "G loss: 0.19694820046424866\n",
      "Training Model  ...\n",
      "E loss:  0.4423733949661255\n",
      "G loss: 0.2533264756202698\n",
      "E loss:  0.43911296129226685\n",
      "G loss: 0.2589218020439148\n",
      "E loss:  0.4306371212005615\n",
      "G loss: 0.2515854239463806\n",
      "E loss:  0.42917683720588684\n",
      "G loss: 0.2231033593416214\n",
      "E loss:  0.43298953771591187\n",
      "G loss: 0.24338330328464508\n",
      "Training Model  ...\n",
      "E loss:  0.479795902967453\n",
      "G loss: 0.22990737855434418\n",
      "E loss:  0.46491995453834534\n",
      "G loss: 0.19740130007266998\n",
      "E loss:  0.46012380719184875\n",
      "G loss: 0.23238638043403625\n",
      "E loss:  0.4545374810695648\n",
      "G loss: 0.2451317459344864\n",
      "E loss:  0.4562259316444397\n",
      "G loss: 0.22996172308921814\n",
      "Training Model  ...\n",
      "E loss:  0.44734305143356323\n",
      "G loss: 0.18161630630493164\n",
      "E loss:  0.45211929082870483\n",
      "G loss: 0.20331528782844543\n",
      "E loss:  0.45376959443092346\n",
      "G loss: 0.20955979824066162\n",
      "E loss:  0.45471087098121643\n",
      "G loss: 0.187726229429245\n",
      "E loss:  0.4622558355331421\n",
      "G loss: 0.22171422839164734\n",
      "Training Model  ...\n",
      "E loss:  0.444868266582489\n",
      "G loss: 0.19478361308574677\n",
      "E loss:  0.45193690061569214\n",
      "G loss: 0.241756871342659\n",
      "E loss:  0.44519105553627014\n",
      "G loss: 0.2568276822566986\n",
      "E loss:  0.4329064190387726\n",
      "G loss: 0.2152891606092453\n",
      "E loss:  0.4386511445045471\n",
      "G loss: 0.24276211857795715\n",
      "Training Model  ...\n",
      "E loss:  0.43712958693504333\n",
      "G loss: 0.2286551594734192\n",
      "E loss:  0.42338481545448303\n",
      "G loss: 0.1879929006099701\n",
      "E loss:  0.4245922267436981\n",
      "G loss: 0.21007567644119263\n",
      "E loss:  0.4227176606655121\n",
      "G loss: 0.21697567403316498\n",
      "E loss:  0.40970349311828613\n",
      "G loss: 0.21601681411266327\n",
      "Training Model  ...\n",
      "E loss:  0.4498051106929779\n",
      "G loss: 0.21788892149925232\n",
      "E loss:  0.4499484598636627\n",
      "G loss: 0.1867934912443161\n",
      "E loss:  0.44271060824394226\n",
      "G loss: 0.20123684406280518\n",
      "E loss:  0.4506409764289856\n",
      "G loss: 0.19946707785129547\n",
      "E loss:  0.44254422187805176\n",
      "G loss: 0.22592312097549438\n",
      "Training Model  ...\n",
      "E loss:  0.4661027789115906\n",
      "G loss: 0.19330249726772308\n",
      "E loss:  0.46622371673583984\n",
      "G loss: 0.21161726117134094\n",
      "E loss:  0.46344858407974243\n",
      "G loss: 0.2604757249355316\n",
      "E loss:  0.4589129686355591\n",
      "G loss: 0.22692358493804932\n",
      "E loss:  0.4613606631755829\n",
      "G loss: 0.2164202332496643\n",
      "Training Model  ...\n",
      "E loss:  0.50921630859375\n",
      "G loss: 0.24976924061775208\n",
      "E loss:  0.4978567957878113\n",
      "G loss: 0.2090870887041092\n",
      "E loss:  0.5003877282142639\n",
      "G loss: 0.25112032890319824\n",
      "E loss:  0.49388283491134644\n",
      "G loss: 0.24942922592163086\n",
      "E loss:  0.49278566241264343\n",
      "G loss: 0.21020421385765076\n",
      "Training Model  ...\n",
      "E loss:  0.4341851472854614\n",
      "G loss: 0.18103092908859253\n",
      "E loss:  0.4266989827156067\n",
      "G loss: 0.19234324991703033\n",
      "E loss:  0.4205186665058136\n",
      "G loss: 0.16395582258701324\n",
      "E loss:  0.42081573605537415\n",
      "G loss: 0.1649937927722931\n",
      "E loss:  0.4219135046005249\n",
      "G loss: 0.19536617398262024\n",
      "Training Model  ...\n",
      "E loss:  0.4349542558193207\n",
      "G loss: 0.19484828412532806\n",
      "E loss:  0.4364696741104126\n",
      "G loss: 0.23126111924648285\n",
      "E loss:  0.435348778963089\n",
      "G loss: 0.20913080871105194\n",
      "E loss:  0.4333905577659607\n",
      "G loss: 0.20807453989982605\n",
      "E loss:  0.42841583490371704\n",
      "G loss: 0.20524169504642487\n",
      "Training Model  ...\n",
      "E loss:  0.41734474897384644\n",
      "G loss: 0.22694236040115356\n",
      "E loss:  0.41171175241470337\n",
      "G loss: 0.21073082089424133\n",
      "E loss:  0.41350769996643066\n",
      "G loss: 0.24349486827850342\n",
      "E loss:  0.405707985162735\n",
      "G loss: 0.17500272393226624\n",
      "E loss:  0.402368426322937\n",
      "G loss: 0.1821504831314087\n",
      "Training Model  ...\n",
      "E loss:  0.4413158595561981\n",
      "G loss: 0.16825905442237854\n",
      "E loss:  0.4281855821609497\n",
      "G loss: 0.20716550946235657\n",
      "E loss:  0.42388007044792175\n",
      "G loss: 0.1897273063659668\n",
      "E loss:  0.42421993613243103\n",
      "G loss: 0.2250790297985077\n",
      "E loss:  0.42045608162879944\n",
      "G loss: 0.217146098613739\n",
      "Training Model  ...\n",
      "E loss:  0.42906811833381653\n",
      "G loss: 0.28177177906036377\n",
      "E loss:  0.4287601411342621\n",
      "G loss: 0.2534525394439697\n",
      "E loss:  0.4311363697052002\n",
      "G loss: 0.31650781631469727\n",
      "E loss:  0.4252314865589142\n",
      "G loss: 0.2693535387516022\n",
      "E loss:  0.41727301478385925\n",
      "G loss: 0.27379900217056274\n",
      "Training Model  ...\n",
      "E loss:  0.4385393261909485\n",
      "G loss: 0.23078273236751556\n",
      "E loss:  0.4359012246131897\n",
      "G loss: 0.21341338753700256\n",
      "E loss:  0.4326011836528778\n",
      "G loss: 0.1844644844532013\n",
      "E loss:  0.43225598335266113\n",
      "G loss: 0.21611738204956055\n",
      "E loss:  0.43207651376724243\n",
      "G loss: 0.21119168400764465\n",
      "Training Model  ...\n",
      "E loss:  0.44865483045578003\n",
      "G loss: 0.21476350724697113\n",
      "E loss:  0.45572730898857117\n",
      "G loss: 0.20053456723690033\n",
      "E loss:  0.45181524753570557\n",
      "G loss: 0.1687234342098236\n",
      "E loss:  0.4478384256362915\n",
      "G loss: 0.1739191710948944\n",
      "E loss:  0.42161062359809875\n",
      "G loss: 0.2023753523826599\n",
      "Training Model  ...\n",
      "E loss:  0.46896541118621826\n",
      "G loss: 0.1774309277534485\n",
      "E loss:  0.46377724409103394\n",
      "G loss: 0.20574727654457092\n",
      "E loss:  0.46371105313301086\n",
      "G loss: 0.26589760184288025\n",
      "E loss:  0.4754835367202759\n",
      "G loss: 0.26173320412635803\n",
      "E loss:  0.4702185392379761\n",
      "G loss: 0.2146085947751999\n",
      "Training Model  ...\n",
      "E loss:  0.4512180984020233\n",
      "G loss: 0.20914733409881592\n",
      "E loss:  0.4494529068470001\n",
      "G loss: 0.2222166359424591\n",
      "E loss:  0.44906488060951233\n",
      "G loss: 0.1964111626148224\n",
      "E loss:  0.4539143145084381\n",
      "G loss: 0.1828499436378479\n",
      "E loss:  0.4505201280117035\n",
      "G loss: 0.1803760826587677\n",
      "Training Model  ...\n",
      "E loss:  0.42161208391189575\n",
      "G loss: 0.194422647356987\n",
      "E loss:  0.4244476556777954\n",
      "G loss: 0.1905302107334137\n",
      "E loss:  0.41875869035720825\n",
      "G loss: 0.22062796354293823\n",
      "E loss:  0.4144465923309326\n",
      "G loss: 0.20770718157291412\n",
      "E loss:  0.415096640586853\n",
      "G loss: 0.22165663540363312\n",
      "Training Model  ...\n",
      "E loss:  0.3965546190738678\n",
      "G loss: 0.20393002033233643\n",
      "E loss:  0.4051749110221863\n",
      "G loss: 0.2157490849494934\n",
      "E loss:  0.4131113886833191\n",
      "G loss: 0.236336350440979\n",
      "E loss:  0.40845048427581787\n",
      "G loss: 0.2252771407365799\n",
      "E loss:  0.40356406569480896\n",
      "G loss: 0.21382027864456177\n",
      "Training Model  ...\n",
      "E loss:  0.4331364631652832\n",
      "G loss: 0.18443986773490906\n",
      "E loss:  0.42648038268089294\n",
      "G loss: 0.20474326610565186\n",
      "E loss:  0.42450273036956787\n",
      "G loss: 0.19551901519298553\n",
      "E loss:  0.4275612533092499\n",
      "G loss: 0.21956977248191833\n",
      "E loss:  0.41369912028312683\n",
      "G loss: 0.1812441498041153\n",
      "Training Model  ...\n",
      "E loss:  0.4689989686012268\n",
      "G loss: 0.19778235256671906\n",
      "E loss:  0.4585407078266144\n",
      "G loss: 0.2180216908454895\n",
      "E loss:  0.4445032477378845\n",
      "G loss: 0.21256081759929657\n",
      "E loss:  0.4447619318962097\n",
      "G loss: 0.21177418529987335\n",
      "E loss:  0.44732344150543213\n",
      "G loss: 0.21248966455459595\n",
      "Training Model  ...\n",
      "E loss:  0.4537906348705292\n",
      "G loss: 0.23843927681446075\n",
      "E loss:  0.4408448338508606\n",
      "G loss: 0.28257328271865845\n",
      "E loss:  0.445208877325058\n",
      "G loss: 0.28356385231018066\n",
      "E loss:  0.4522267282009125\n",
      "G loss: 0.30832570791244507\n",
      "E loss:  0.43976208567619324\n",
      "G loss: 0.24521273374557495\n",
      "Training Model  ...\n",
      "E loss:  0.4501839280128479\n",
      "G loss: 0.21628770232200623\n",
      "E loss:  0.44270190596580505\n",
      "G loss: 0.1616264134645462\n",
      "E loss:  0.4330628514289856\n",
      "G loss: 0.2019253671169281\n",
      "E loss:  0.4363511800765991\n",
      "G loss: 0.21336662769317627\n",
      "E loss:  0.4367005527019501\n",
      "G loss: 0.19177548587322235\n",
      "Training Model  ...\n",
      "E loss:  0.4124036729335785\n",
      "G loss: 0.2293672263622284\n",
      "E loss:  0.4028768539428711\n",
      "G loss: 0.24417829513549805\n",
      "E loss:  0.41980618238449097\n",
      "G loss: 0.30000120401382446\n",
      "E loss:  0.42148542404174805\n",
      "G loss: 0.2400408834218979\n",
      "E loss:  0.4239872097969055\n",
      "G loss: 0.2443523108959198\n",
      "Training Model  ...\n",
      "E loss:  0.4444769620895386\n",
      "G loss: 0.23484018445014954\n",
      "E loss:  0.44866830110549927\n",
      "G loss: 0.22353366017341614\n",
      "E loss:  0.45321124792099\n",
      "G loss: 0.23290684819221497\n",
      "E loss:  0.4614620804786682\n",
      "G loss: 0.21775272488594055\n",
      "E loss:  0.46102672815322876\n",
      "G loss: 0.19517210125923157\n",
      "Training Model  ...\n",
      "E loss:  0.43883371353149414\n",
      "G loss: 0.1897462159395218\n",
      "E loss:  0.4317387342453003\n",
      "G loss: 0.22187212109565735\n",
      "E loss:  0.4318920075893402\n",
      "G loss: 0.20688138902187347\n",
      "E loss:  0.4283885955810547\n",
      "G loss: 0.1995600312948227\n",
      "E loss:  0.42996758222579956\n",
      "G loss: 0.22320425510406494\n",
      "Training Model  ...\n",
      "E loss:  0.42499297857284546\n",
      "G loss: 0.19994661211967468\n",
      "E loss:  0.42100876569747925\n",
      "G loss: 0.19792911410331726\n",
      "E loss:  0.42147737741470337\n",
      "G loss: 0.22554422914981842\n",
      "E loss:  0.4210512638092041\n",
      "G loss: 0.22824722528457642\n",
      "E loss:  0.4179839491844177\n",
      "G loss: 0.20535621047019958\n",
      "Training Model  ...\n",
      "E loss:  0.4289693832397461\n",
      "G loss: 0.23950165510177612\n",
      "E loss:  0.42916303873062134\n",
      "G loss: 0.1857472062110901\n",
      "E loss:  0.43098247051239014\n",
      "G loss: 0.2013281285762787\n",
      "E loss:  0.424694687128067\n",
      "G loss: 0.23856234550476074\n",
      "E loss:  0.4252932667732239\n",
      "G loss: 0.18748846650123596\n",
      "Training Model  ...\n",
      "E loss:  0.4292866587638855\n",
      "G loss: 0.21362605690956116\n",
      "E loss:  0.4309479594230652\n",
      "G loss: 0.24205780029296875\n",
      "E loss:  0.4273235499858856\n",
      "G loss: 0.2549408972263336\n",
      "E loss:  0.43155795335769653\n",
      "G loss: 0.2152220755815506\n",
      "E loss:  0.42731931805610657\n",
      "G loss: 0.24386659264564514\n",
      "Training Model  ...\n",
      "E loss:  0.43466922640800476\n",
      "G loss: 0.24962462484836578\n",
      "E loss:  0.4306327998638153\n",
      "G loss: 0.23494316637516022\n",
      "E loss:  0.43492037057876587\n",
      "G loss: 0.23342274129390717\n",
      "E loss:  0.4298335313796997\n",
      "G loss: 0.27889174222946167\n",
      "E loss:  0.42810317873954773\n",
      "G loss: 0.2251054048538208\n",
      "Training Model  ...\n",
      "E loss:  0.48573946952819824\n",
      "G loss: 0.22482846677303314\n",
      "E loss:  0.4771449863910675\n",
      "G loss: 0.1887659877538681\n",
      "E loss:  0.46976345777511597\n",
      "G loss: 0.19488221406936646\n",
      "E loss:  0.4751160442829132\n",
      "G loss: 0.18318834900856018\n",
      "E loss:  0.47390979528427124\n",
      "G loss: 0.19318222999572754\n",
      "Training Model  ...\n",
      "E loss:  0.4166202247142792\n",
      "G loss: 0.21697565913200378\n",
      "E loss:  0.41136860847473145\n",
      "G loss: 0.1956450641155243\n",
      "E loss:  0.4130467176437378\n",
      "G loss: 0.24050180613994598\n",
      "E loss:  0.4190230965614319\n",
      "G loss: 0.232417494058609\n",
      "E loss:  0.41375184059143066\n",
      "G loss: 0.20465168356895447\n",
      "Training Model  ...\n",
      "E loss:  0.42501524090766907\n",
      "G loss: 0.2211821973323822\n",
      "E loss:  0.4369713366031647\n",
      "G loss: 0.23711159825325012\n",
      "E loss:  0.44286713004112244\n",
      "G loss: 0.21001558005809784\n",
      "E loss:  0.4339732825756073\n",
      "G loss: 0.22916315495967865\n",
      "E loss:  0.4313313364982605\n",
      "G loss: 0.2441418170928955\n",
      "Training Model  ...\n",
      "E loss:  0.43596532940864563\n",
      "G loss: 0.2232443392276764\n",
      "E loss:  0.42886313796043396\n",
      "G loss: 0.24174864590168\n",
      "E loss:  0.42816102504730225\n",
      "G loss: 0.19900260865688324\n",
      "E loss:  0.4202829897403717\n",
      "G loss: 0.23247382044792175\n",
      "E loss:  0.4242705702781677\n",
      "G loss: 0.19137854874134064\n",
      "Training Model  ...\n",
      "E loss:  0.47344473004341125\n",
      "G loss: 0.23006582260131836\n",
      "E loss:  0.46778935194015503\n",
      "G loss: 0.21663838624954224\n",
      "E loss:  0.45941808819770813\n",
      "G loss: 0.2344040721654892\n",
      "E loss:  0.45314157009124756\n",
      "G loss: 0.23455142974853516\n",
      "E loss:  0.45141327381134033\n",
      "G loss: 0.22535933554172516\n",
      "Training Model  ...\n",
      "E loss:  0.4324551522731781\n",
      "G loss: 0.23278246819972992\n",
      "E loss:  0.44055330753326416\n",
      "G loss: 0.20045840740203857\n",
      "E loss:  0.4292810261249542\n",
      "G loss: 0.23488900065422058\n",
      "E loss:  0.43232154846191406\n",
      "G loss: 0.23477256298065186\n",
      "E loss:  0.4308129549026489\n",
      "G loss: 0.2220403403043747\n",
      "Training Model  ...\n",
      "E loss:  0.47859036922454834\n",
      "G loss: 0.2451062947511673\n",
      "E loss:  0.47228243947029114\n",
      "G loss: 0.2289183884859085\n",
      "E loss:  0.474253386259079\n",
      "G loss: 0.25355368852615356\n",
      "E loss:  0.46908435225486755\n",
      "G loss: 0.24225327372550964\n",
      "E loss:  0.4676097631454468\n",
      "G loss: 0.2284313589334488\n",
      "Training Model  ...\n",
      "E loss:  0.4315854609012604\n",
      "G loss: 0.21028590202331543\n",
      "E loss:  0.42710596323013306\n",
      "G loss: 0.1820758432149887\n",
      "E loss:  0.43721699714660645\n",
      "G loss: 0.21672767400741577\n",
      "E loss:  0.4389194846153259\n",
      "G loss: 0.21417692303657532\n",
      "E loss:  0.43944454193115234\n",
      "G loss: 0.18905490636825562\n",
      "Training Model  ...\n",
      "E loss:  0.4460512399673462\n",
      "G loss: 0.22575266659259796\n",
      "E loss:  0.41864874958992004\n",
      "G loss: 0.3229510188102722\n",
      "E loss:  0.42277616262435913\n",
      "G loss: 0.39788544178009033\n",
      "E loss:  0.4394277334213257\n",
      "G loss: 0.36393195390701294\n",
      "E loss:  0.413920521736145\n",
      "G loss: 0.28131380677223206\n",
      "Training Model  ...\n",
      "E loss:  0.44531694054603577\n",
      "G loss: 0.25001320242881775\n",
      "E loss:  0.4371698796749115\n",
      "G loss: 0.22413234412670135\n",
      "E loss:  0.4521300792694092\n",
      "G loss: 0.19946256279945374\n",
      "E loss:  0.4554022550582886\n",
      "G loss: 0.25904661417007446\n",
      "E loss:  0.4508521854877472\n",
      "G loss: 0.26893484592437744\n",
      "Training Model  ...\n",
      "E loss:  0.5002445578575134\n",
      "G loss: 0.28791531920433044\n",
      "E loss:  0.4855476915836334\n",
      "G loss: 0.3192833662033081\n",
      "E loss:  0.4799784719944\n",
      "G loss: 0.35971567034721375\n",
      "E loss:  0.4785655736923218\n",
      "G loss: 0.3197677433490753\n",
      "E loss:  0.4728197157382965\n",
      "G loss: 0.2772974669933319\n",
      "Training Model  ...\n",
      "E loss:  0.4530503451824188\n",
      "G loss: 0.28106093406677246\n",
      "E loss:  0.45035964250564575\n",
      "G loss: 0.22724416851997375\n",
      "E loss:  0.4468940496444702\n",
      "G loss: 0.23570024967193604\n",
      "E loss:  0.44365426898002625\n",
      "G loss: 0.21575625240802765\n",
      "E loss:  0.43654561042785645\n",
      "G loss: 0.2241293489933014\n",
      "Training Model  ...\n",
      "E loss:  0.45439180731773376\n",
      "G loss: 0.24415679275989532\n",
      "E loss:  0.44846636056900024\n",
      "G loss: 0.27925431728363037\n",
      "E loss:  0.45311933755874634\n",
      "G loss: 0.27559059858322144\n",
      "E loss:  0.4606083035469055\n",
      "G loss: 0.31250011920928955\n",
      "E loss:  0.45025813579559326\n",
      "G loss: 0.269329696893692\n",
      "Training Model  ...\n",
      "E loss:  0.43065017461776733\n",
      "G loss: 0.25138211250305176\n",
      "E loss:  0.41768813133239746\n",
      "G loss: 0.2237672358751297\n",
      "E loss:  0.41678211092948914\n",
      "G loss: 0.20745114982128143\n",
      "E loss:  0.4218479096889496\n",
      "G loss: 0.23921117186546326\n",
      "E loss:  0.41826891899108887\n",
      "G loss: 0.25798285007476807\n",
      "Training Model  ...\n",
      "E loss:  0.40956932306289673\n",
      "G loss: 0.2591724097728729\n",
      "E loss:  0.41497769951820374\n",
      "G loss: 0.22624540328979492\n",
      "E loss:  0.4139211177825928\n",
      "G loss: 0.24603036046028137\n",
      "E loss:  0.4186527729034424\n",
      "G loss: 0.22051775455474854\n",
      "E loss:  0.4343565106391907\n",
      "G loss: 0.26158303022384644\n",
      "Training Model  ...\n",
      "E loss:  0.4867381751537323\n",
      "G loss: 0.2747347950935364\n",
      "E loss:  0.4797724187374115\n",
      "G loss: 0.23712734878063202\n",
      "E loss:  0.48514559864997864\n",
      "G loss: 0.3008805811405182\n",
      "E loss:  0.4876069128513336\n",
      "G loss: 0.2652784287929535\n",
      "E loss:  0.4866648316383362\n",
      "G loss: 0.24830543994903564\n",
      "Training Model  ...\n",
      "E loss:  0.45384085178375244\n",
      "G loss: 0.23751899600028992\n",
      "E loss:  0.4506343901157379\n",
      "G loss: 0.23125852644443512\n",
      "E loss:  0.4541400969028473\n",
      "G loss: 0.21510829031467438\n",
      "E loss:  0.4517330527305603\n",
      "G loss: 0.23532752692699432\n",
      "E loss:  0.43742403388023376\n",
      "G loss: 0.23497185111045837\n",
      "Training Model  ...\n",
      "E loss:  0.42091071605682373\n",
      "G loss: 0.189550518989563\n",
      "E loss:  0.4106270670890808\n",
      "G loss: 0.18845148384571075\n",
      "E loss:  0.4156390130519867\n",
      "G loss: 0.1833244413137436\n",
      "E loss:  0.40717798471450806\n",
      "G loss: 0.20010504126548767\n",
      "E loss:  0.4098590910434723\n",
      "G loss: 0.19835315644741058\n",
      "Training Model  ...\n",
      "E loss:  0.469995379447937\n",
      "G loss: 0.2123163938522339\n",
      "E loss:  0.4602242410182953\n",
      "G loss: 0.21067367494106293\n",
      "E loss:  0.44703903794288635\n",
      "G loss: 0.25741952657699585\n",
      "E loss:  0.4539141058921814\n",
      "G loss: 0.24481870234012604\n",
      "E loss:  0.4468090534210205\n",
      "G loss: 0.20978102087974548\n",
      "Training Model  ...\n",
      "E loss:  0.4677913188934326\n",
      "G loss: 0.2289353609085083\n",
      "E loss:  0.46451833844184875\n",
      "G loss: 0.1961551308631897\n",
      "E loss:  0.4614071249961853\n",
      "G loss: 0.21854951977729797\n",
      "E loss:  0.46121785044670105\n",
      "G loss: 0.23114287853240967\n",
      "E loss:  0.4607301950454712\n",
      "G loss: 0.19837374985218048\n",
      "Training Model  ...\n",
      "E loss:  0.4409726858139038\n",
      "G loss: 0.21640488505363464\n",
      "E loss:  0.4383850395679474\n",
      "G loss: 0.25450626015663147\n",
      "E loss:  0.4384980797767639\n",
      "G loss: 0.2124776393175125\n",
      "E loss:  0.43809962272644043\n",
      "G loss: 0.23689939081668854\n",
      "E loss:  0.43392568826675415\n",
      "G loss: 0.2323046624660492\n",
      "Training Model  ...\n",
      "E loss:  0.4324699938297272\n",
      "G loss: 0.221368208527565\n",
      "E loss:  0.43219056725502014\n",
      "G loss: 0.21420909464359283\n",
      "E loss:  0.416733056306839\n",
      "G loss: 0.23374387621879578\n",
      "E loss:  0.4125734567642212\n",
      "G loss: 0.24538423120975494\n",
      "E loss:  0.4136674106121063\n",
      "G loss: 0.2230958640575409\n",
      "Training Model  ...\n",
      "E loss:  0.42634260654449463\n",
      "G loss: 0.1965043991804123\n",
      "E loss:  0.42623841762542725\n",
      "G loss: 0.21973495185375214\n",
      "E loss:  0.4208966791629791\n",
      "G loss: 0.21530991792678833\n",
      "E loss:  0.4152936637401581\n",
      "G loss: 0.21521447598934174\n",
      "E loss:  0.4131234288215637\n",
      "G loss: 0.22778940200805664\n",
      "Training Model  ...\n",
      "E loss:  0.4383677840232849\n",
      "G loss: 0.21928024291992188\n",
      "E loss:  0.4301408529281616\n",
      "G loss: 0.27152228355407715\n",
      "E loss:  0.4397622346878052\n",
      "G loss: 0.24467001855373383\n",
      "E loss:  0.43966951966285706\n",
      "G loss: 0.28670093417167664\n",
      "E loss:  0.4390011727809906\n",
      "G loss: 0.2580500543117523\n",
      "Training Model  ...\n",
      "E loss:  0.4875921607017517\n",
      "G loss: 0.27363163232803345\n",
      "E loss:  0.4704445004463196\n",
      "G loss: 0.317241907119751\n",
      "E loss:  0.46920526027679443\n",
      "G loss: 0.3323475122451782\n",
      "E loss:  0.4780692458152771\n",
      "G loss: 0.30462053418159485\n",
      "E loss:  0.48087936639785767\n",
      "G loss: 0.24132482707500458\n",
      "Training Model  ...\n",
      "E loss:  0.4667654037475586\n",
      "G loss: 0.24893420934677124\n",
      "E loss:  0.46022555232048035\n",
      "G loss: 0.23155561089515686\n",
      "E loss:  0.4757470190525055\n",
      "G loss: 0.21663448214530945\n",
      "E loss:  0.47179627418518066\n",
      "G loss: 0.21391311287879944\n",
      "E loss:  0.47778624296188354\n",
      "G loss: 0.2419387698173523\n",
      "Training Model  ...\n",
      "E loss:  0.4001355469226837\n",
      "G loss: 0.2194398045539856\n",
      "E loss:  0.40024131536483765\n",
      "G loss: 0.23623789846897125\n",
      "E loss:  0.4093319773674011\n",
      "G loss: 0.2395496815443039\n",
      "E loss:  0.40334346890449524\n",
      "G loss: 0.20280958712100983\n",
      "E loss:  0.40552055835723877\n",
      "G loss: 0.19069771468639374\n",
      "Training Model  ...\n",
      "E loss:  0.4195285439491272\n",
      "G loss: 0.22248335182666779\n",
      "E loss:  0.4301016926765442\n",
      "G loss: 0.2219950407743454\n",
      "E loss:  0.42028042674064636\n",
      "G loss: 0.20136390626430511\n",
      "E loss:  0.4201241135597229\n",
      "G loss: 0.1891503930091858\n",
      "E loss:  0.4331594705581665\n",
      "G loss: 0.2358793467283249\n",
      "Training Model  ...\n",
      "E loss:  0.42578932642936707\n",
      "G loss: 0.21113091707229614\n",
      "E loss:  0.4300982654094696\n",
      "G loss: 0.23203842341899872\n",
      "E loss:  0.4319332242012024\n",
      "G loss: 0.23756815493106842\n",
      "E loss:  0.4296762943267822\n",
      "G loss: 0.24951088428497314\n",
      "E loss:  0.42835932970046997\n",
      "G loss: 0.23531050980091095\n",
      "Training Model  ...\n",
      "E loss:  0.4155958294868469\n",
      "G loss: 0.22979708015918732\n",
      "E loss:  0.4108777344226837\n",
      "G loss: 0.20189781486988068\n",
      "E loss:  0.4151768982410431\n",
      "G loss: 0.206941157579422\n",
      "E loss:  0.4220646023750305\n",
      "G loss: 0.17036178708076477\n",
      "E loss:  0.40138694643974304\n",
      "G loss: 0.25571608543395996\n",
      "Training Model  ...\n",
      "E loss:  0.4718910753726959\n",
      "G loss: 0.19318050146102905\n",
      "E loss:  0.4487622082233429\n",
      "G loss: 0.21942183375358582\n",
      "E loss:  0.4438008666038513\n",
      "G loss: 0.2164163738489151\n",
      "E loss:  0.449690043926239\n",
      "G loss: 0.20570310950279236\n",
      "E loss:  0.45550212264060974\n",
      "G loss: 0.2109333872795105\n",
      "Training Model  ...\n",
      "E loss:  0.4772534966468811\n",
      "G loss: 0.21891918778419495\n",
      "E loss:  0.46450522541999817\n",
      "G loss: 0.21764636039733887\n",
      "E loss:  0.45773327350616455\n",
      "G loss: 0.21197472512722015\n",
      "E loss:  0.45582589507102966\n",
      "G loss: 0.21970559656620026\n",
      "E loss:  0.4683183431625366\n",
      "G loss: 0.22972890734672546\n",
      "Training Model  ...\n",
      "E loss:  0.4581458568572998\n",
      "G loss: 0.21567201614379883\n",
      "E loss:  0.4509768486022949\n",
      "G loss: 0.2268495410680771\n",
      "E loss:  0.45133131742477417\n",
      "G loss: 0.23653624951839447\n",
      "E loss:  0.4466986656188965\n",
      "G loss: 0.23083403706550598\n",
      "E loss:  0.4459396004676819\n",
      "G loss: 0.23186597228050232\n",
      "Training Model  ...\n",
      "E loss:  0.4115560054779053\n",
      "G loss: 0.23122981190681458\n",
      "E loss:  0.3974640965461731\n",
      "G loss: 0.2203332632780075\n",
      "E loss:  0.39481163024902344\n",
      "G loss: 0.17556221783161163\n",
      "E loss:  0.39266762137413025\n",
      "G loss: 0.2214735746383667\n",
      "E loss:  0.3928462862968445\n",
      "G loss: 0.24842825531959534\n",
      "Training Model  ...\n",
      "E loss:  0.4629839062690735\n",
      "G loss: 0.20780962705612183\n",
      "E loss:  0.4716508388519287\n",
      "G loss: 0.23737363517284393\n",
      "E loss:  0.4738311469554901\n",
      "G loss: 0.21442069113254547\n",
      "E loss:  0.4520687758922577\n",
      "G loss: 0.24069271981716156\n",
      "E loss:  0.45695897936820984\n",
      "G loss: 0.2042650431394577\n",
      "Training Model  ...\n",
      "E loss:  0.39514797925949097\n",
      "G loss: 0.24694932997226715\n",
      "E loss:  0.3913947343826294\n",
      "G loss: 0.18262730538845062\n",
      "E loss:  0.3930397927761078\n",
      "G loss: 0.1555517464876175\n",
      "E loss:  0.39466625452041626\n",
      "G loss: 0.17288164794445038\n",
      "E loss:  0.39458683133125305\n",
      "G loss: 0.19798356294631958\n",
      "Training Model  ...\n",
      "E loss:  0.4062313437461853\n",
      "G loss: 0.20291227102279663\n",
      "E loss:  0.4128865599632263\n",
      "G loss: 0.21753549575805664\n",
      "E loss:  0.4234102964401245\n",
      "G loss: 0.24186928570270538\n",
      "E loss:  0.42547303438186646\n",
      "G loss: 0.2409583032131195\n",
      "E loss:  0.4165554940700531\n",
      "G loss: 0.18630351126194\n",
      "Training Model  ...\n",
      "E loss:  0.4136487543582916\n",
      "G loss: 0.21190690994262695\n",
      "E loss:  0.41693487763404846\n",
      "G loss: 0.22418315708637238\n",
      "E loss:  0.4147762358188629\n",
      "G loss: 0.21910277009010315\n",
      "E loss:  0.4118031859397888\n",
      "G loss: 0.20833180844783783\n",
      "E loss:  0.40959006547927856\n",
      "G loss: 0.21432791650295258\n",
      "Training Model  ...\n",
      "E loss:  0.3980192244052887\n",
      "G loss: 0.2198333442211151\n",
      "E loss:  0.3975274860858917\n",
      "G loss: 0.18891076743602753\n",
      "E loss:  0.401525616645813\n",
      "G loss: 0.21406130492687225\n",
      "E loss:  0.4058988094329834\n",
      "G loss: 0.19868698716163635\n",
      "E loss:  0.3842220902442932\n",
      "G loss: 0.21228347718715668\n",
      "Training Model  ...\n",
      "E loss:  0.45038890838623047\n",
      "G loss: 0.19853723049163818\n",
      "E loss:  0.4546310305595398\n",
      "G loss: 0.21555480360984802\n",
      "E loss:  0.4595690071582794\n",
      "G loss: 0.2124681919813156\n",
      "E loss:  0.45535656809806824\n",
      "G loss: 0.25207608938217163\n",
      "E loss:  0.4531865417957306\n",
      "G loss: 0.2213079333305359\n",
      "Training Model  ...\n",
      "E loss:  0.42581263184547424\n",
      "G loss: 0.22844287753105164\n",
      "E loss:  0.4249840974807739\n",
      "G loss: 0.21894234418869019\n",
      "E loss:  0.41902947425842285\n",
      "G loss: 0.21442726254463196\n",
      "E loss:  0.4109411835670471\n",
      "G loss: 0.19405482709407806\n",
      "E loss:  0.4050160050392151\n",
      "G loss: 0.2235279381275177\n",
      "Training Model  ...\n",
      "E loss:  0.42825978994369507\n",
      "G loss: 0.2202668935060501\n",
      "E loss:  0.4354921281337738\n",
      "G loss: 0.25099822878837585\n",
      "E loss:  0.44129323959350586\n",
      "G loss: 0.23444531857967377\n",
      "E loss:  0.44251036643981934\n",
      "G loss: 0.23347905278205872\n",
      "E loss:  0.4521884024143219\n",
      "G loss: 0.23521827161312103\n",
      "Training Model  ...\n",
      "E loss:  0.4537394344806671\n",
      "G loss: 0.2436041682958603\n",
      "E loss:  0.4576982259750366\n",
      "G loss: 0.2540023922920227\n",
      "E loss:  0.4701627194881439\n",
      "G loss: 0.2667936086654663\n",
      "E loss:  0.4719294607639313\n",
      "G loss: 0.30155593156814575\n",
      "E loss:  0.4687410295009613\n",
      "G loss: 0.25251448154449463\n",
      "Training Model  ...\n",
      "E loss:  0.4835001230239868\n",
      "G loss: 0.22093260288238525\n",
      "E loss:  0.4868861436843872\n",
      "G loss: 0.2118404507637024\n",
      "E loss:  0.49459531903266907\n",
      "G loss: 0.23884662985801697\n",
      "E loss:  0.5049043297767639\n",
      "G loss: 0.2106848806142807\n",
      "E loss:  0.504248857498169\n",
      "G loss: 0.1902698278427124\n",
      "Training Model  ...\n",
      "E loss:  0.43956589698791504\n",
      "G loss: 0.24302023649215698\n",
      "E loss:  0.4342825412750244\n",
      "G loss: 0.2782639265060425\n",
      "E loss:  0.4321460425853729\n",
      "G loss: 0.2542914152145386\n",
      "E loss:  0.4306755065917969\n",
      "G loss: 0.27982497215270996\n",
      "E loss:  0.4309736490249634\n",
      "G loss: 0.2474454790353775\n",
      "Training Model  ...\n",
      "E loss:  0.4076988101005554\n",
      "G loss: 0.24379342794418335\n",
      "E loss:  0.4077390432357788\n",
      "G loss: 0.22206105291843414\n",
      "E loss:  0.41058623790740967\n",
      "G loss: 0.25853052735328674\n",
      "E loss:  0.41047754883766174\n",
      "G loss: 0.272247314453125\n",
      "E loss:  0.40513086318969727\n",
      "G loss: 0.23592300713062286\n",
      "Training Model  ...\n",
      "E loss:  0.432197630405426\n",
      "G loss: 0.2278081625699997\n",
      "E loss:  0.4294920563697815\n",
      "G loss: 0.2347170114517212\n",
      "E loss:  0.4361603260040283\n",
      "G loss: 0.25039392709732056\n",
      "E loss:  0.44565171003341675\n",
      "G loss: 0.2546968162059784\n",
      "E loss:  0.45236507058143616\n",
      "G loss: 0.23876561224460602\n",
      "Training Model  ...\n",
      "E loss:  0.46972447633743286\n",
      "G loss: 0.25746864080429077\n",
      "E loss:  0.4621097147464752\n",
      "G loss: 0.24957837164402008\n",
      "E loss:  0.4572921693325043\n",
      "G loss: 0.2749483287334442\n",
      "E loss:  0.4544817805290222\n",
      "G loss: 0.2288026362657547\n",
      "E loss:  0.44920963048934937\n",
      "G loss: 0.2584502398967743\n",
      "Training Model  ...\n",
      "E loss:  0.46251386404037476\n",
      "G loss: 0.2598910927772522\n",
      "E loss:  0.45867228507995605\n",
      "G loss: 0.2392461597919464\n",
      "E loss:  0.45614445209503174\n",
      "G loss: 0.2540733218193054\n",
      "E loss:  0.4377322494983673\n",
      "G loss: 0.2804964482784271\n",
      "E loss:  0.4327271580696106\n",
      "G loss: 0.22801727056503296\n",
      "Training Model  ...\n",
      "E loss:  0.48789459466934204\n",
      "G loss: 0.2187604010105133\n",
      "E loss:  0.4961834251880646\n",
      "G loss: 0.21883219480514526\n",
      "E loss:  0.4926050901412964\n",
      "G loss: 0.20774774253368378\n",
      "E loss:  0.4959881901741028\n",
      "G loss: 0.18252450227737427\n",
      "E loss:  0.4960833191871643\n",
      "G loss: 0.2387382835149765\n",
      "Training Model  ...\n",
      "E loss:  0.4727889895439148\n",
      "G loss: 0.22489723563194275\n",
      "E loss:  0.4729239344596863\n",
      "G loss: 0.22763285040855408\n",
      "E loss:  0.4733748137950897\n",
      "G loss: 0.20646154880523682\n",
      "E loss:  0.4641689658164978\n",
      "G loss: 0.2223842293024063\n",
      "E loss:  0.47586938738822937\n",
      "G loss: 0.23757515847682953\n",
      "Training Model  ...\n",
      "E loss:  0.4579318165779114\n",
      "G loss: 0.22831006348133087\n",
      "E loss:  0.4517868459224701\n",
      "G loss: 0.24382241070270538\n",
      "E loss:  0.4522685110569\n",
      "G loss: 0.25085756182670593\n",
      "E loss:  0.45337796211242676\n",
      "G loss: 0.22778932750225067\n",
      "E loss:  0.44504207372665405\n",
      "G loss: 0.24648302793502808\n",
      "Training Model  ...\n",
      "E loss:  0.4413537383079529\n",
      "G loss: 0.2621220648288727\n",
      "E loss:  0.4368666112422943\n",
      "G loss: 0.194345623254776\n",
      "E loss:  0.44157055020332336\n",
      "G loss: 0.2391757369041443\n",
      "E loss:  0.435762882232666\n",
      "G loss: 0.19554449617862701\n",
      "E loss:  0.4288603663444519\n",
      "G loss: 0.22045284509658813\n",
      "Training Model  ...\n",
      "E loss:  0.4212118685245514\n",
      "G loss: 0.2552252411842346\n",
      "E loss:  0.42369669675827026\n",
      "G loss: 0.20419679582118988\n",
      "E loss:  0.41361963748931885\n",
      "G loss: 0.20955872535705566\n",
      "E loss:  0.40108364820480347\n",
      "G loss: 0.22083839774131775\n",
      "E loss:  0.39796704053878784\n",
      "G loss: 0.21208901703357697\n",
      "Training Model  ...\n",
      "E loss:  0.4442675709724426\n",
      "G loss: 0.2338322252035141\n",
      "E loss:  0.44635865092277527\n",
      "G loss: 0.22179752588272095\n",
      "E loss:  0.4422787129878998\n",
      "G loss: 0.2055044025182724\n",
      "E loss:  0.43594151735305786\n",
      "G loss: 0.2395462691783905\n",
      "E loss:  0.4366139769554138\n",
      "G loss: 0.2253318578004837\n",
      "Training Model  ...\n",
      "E loss:  0.44952306151390076\n",
      "G loss: 0.23347556591033936\n",
      "E loss:  0.44338202476501465\n",
      "G loss: 0.25512179732322693\n",
      "E loss:  0.4469541609287262\n",
      "G loss: 0.21467889845371246\n",
      "E loss:  0.45724183320999146\n",
      "G loss: 0.23680973052978516\n",
      "E loss:  0.44882097840309143\n",
      "G loss: 0.23061200976371765\n",
      "Training Model  ...\n",
      "E loss:  0.4170147776603699\n",
      "G loss: 0.24181613326072693\n",
      "E loss:  0.418774276971817\n",
      "G loss: 0.24206805229187012\n",
      "E loss:  0.4079829454421997\n",
      "G loss: 0.24138113856315613\n",
      "E loss:  0.40344589948654175\n",
      "G loss: 0.22685900330543518\n",
      "E loss:  0.4015129804611206\n",
      "G loss: 0.21565254032611847\n",
      "Training Model  ...\n",
      "E loss:  0.4094562828540802\n",
      "G loss: 0.20913927257061005\n",
      "E loss:  0.41429653763771057\n",
      "G loss: 0.1931728571653366\n",
      "E loss:  0.42199182510375977\n",
      "G loss: 0.22587043046951294\n",
      "E loss:  0.42552420496940613\n",
      "G loss: 0.1877063512802124\n",
      "E loss:  0.43776679039001465\n",
      "G loss: 0.20710481703281403\n",
      "Training Model  ...\n",
      "E loss:  0.4415491819381714\n",
      "G loss: 0.2361980676651001\n",
      "E loss:  0.43567919731140137\n",
      "G loss: 0.20114372670650482\n",
      "E loss:  0.4374570846557617\n",
      "G loss: 0.20841258764266968\n",
      "E loss:  0.42925184965133667\n",
      "G loss: 0.2634047865867615\n",
      "E loss:  0.43366265296936035\n",
      "G loss: 0.1897079199552536\n",
      "Training Model  ...\n",
      "E loss:  0.3712439239025116\n",
      "G loss: 0.23468583822250366\n",
      "E loss:  0.36963677406311035\n",
      "G loss: 0.19708581268787384\n",
      "E loss:  0.3744138479232788\n",
      "G loss: 0.20192578434944153\n",
      "E loss:  0.3726693391799927\n",
      "G loss: 0.2309715449810028\n",
      "E loss:  0.3681490421295166\n",
      "G loss: 0.27129149436950684\n",
      "Training Model  ...\n",
      "E loss:  0.39803051948547363\n",
      "G loss: 0.2259587198495865\n",
      "E loss:  0.3980093002319336\n",
      "G loss: 0.2127416431903839\n",
      "E loss:  0.40158161520957947\n",
      "G loss: 0.2228887379169464\n",
      "E loss:  0.40041497349739075\n",
      "G loss: 0.18731264770030975\n",
      "E loss:  0.39912617206573486\n",
      "G loss: 0.22634190320968628\n",
      "Training Model  ...\n",
      "E loss:  0.4078342318534851\n",
      "G loss: 0.2463851273059845\n",
      "E loss:  0.3994967043399811\n",
      "G loss: 0.2723086476325989\n",
      "E loss:  0.40505677461624146\n",
      "G loss: 0.2910691499710083\n",
      "E loss:  0.41212254762649536\n",
      "G loss: 0.25053271651268005\n",
      "E loss:  0.40404456853866577\n",
      "G loss: 0.22119788825511932\n",
      "Training Model  ...\n",
      "E loss:  0.4141926169395447\n",
      "G loss: 0.2337701916694641\n",
      "E loss:  0.41316625475883484\n",
      "G loss: 0.2096322476863861\n",
      "E loss:  0.42582613229751587\n",
      "G loss: 0.2456018328666687\n",
      "E loss:  0.42723023891448975\n",
      "G loss: 0.22366780042648315\n",
      "E loss:  0.4244120717048645\n",
      "G loss: 0.23109902441501617\n",
      "Training Model  ...\n",
      "E loss:  0.41978010535240173\n",
      "G loss: 0.23421727120876312\n",
      "E loss:  0.4247056841850281\n",
      "G loss: 0.24380095303058624\n",
      "E loss:  0.42025718092918396\n",
      "G loss: 0.21390798687934875\n",
      "E loss:  0.4124090373516083\n",
      "G loss: 0.20182424783706665\n",
      "E loss:  0.411210834980011\n",
      "G loss: 0.2414114624261856\n",
      "Training Model  ...\n",
      "E loss:  0.44240862131118774\n",
      "G loss: 0.23677971959114075\n",
      "E loss:  0.44238540530204773\n",
      "G loss: 0.24398936331272125\n",
      "E loss:  0.4371471107006073\n",
      "G loss: 0.23032280802726746\n",
      "E loss:  0.42580822110176086\n",
      "G loss: 0.23338717222213745\n",
      "E loss:  0.4173150658607483\n",
      "G loss: 0.20673353970050812\n",
      "Training Model  ...\n",
      "E loss:  0.4602009952068329\n",
      "G loss: 0.25837627053260803\n",
      "E loss:  0.4502762258052826\n",
      "G loss: 0.268026202917099\n",
      "E loss:  0.46140724420547485\n",
      "G loss: 0.26302242279052734\n",
      "E loss:  0.4554029107093811\n",
      "G loss: 0.2608753442764282\n",
      "E loss:  0.4546636939048767\n",
      "G loss: 0.2560984492301941\n",
      "Training Model  ...\n",
      "E loss:  0.4366397261619568\n",
      "G loss: 0.20824050903320312\n",
      "E loss:  0.4378568232059479\n",
      "G loss: 0.21039645373821259\n",
      "E loss:  0.41766002774238586\n",
      "G loss: 0.1896933913230896\n",
      "E loss:  0.42339155077934265\n",
      "G loss: 0.2133522927761078\n",
      "E loss:  0.42397499084472656\n",
      "G loss: 0.21314743161201477\n",
      "Training Model  ...\n",
      "E loss:  0.4299232065677643\n",
      "G loss: 0.21891888976097107\n",
      "E loss:  0.418759822845459\n",
      "G loss: 0.24601732194423676\n",
      "E loss:  0.42295247316360474\n",
      "G loss: 0.22774359583854675\n",
      "E loss:  0.42352110147476196\n",
      "G loss: 0.25717538595199585\n",
      "E loss:  0.41931524872779846\n",
      "G loss: 0.23708227276802063\n",
      "Training Model  ...\n",
      "E loss:  0.3967086672782898\n",
      "G loss: 0.19764694571495056\n",
      "E loss:  0.40834757685661316\n",
      "G loss: 0.2281942069530487\n",
      "E loss:  0.4063597619533539\n",
      "G loss: 0.2275073230266571\n",
      "E loss:  0.4124351739883423\n",
      "G loss: 0.21087487041950226\n",
      "E loss:  0.408143013715744\n",
      "G loss: 0.22707293927669525\n",
      "Training Model  ...\n",
      "E loss:  0.40130436420440674\n",
      "G loss: 0.22663073241710663\n",
      "E loss:  0.4013512432575226\n",
      "G loss: 0.2084590643644333\n",
      "E loss:  0.4073958098888397\n",
      "G loss: 0.20956970751285553\n",
      "E loss:  0.40159085392951965\n",
      "G loss: 0.18856306374073029\n",
      "E loss:  0.3993213176727295\n",
      "G loss: 0.2393251657485962\n",
      "Training Model  ...\n",
      "E loss:  0.4171339273452759\n",
      "G loss: 0.26356446743011475\n",
      "E loss:  0.4061844050884247\n",
      "G loss: 0.2690593898296356\n",
      "E loss:  0.4137576222419739\n",
      "G loss: 0.24525566399097443\n",
      "E loss:  0.4013104736804962\n",
      "G loss: 0.22179515659809113\n",
      "E loss:  0.4076690673828125\n",
      "G loss: 0.20018132030963898\n",
      "Training Model  ...\n",
      "E loss:  0.4396551549434662\n",
      "G loss: 0.2567981481552124\n",
      "E loss:  0.4429258406162262\n",
      "G loss: 0.26090696454048157\n",
      "E loss:  0.4530879259109497\n",
      "G loss: 0.30345192551612854\n",
      "E loss:  0.45660147070884705\n",
      "G loss: 0.2663518786430359\n",
      "E loss:  0.4547024965286255\n",
      "G loss: 0.2561115324497223\n",
      "Training Model  ...\n",
      "E loss:  0.41742998361587524\n",
      "G loss: 0.2567216157913208\n",
      "E loss:  0.41133713722229004\n",
      "G loss: 0.2616669833660126\n",
      "E loss:  0.412641704082489\n",
      "G loss: 0.2466575801372528\n",
      "E loss:  0.4073527455329895\n",
      "G loss: 0.23864272236824036\n",
      "E loss:  0.4167309105396271\n",
      "G loss: 0.22223564982414246\n",
      "Training Model  ...\n",
      "E loss:  0.45724406838417053\n",
      "G loss: 0.2427864670753479\n",
      "E loss:  0.4652257561683655\n",
      "G loss: 0.2521979510784149\n",
      "E loss:  0.461639404296875\n",
      "G loss: 0.22117048501968384\n",
      "E loss:  0.45524629950523376\n",
      "G loss: 0.2615780234336853\n",
      "E loss:  0.45400914549827576\n",
      "G loss: 0.24924764037132263\n",
      "Training Model  ...\n",
      "E loss:  0.3969739079475403\n",
      "G loss: 0.2590198516845703\n",
      "E loss:  0.3927375376224518\n",
      "G loss: 0.2683873474597931\n",
      "E loss:  0.3963537812232971\n",
      "G loss: 0.2772614061832428\n",
      "E loss:  0.4060683250427246\n",
      "G loss: 0.24471069872379303\n",
      "E loss:  0.4260823130607605\n",
      "G loss: 0.24776813387870789\n",
      "Training Model  ...\n",
      "E loss:  0.3948490619659424\n",
      "G loss: 0.2406500279903412\n",
      "E loss:  0.3930272161960602\n",
      "G loss: 0.22510750591754913\n",
      "E loss:  0.3942200243473053\n",
      "G loss: 0.22551944851875305\n",
      "E loss:  0.39441877603530884\n",
      "G loss: 0.20623834431171417\n",
      "E loss:  0.40578752756118774\n",
      "G loss: 0.24765455722808838\n",
      "Training Model  ...\n",
      "E loss:  0.4401470124721527\n",
      "G loss: 0.23773285746574402\n",
      "E loss:  0.4394906461238861\n",
      "G loss: 0.21987321972846985\n",
      "E loss:  0.4374074935913086\n",
      "G loss: 0.2508707046508789\n",
      "E loss:  0.44978371262550354\n",
      "G loss: 0.2182740718126297\n",
      "E loss:  0.4386644661426544\n",
      "G loss: 0.2211005687713623\n",
      "Training Model  ...\n",
      "E loss:  0.44254422187805176\n",
      "G loss: 0.2305888682603836\n",
      "E loss:  0.44115495681762695\n",
      "G loss: 0.2521260976791382\n",
      "E loss:  0.43624332547187805\n",
      "G loss: 0.2401389479637146\n",
      "E loss:  0.43800267577171326\n",
      "G loss: 0.2570101320743561\n",
      "E loss:  0.4259048104286194\n",
      "G loss: 0.21220709383487701\n",
      "Training Model  ...\n",
      "E loss:  0.43446582555770874\n",
      "G loss: 0.2400510609149933\n",
      "E loss:  0.42482292652130127\n",
      "G loss: 0.2719302475452423\n",
      "E loss:  0.43242764472961426\n",
      "G loss: 0.2608102858066559\n",
      "E loss:  0.43338924646377563\n",
      "G loss: 0.2648262679576874\n",
      "E loss:  0.4363420307636261\n",
      "G loss: 0.24425965547561646\n",
      "Training Model  ...\n",
      "E loss:  0.40803346037864685\n",
      "G loss: 0.21556004881858826\n",
      "E loss:  0.39708587527275085\n",
      "G loss: 0.17783597111701965\n",
      "E loss:  0.3940579295158386\n",
      "G loss: 0.18365439772605896\n",
      "E loss:  0.39924106001853943\n",
      "G loss: 0.20826515555381775\n",
      "E loss:  0.39447444677352905\n",
      "G loss: 0.22048626840114594\n",
      "Training Model  ...\n",
      "E loss:  0.4233558475971222\n",
      "G loss: 0.228750079870224\n",
      "E loss:  0.42266136407852173\n",
      "G loss: 0.2791033983230591\n",
      "E loss:  0.4236729145050049\n",
      "G loss: 0.27745187282562256\n",
      "E loss:  0.42433181405067444\n",
      "G loss: 0.23823830485343933\n",
      "E loss:  0.4268829822540283\n",
      "G loss: 0.21769653260707855\n",
      "Training Model  ...\n",
      "E loss:  0.4737043082714081\n",
      "G loss: 0.22119444608688354\n",
      "E loss:  0.48341405391693115\n",
      "G loss: 0.255086749792099\n",
      "E loss:  0.48249417543411255\n",
      "G loss: 0.22918516397476196\n",
      "E loss:  0.4820061922073364\n",
      "G loss: 0.2491631656885147\n",
      "E loss:  0.4803162217140198\n",
      "G loss: 0.20008890330791473\n",
      "Training Model  ...\n",
      "E loss:  0.42846617102622986\n",
      "G loss: 0.2294330596923828\n",
      "E loss:  0.4263904392719269\n",
      "G loss: 0.21042442321777344\n",
      "E loss:  0.43071556091308594\n",
      "G loss: 0.20279642939567566\n",
      "E loss:  0.43465107679367065\n",
      "G loss: 0.20929840207099915\n",
      "E loss:  0.43763554096221924\n",
      "G loss: 0.2412981390953064\n",
      "Training Model  ...\n",
      "E loss:  0.4362821578979492\n",
      "G loss: 0.24792319536209106\n",
      "E loss:  0.43786630034446716\n",
      "G loss: 0.23806728422641754\n",
      "E loss:  0.4321116805076599\n",
      "G loss: 0.21274611353874207\n",
      "E loss:  0.4152383804321289\n",
      "G loss: 0.2647574245929718\n",
      "E loss:  0.42358726263046265\n",
      "G loss: 0.23167453706264496\n",
      "Training Model  ...\n",
      "E loss:  0.4520360231399536\n",
      "G loss: 0.2436344027519226\n",
      "E loss:  0.44379404187202454\n",
      "G loss: 0.2773016691207886\n",
      "E loss:  0.441360741853714\n",
      "G loss: 0.3002130687236786\n",
      "E loss:  0.4418935179710388\n",
      "G loss: 0.26639944314956665\n",
      "E loss:  0.4310304522514343\n",
      "G loss: 0.22464334964752197\n",
      "Training Model  ...\n",
      "E loss:  0.4294087588787079\n",
      "G loss: 0.2488221526145935\n",
      "E loss:  0.42932969331741333\n",
      "G loss: 0.21847738325595856\n",
      "E loss:  0.42526137828826904\n",
      "G loss: 0.20856796205043793\n",
      "E loss:  0.4242594540119171\n",
      "G loss: 0.2426718771457672\n",
      "E loss:  0.4257534444332123\n",
      "G loss: 0.22163893282413483\n",
      "Training Model  ...\n",
      "E loss:  0.4566778838634491\n",
      "G loss: 0.2596472501754761\n",
      "E loss:  0.46092984080314636\n",
      "G loss: 0.2363637387752533\n",
      "E loss:  0.4548490345478058\n",
      "G loss: 0.24038590490818024\n",
      "E loss:  0.4565201699733734\n",
      "G loss: 0.2568344473838806\n",
      "E loss:  0.44977477192878723\n",
      "G loss: 0.23917099833488464\n",
      "Training Model  ...\n",
      "E loss:  0.4185138940811157\n",
      "G loss: 0.23970767855644226\n",
      "E loss:  0.41134169697761536\n",
      "G loss: 0.24394941329956055\n",
      "E loss:  0.4121740460395813\n",
      "G loss: 0.23187407851219177\n",
      "E loss:  0.41748201847076416\n",
      "G loss: 0.23369699716567993\n",
      "E loss:  0.41776779294013977\n",
      "G loss: 0.2385159581899643\n",
      "Training Model  ...\n",
      "E loss:  0.42543941736221313\n",
      "G loss: 0.23822979629039764\n",
      "E loss:  0.4232815206050873\n",
      "G loss: 0.2535336911678314\n",
      "E loss:  0.4226035475730896\n",
      "G loss: 0.24650314450263977\n",
      "E loss:  0.41969043016433716\n",
      "G loss: 0.254192054271698\n",
      "E loss:  0.4221190810203552\n",
      "G loss: 0.2704900801181793\n",
      "Training Model  ...\n",
      "E loss:  0.436879962682724\n",
      "G loss: 0.2751394212245941\n",
      "E loss:  0.4124453067779541\n",
      "G loss: 0.1889176368713379\n",
      "E loss:  0.414587140083313\n",
      "G loss: 0.19681906700134277\n",
      "E loss:  0.4305627942085266\n",
      "G loss: 0.1996978372335434\n",
      "E loss:  0.42257705330848694\n",
      "G loss: 0.23737165331840515\n",
      "Training Model  ...\n",
      "E loss:  0.42353764176368713\n",
      "G loss: 0.22637362778186798\n",
      "E loss:  0.4265609383583069\n",
      "G loss: 0.24476665258407593\n",
      "E loss:  0.42574942111968994\n",
      "G loss: 0.21448713541030884\n",
      "E loss:  0.4182858169078827\n",
      "G loss: 0.25027596950531006\n",
      "E loss:  0.4267793595790863\n",
      "G loss: 0.24112004041671753\n",
      "Training Model  ...\n",
      "E loss:  0.40370050072669983\n",
      "G loss: 0.25982797145843506\n",
      "E loss:  0.4058659076690674\n",
      "G loss: 0.21092650294303894\n",
      "E loss:  0.40395721793174744\n",
      "G loss: 0.21740323305130005\n",
      "E loss:  0.40366825461387634\n",
      "G loss: 0.2222997546195984\n",
      "E loss:  0.40994876623153687\n",
      "G loss: 0.24710321426391602\n",
      "Training Model  ...\n",
      "E loss:  0.42576277256011963\n",
      "G loss: 0.2398187518119812\n",
      "E loss:  0.41166922450065613\n",
      "G loss: 0.19615140557289124\n",
      "E loss:  0.3866683840751648\n",
      "G loss: 0.20347994565963745\n",
      "E loss:  0.3780125677585602\n",
      "G loss: 0.21711504459381104\n",
      "E loss:  0.37617406249046326\n",
      "G loss: 0.22100256383419037\n",
      "Training Model  ...\n",
      "E loss:  0.4168296754360199\n",
      "G loss: 0.24660040438175201\n",
      "E loss:  0.41816723346710205\n",
      "G loss: 0.22851288318634033\n",
      "E loss:  0.4159111976623535\n",
      "G loss: 0.22331462800502777\n",
      "E loss:  0.4211559593677521\n",
      "G loss: 0.2410569041967392\n",
      "E loss:  0.4112926721572876\n",
      "G loss: 0.20433789491653442\n",
      "Training Model  ...\n",
      "E loss:  0.4051739573478699\n",
      "G loss: 0.21961230039596558\n",
      "E loss:  0.40026575326919556\n",
      "G loss: 0.21831448376178741\n",
      "E loss:  0.4017661213874817\n",
      "G loss: 0.2484501600265503\n",
      "E loss:  0.4058498740196228\n",
      "G loss: 0.2169291228055954\n",
      "E loss:  0.4006143808364868\n",
      "G loss: 0.2180972695350647\n",
      "Training Model  ...\n",
      "E loss:  0.39956235885620117\n",
      "G loss: 0.2414122223854065\n",
      "E loss:  0.4012143313884735\n",
      "G loss: 0.20495647192001343\n",
      "E loss:  0.4027949273586273\n",
      "G loss: 0.24487917125225067\n",
      "E loss:  0.3957040011882782\n",
      "G loss: 0.22664882242679596\n",
      "E loss:  0.39991629123687744\n",
      "G loss: 0.2123672515153885\n",
      "Training Model  ...\n",
      "E loss:  0.42499840259552\n",
      "G loss: 0.21806354820728302\n",
      "E loss:  0.42597854137420654\n",
      "G loss: 0.18770836293697357\n",
      "E loss:  0.42887765169143677\n",
      "G loss: 0.21366485953330994\n",
      "E loss:  0.42676764726638794\n",
      "G loss: 0.22246474027633667\n",
      "E loss:  0.4273563325405121\n",
      "G loss: 0.21444430947303772\n",
      "Training Model  ...\n",
      "E loss:  0.3876827657222748\n",
      "G loss: 0.2046641707420349\n",
      "E loss:  0.3921945095062256\n",
      "G loss: 0.22659388184547424\n",
      "E loss:  0.39310696721076965\n",
      "G loss: 0.20208469033241272\n",
      "E loss:  0.3909006118774414\n",
      "G loss: 0.20212459564208984\n",
      "E loss:  0.3914888799190521\n",
      "G loss: 0.20673125982284546\n",
      "Training Model  ...\n",
      "E loss:  0.4042327404022217\n",
      "G loss: 0.17780166864395142\n",
      "E loss:  0.39718472957611084\n",
      "G loss: 0.20095756649971008\n",
      "E loss:  0.3857511579990387\n",
      "G loss: 0.20397649705410004\n",
      "E loss:  0.3856356739997864\n",
      "G loss: 0.1766623556613922\n",
      "E loss:  0.39088132977485657\n",
      "G loss: 0.18752652406692505\n",
      "Training Model  ...\n",
      "E loss:  0.46004071831703186\n",
      "G loss: 0.21548715233802795\n",
      "E loss:  0.46076124906539917\n",
      "G loss: 0.21551522612571716\n",
      "E loss:  0.45578792691230774\n",
      "G loss: 0.20828655362129211\n",
      "E loss:  0.4532320499420166\n",
      "G loss: 0.22321778535842896\n",
      "E loss:  0.4639437794685364\n",
      "G loss: 0.21044586598873138\n",
      "Training Model  ...\n",
      "E loss:  0.4169350862503052\n",
      "G loss: 0.23661784827709198\n",
      "E loss:  0.4145488739013672\n",
      "G loss: 0.2223157286643982\n",
      "E loss:  0.40977051854133606\n",
      "G loss: 0.25221455097198486\n",
      "E loss:  0.39663416147232056\n",
      "G loss: 0.2669697701931\n",
      "E loss:  0.38874122500419617\n",
      "G loss: 0.20398768782615662\n",
      "Training Model  ...\n",
      "E loss:  0.38713234663009644\n",
      "G loss: 0.26906391978263855\n",
      "E loss:  0.38972359895706177\n",
      "G loss: 0.21998858451843262\n",
      "E loss:  0.40096545219421387\n",
      "G loss: 0.28027987480163574\n",
      "E loss:  0.41099536418914795\n",
      "G loss: 0.23648890852928162\n",
      "E loss:  0.41446128487586975\n",
      "G loss: 0.22668519616127014\n",
      "Training Model  ...\n",
      "E loss:  0.3999859690666199\n",
      "G loss: 0.2678649127483368\n",
      "E loss:  0.392010360956192\n",
      "G loss: 0.27944642305374146\n",
      "E loss:  0.38519778847694397\n",
      "G loss: 0.2963785231113434\n",
      "E loss:  0.3898983597755432\n",
      "G loss: 0.31752443313598633\n",
      "E loss:  0.3850966691970825\n",
      "G loss: 0.2517043948173523\n",
      "Training Model  ...\n",
      "E loss:  0.41752392053604126\n",
      "G loss: 0.24337553977966309\n",
      "E loss:  0.41104939579963684\n",
      "G loss: 0.22456835210323334\n",
      "E loss:  0.41053035855293274\n",
      "G loss: 0.21221917867660522\n",
      "E loss:  0.4035944938659668\n",
      "G loss: 0.21812400221824646\n",
      "E loss:  0.39918452501296997\n",
      "G loss: 0.22362709045410156\n",
      "Training Model  ...\n",
      "E loss:  0.3655959963798523\n",
      "G loss: 0.21906143426895142\n",
      "E loss:  0.368620365858078\n",
      "G loss: 0.23543553054332733\n",
      "E loss:  0.36350902915000916\n",
      "G loss: 0.25435999035835266\n",
      "E loss:  0.3610796332359314\n",
      "G loss: 0.19713027775287628\n",
      "E loss:  0.3590726852416992\n",
      "G loss: 0.22656452655792236\n",
      "Training Model  ...\n",
      "E loss:  0.41070231795310974\n",
      "G loss: 0.23532003164291382\n",
      "E loss:  0.40972813963890076\n",
      "G loss: 0.2307739406824112\n",
      "E loss:  0.4050385653972626\n",
      "G loss: 0.2489965260028839\n",
      "E loss:  0.40792709589004517\n",
      "G loss: 0.2817484736442566\n",
      "E loss:  0.4137786328792572\n",
      "G loss: 0.2335381656885147\n",
      "Training Model  ...\n",
      "E loss:  0.4418177604675293\n",
      "G loss: 0.23075030744075775\n",
      "E loss:  0.4425010085105896\n",
      "G loss: 0.27996566891670227\n",
      "E loss:  0.4333651661872864\n",
      "G loss: 0.2683894634246826\n",
      "E loss:  0.43708497285842896\n",
      "G loss: 0.26787060499191284\n",
      "E loss:  0.4367733597755432\n",
      "G loss: 0.24137333035469055\n",
      "Training Model  ...\n",
      "E loss:  0.4279553294181824\n",
      "G loss: 0.21648041903972626\n",
      "E loss:  0.42227280139923096\n",
      "G loss: 0.20976261794567108\n",
      "E loss:  0.4349967837333679\n",
      "G loss: 0.20988893508911133\n",
      "E loss:  0.43429800868034363\n",
      "G loss: 0.2194729596376419\n",
      "E loss:  0.4346683919429779\n",
      "G loss: 0.22304773330688477\n",
      "Training Model  ...\n",
      "E loss:  0.4052518606185913\n",
      "G loss: 0.219550222158432\n",
      "E loss:  0.4080483019351959\n",
      "G loss: 0.26054847240448\n",
      "E loss:  0.40456870198249817\n",
      "G loss: 0.24665996432304382\n",
      "E loss:  0.40726330876350403\n",
      "G loss: 0.28439566493034363\n",
      "E loss:  0.42240220308303833\n",
      "G loss: 0.26514893770217896\n",
      "Training Model  ...\n",
      "E loss:  0.42366576194763184\n",
      "G loss: 0.2376949042081833\n",
      "E loss:  0.4206353425979614\n",
      "G loss: 0.22796069085597992\n",
      "E loss:  0.4204489290714264\n",
      "G loss: 0.2437838912010193\n",
      "E loss:  0.4241127371788025\n",
      "G loss: 0.23946860432624817\n",
      "E loss:  0.41380074620246887\n",
      "G loss: 0.24152542650699615\n",
      "Training Model  ...\n",
      "E loss:  0.4069226384162903\n",
      "G loss: 0.19348406791687012\n",
      "E loss:  0.41061416268348694\n",
      "G loss: 0.21127934753894806\n",
      "E loss:  0.4088996648788452\n",
      "G loss: 0.19428786635398865\n",
      "E loss:  0.4141864478588104\n",
      "G loss: 0.2035456895828247\n",
      "E loss:  0.4100903868675232\n",
      "G loss: 0.21086619794368744\n",
      "Training Model  ...\n",
      "E loss:  0.40810880064964294\n",
      "G loss: 0.2728375494480133\n",
      "E loss:  0.4040865898132324\n",
      "G loss: 0.29432740807533264\n",
      "E loss:  0.402946412563324\n",
      "G loss: 0.2558727562427521\n",
      "E loss:  0.40414959192276\n",
      "G loss: 0.2722676992416382\n",
      "E loss:  0.3956112265586853\n",
      "G loss: 0.25968945026397705\n",
      "Training Model  ...\n",
      "E loss:  0.4146517217159271\n",
      "G loss: 0.20647498965263367\n",
      "E loss:  0.4139941334724426\n",
      "G loss: 0.21287879347801208\n",
      "E loss:  0.4193302094936371\n",
      "G loss: 0.24326182901859283\n",
      "E loss:  0.4183034300804138\n",
      "G loss: 0.22522807121276855\n",
      "E loss:  0.4136868715286255\n",
      "G loss: 0.22590282559394836\n",
      "Training Model  ...\n",
      "E loss:  0.3999346196651459\n",
      "G loss: 0.24540629982948303\n",
      "E loss:  0.39863306283950806\n",
      "G loss: 0.23456135392189026\n",
      "E loss:  0.4060206115245819\n",
      "G loss: 0.2657961845397949\n",
      "E loss:  0.41034242510795593\n",
      "G loss: 0.24217678606510162\n",
      "E loss:  0.40716302394866943\n",
      "G loss: 0.22387269139289856\n",
      "Training Model  ...\n",
      "E loss:  0.4136641025543213\n",
      "G loss: 0.21278634667396545\n",
      "E loss:  0.4099927544593811\n",
      "G loss: 0.17369535565376282\n",
      "E loss:  0.4026726186275482\n",
      "G loss: 0.20327205955982208\n",
      "E loss:  0.392873615026474\n",
      "G loss: 0.1818840503692627\n",
      "E loss:  0.39215126633644104\n",
      "G loss: 0.23336273431777954\n",
      "Training Model  ...\n",
      "E loss:  0.43131911754608154\n",
      "G loss: 0.24987858533859253\n",
      "E loss:  0.43042758107185364\n",
      "G loss: 0.2248951643705368\n",
      "E loss:  0.43015244603157043\n",
      "G loss: 0.23322923481464386\n",
      "E loss:  0.42610397934913635\n",
      "G loss: 0.21626624464988708\n",
      "E loss:  0.43504267930984497\n",
      "G loss: 0.2085888385772705\n",
      "Training Model  ...\n",
      "E loss:  0.4449704885482788\n",
      "G loss: 0.2185407131910324\n",
      "E loss:  0.44988423585891724\n",
      "G loss: 0.26410990953445435\n",
      "E loss:  0.446383535861969\n",
      "G loss: 0.22166693210601807\n",
      "E loss:  0.4312436282634735\n",
      "G loss: 0.24820299446582794\n",
      "E loss:  0.4319777488708496\n",
      "G loss: 0.24391499161720276\n",
      "Training Model  ...\n",
      "E loss:  0.43014392256736755\n",
      "G loss: 0.255007266998291\n",
      "E loss:  0.42250359058380127\n",
      "G loss: 0.24516932666301727\n",
      "E loss:  0.4286041557788849\n",
      "G loss: 0.23161651194095612\n",
      "E loss:  0.4260834753513336\n",
      "G loss: 0.2463490217924118\n",
      "E loss:  0.42051711678504944\n",
      "G loss: 0.24718284606933594\n",
      "Training Model  ...\n",
      "E loss:  0.36601951718330383\n",
      "G loss: 0.27286767959594727\n",
      "E loss:  0.36238616704940796\n",
      "G loss: 0.27021825313568115\n",
      "E loss:  0.3696576654911041\n",
      "G loss: 0.2812957167625427\n",
      "E loss:  0.3676981031894684\n",
      "G loss: 0.28212326765060425\n",
      "E loss:  0.36968863010406494\n",
      "G loss: 0.2637355625629425\n",
      "Training Model  ...\n",
      "E loss:  0.4332565367221832\n",
      "G loss: 0.23469609022140503\n",
      "E loss:  0.42704999446868896\n",
      "G loss: 0.25025445222854614\n",
      "E loss:  0.43692746758461\n",
      "G loss: 0.2797471880912781\n",
      "E loss:  0.43361762166023254\n",
      "G loss: 0.24580559134483337\n",
      "E loss:  0.427559494972229\n",
      "G loss: 0.21284399926662445\n",
      "Training Model  ...\n",
      "E loss:  0.40994617342948914\n",
      "G loss: 0.23152288794517517\n",
      "E loss:  0.4112510681152344\n",
      "G loss: 0.22504451870918274\n",
      "E loss:  0.40503421425819397\n",
      "G loss: 0.23005473613739014\n",
      "E loss:  0.39982137084007263\n",
      "G loss: 0.23664286732673645\n",
      "E loss:  0.3926244080066681\n",
      "G loss: 0.2546237111091614\n",
      "Training Model  ...\n",
      "E loss:  0.41831982135772705\n",
      "G loss: 0.25793150067329407\n",
      "E loss:  0.43510666489601135\n",
      "G loss: 0.2351531684398651\n",
      "E loss:  0.432844877243042\n",
      "G loss: 0.23565199971199036\n",
      "E loss:  0.4429156482219696\n",
      "G loss: 0.2595117688179016\n",
      "E loss:  0.44273990392684937\n",
      "G loss: 0.23808042705059052\n",
      "Training Model  ...\n",
      "E loss:  0.4203811585903168\n",
      "G loss: 0.2119465172290802\n",
      "E loss:  0.4068056643009186\n",
      "G loss: 0.18573690950870514\n",
      "E loss:  0.4207993745803833\n",
      "G loss: 0.17819777131080627\n",
      "E loss:  0.4197434186935425\n",
      "G loss: 0.21872112154960632\n",
      "E loss:  0.39893466234207153\n",
      "G loss: 0.22457093000411987\n",
      "Training Model  ...\n",
      "E loss:  0.4024101495742798\n",
      "G loss: 0.22776789963245392\n",
      "E loss:  0.38920122385025024\n",
      "G loss: 0.21402055025100708\n",
      "E loss:  0.3735780119895935\n",
      "G loss: 0.23128396272659302\n",
      "E loss:  0.3719196915626526\n",
      "G loss: 0.22393393516540527\n",
      "E loss:  0.37034597992897034\n",
      "G loss: 0.20678381621837616\n",
      "Training Model  ...\n",
      "E loss:  0.40648430585861206\n",
      "G loss: 0.26672178506851196\n",
      "E loss:  0.40421023964881897\n",
      "G loss: 0.28735557198524475\n",
      "E loss:  0.40497446060180664\n",
      "G loss: 0.27332603931427\n",
      "E loss:  0.4102678894996643\n",
      "G loss: 0.2420608550310135\n",
      "E loss:  0.40104028582572937\n",
      "G loss: 0.23627939820289612\n",
      "Training Model  ...\n",
      "E loss:  0.3952726721763611\n",
      "G loss: 0.23195400834083557\n",
      "E loss:  0.39937731623649597\n",
      "G loss: 0.20998899638652802\n",
      "E loss:  0.38694924116134644\n",
      "G loss: 0.21943634748458862\n",
      "E loss:  0.3883896470069885\n",
      "G loss: 0.18532432615756989\n",
      "E loss:  0.3725326359272003\n",
      "G loss: 0.2170931100845337\n",
      "Training Model  ...\n",
      "E loss:  0.41218405961990356\n",
      "G loss: 0.1937033236026764\n",
      "E loss:  0.4120597839355469\n",
      "G loss: 0.21993857622146606\n",
      "E loss:  0.4065002501010895\n",
      "G loss: 0.21367518603801727\n",
      "E loss:  0.39864176511764526\n",
      "G loss: 0.22050665318965912\n",
      "E loss:  0.40066856145858765\n",
      "G loss: 0.22816769778728485\n",
      "Training Model  ...\n",
      "E loss:  0.4308282136917114\n",
      "G loss: 0.22313883900642395\n",
      "E loss:  0.4369542598724365\n",
      "G loss: 0.26037493348121643\n",
      "E loss:  0.4486550986766815\n",
      "G loss: 0.3222426176071167\n",
      "E loss:  0.45096710324287415\n",
      "G loss: 0.25274598598480225\n",
      "E loss:  0.43513381481170654\n",
      "G loss: 0.2187211811542511\n",
      "Training Model  ...\n",
      "E loss:  0.3889382779598236\n",
      "G loss: 0.2291572093963623\n",
      "E loss:  0.39413735270500183\n",
      "G loss: 0.24052871763706207\n",
      "E loss:  0.39624661207199097\n",
      "G loss: 0.22343818843364716\n",
      "E loss:  0.40141943097114563\n",
      "G loss: 0.20081287622451782\n",
      "E loss:  0.38596072793006897\n",
      "G loss: 0.19998900592327118\n",
      "Training Model  ...\n",
      "E loss:  0.4053896367549896\n",
      "G loss: 0.22827722132205963\n",
      "E loss:  0.39899933338165283\n",
      "G loss: 0.20685221254825592\n",
      "E loss:  0.3967130184173584\n",
      "G loss: 0.21534673869609833\n",
      "E loss:  0.4073809087276459\n",
      "G loss: 0.2148672342300415\n",
      "E loss:  0.4030270576477051\n",
      "G loss: 0.19699114561080933\n",
      "Training Model  ...\n",
      "E loss:  0.4112750291824341\n",
      "G loss: 0.22309726476669312\n",
      "E loss:  0.413056343793869\n",
      "G loss: 0.2480052262544632\n",
      "E loss:  0.41370853781700134\n",
      "G loss: 0.25140267610549927\n",
      "E loss:  0.410755455493927\n",
      "G loss: 0.2050803154706955\n",
      "E loss:  0.41150107979774475\n",
      "G loss: 0.22830909490585327\n",
      "Training Model  ...\n",
      "E loss:  0.46831679344177246\n",
      "G loss: 0.29642003774642944\n",
      "E loss:  0.4637872278690338\n",
      "G loss: 0.25305116176605225\n",
      "E loss:  0.46360155940055847\n",
      "G loss: 0.26695170998573303\n",
      "E loss:  0.46683093905448914\n",
      "G loss: 0.28652718663215637\n",
      "E loss:  0.4689754247665405\n",
      "G loss: 0.2552824020385742\n",
      "Training Model  ...\n",
      "E loss:  0.4444655179977417\n",
      "G loss: 0.25790464878082275\n",
      "E loss:  0.44170135259628296\n",
      "G loss: 0.21612274646759033\n",
      "E loss:  0.4417519271373749\n",
      "G loss: 0.20714181661605835\n",
      "E loss:  0.4446868300437927\n",
      "G loss: 0.23209568858146667\n",
      "E loss:  0.4437243342399597\n",
      "G loss: 0.28013941645622253\n",
      "Training Model  ...\n",
      "E loss:  0.44571417570114136\n",
      "G loss: 0.29096755385398865\n",
      "E loss:  0.4432137608528137\n",
      "G loss: 0.24382761120796204\n",
      "E loss:  0.4412047266960144\n",
      "G loss: 0.2538365423679352\n",
      "E loss:  0.44508153200149536\n",
      "G loss: 0.2841300964355469\n",
      "E loss:  0.4441835582256317\n",
      "G loss: 0.27564728260040283\n",
      "Training Model  ...\n",
      "E loss:  0.37876859307289124\n",
      "G loss: 0.23719725012779236\n",
      "E loss:  0.37926605343818665\n",
      "G loss: 0.25680670142173767\n",
      "E loss:  0.38404130935668945\n",
      "G loss: 0.2424391210079193\n",
      "E loss:  0.37554025650024414\n",
      "G loss: 0.2584843933582306\n",
      "E loss:  0.37302321195602417\n",
      "G loss: 0.24873003363609314\n",
      "Training Model  ...\n",
      "E loss:  0.4385877251625061\n",
      "G loss: 0.22208023071289062\n",
      "E loss:  0.4194449484348297\n",
      "G loss: 0.17604592442512512\n",
      "E loss:  0.4311411380767822\n",
      "G loss: 0.2214120626449585\n",
      "E loss:  0.4285080134868622\n",
      "G loss: 0.19974654912948608\n",
      "E loss:  0.4262385368347168\n",
      "G loss: 0.20648223161697388\n",
      "Training Model  ...\n",
      "E loss:  0.43293890357017517\n",
      "G loss: 0.22811399400234222\n",
      "E loss:  0.43760713934898376\n",
      "G loss: 0.2060282677412033\n",
      "E loss:  0.42724737524986267\n",
      "G loss: 0.2537117004394531\n",
      "E loss:  0.41740643978118896\n",
      "G loss: 0.25899991393089294\n",
      "E loss:  0.41311579942703247\n",
      "G loss: 0.24920451641082764\n",
      "Training Model  ...\n",
      "E loss:  0.3964266777038574\n",
      "G loss: 0.23112867772579193\n",
      "E loss:  0.39368370175361633\n",
      "G loss: 0.19494681060314178\n",
      "E loss:  0.39530783891677856\n",
      "G loss: 0.23435838520526886\n",
      "E loss:  0.39379483461380005\n",
      "G loss: 0.23545286059379578\n",
      "E loss:  0.3981618285179138\n",
      "G loss: 0.2013179212808609\n",
      "Training Model  ...\n",
      "E loss:  0.4358268082141876\n",
      "G loss: 0.26241421699523926\n",
      "E loss:  0.4331769049167633\n",
      "G loss: 0.28471651673316956\n",
      "E loss:  0.4341624081134796\n",
      "G loss: 0.2838597595691681\n",
      "E loss:  0.4285642206668854\n",
      "G loss: 0.2693069279193878\n",
      "E loss:  0.4141424298286438\n",
      "G loss: 0.2504461407661438\n",
      "Training Model  ...\n",
      "E loss:  0.4083024263381958\n",
      "G loss: 0.23707105219364166\n",
      "E loss:  0.40642857551574707\n",
      "G loss: 0.23447538912296295\n",
      "E loss:  0.40646860003471375\n",
      "G loss: 0.21898290514945984\n",
      "E loss:  0.4057363271713257\n",
      "G loss: 0.219964399933815\n",
      "E loss:  0.4073086380958557\n",
      "G loss: 0.238839253783226\n",
      "Training Model  ...\n",
      "E loss:  0.42904314398765564\n",
      "G loss: 0.23068948090076447\n",
      "E loss:  0.4154377281665802\n",
      "G loss: 0.2599385678768158\n",
      "E loss:  0.42719313502311707\n",
      "G loss: 0.25534704327583313\n",
      "E loss:  0.42323410511016846\n",
      "G loss: 0.2807736396789551\n",
      "E loss:  0.42282506823539734\n",
      "G loss: 0.2463841736316681\n",
      "Training Model  ...\n",
      "E loss:  0.404218852519989\n",
      "G loss: 0.22584839165210724\n",
      "E loss:  0.41677749156951904\n",
      "G loss: 0.2354499101638794\n",
      "E loss:  0.41286027431488037\n",
      "G loss: 0.20745804905891418\n",
      "E loss:  0.4136490225791931\n",
      "G loss: 0.25662726163864136\n",
      "E loss:  0.41139280796051025\n",
      "G loss: 0.22543774545192719\n",
      "Training Model  ...\n",
      "E loss:  0.4091527462005615\n",
      "G loss: 0.22148025035858154\n",
      "E loss:  0.41987937688827515\n",
      "G loss: 0.220990851521492\n",
      "E loss:  0.41955870389938354\n",
      "G loss: 0.2355346977710724\n",
      "E loss:  0.4298195540904999\n",
      "G loss: 0.2594985365867615\n",
      "E loss:  0.41962140798568726\n",
      "G loss: 0.27518096566200256\n",
      "Training Model  ...\n",
      "E loss:  0.41025587916374207\n",
      "G loss: 0.2649349272251129\n",
      "E loss:  0.40503042936325073\n",
      "G loss: 0.30854156613349915\n",
      "E loss:  0.41258570551872253\n",
      "G loss: 0.3140532970428467\n",
      "E loss:  0.4106091260910034\n",
      "G loss: 0.3025084435939789\n",
      "E loss:  0.4098356366157532\n",
      "G loss: 0.2543654143810272\n",
      "Training Model  ...\n",
      "E loss:  0.3807561993598938\n",
      "G loss: 0.2594662010669708\n",
      "E loss:  0.38745221495628357\n",
      "G loss: 0.24805811047554016\n",
      "E loss:  0.3916236460208893\n",
      "G loss: 0.20208384096622467\n",
      "E loss:  0.3989624083042145\n",
      "G loss: 0.24851733446121216\n",
      "E loss:  0.3993343710899353\n",
      "G loss: 0.25545012950897217\n",
      "Training Model  ...\n",
      "E loss:  0.4119870960712433\n",
      "G loss: 0.27477771043777466\n",
      "E loss:  0.40694233775138855\n",
      "G loss: 0.2609507143497467\n",
      "E loss:  0.4109344780445099\n",
      "G loss: 0.23662343621253967\n",
      "E loss:  0.4105885922908783\n",
      "G loss: 0.23783764243125916\n",
      "E loss:  0.3998773396015167\n",
      "G loss: 0.20470909774303436\n",
      "Training Model  ...\n",
      "E loss:  0.4233097732067108\n",
      "G loss: 0.24870525300502777\n",
      "E loss:  0.4267720580101013\n",
      "G loss: 0.2210826426744461\n",
      "E loss:  0.42558974027633667\n",
      "G loss: 0.24177289009094238\n",
      "E loss:  0.4207404851913452\n",
      "G loss: 0.22640952467918396\n",
      "E loss:  0.41736680269241333\n",
      "G loss: 0.2195320427417755\n",
      "Training Model  ...\n",
      "E loss:  0.423721581697464\n",
      "G loss: 0.24653339385986328\n",
      "E loss:  0.43256381154060364\n",
      "G loss: 0.23702499270439148\n",
      "E loss:  0.42531973123550415\n",
      "G loss: 0.23935845494270325\n",
      "E loss:  0.42804640531539917\n",
      "G loss: 0.25241485238075256\n",
      "E loss:  0.42114317417144775\n",
      "G loss: 0.2685924172401428\n",
      "Training Model  ...\n",
      "E loss:  0.3912751078605652\n",
      "G loss: 0.20421144366264343\n",
      "E loss:  0.40401265025138855\n",
      "G loss: 0.2086159735918045\n",
      "E loss:  0.4080042541027069\n",
      "G loss: 0.20091482996940613\n",
      "E loss:  0.3995024263858795\n",
      "G loss: 0.2180069386959076\n",
      "E loss:  0.3956855833530426\n",
      "G loss: 0.2609326243400574\n",
      "Training Model  ...\n",
      "E loss:  0.40247243642807007\n",
      "G loss: 0.19811978936195374\n",
      "E loss:  0.40650004148483276\n",
      "G loss: 0.2221945822238922\n",
      "E loss:  0.39945292472839355\n",
      "G loss: 0.21859578788280487\n",
      "E loss:  0.40292882919311523\n",
      "G loss: 0.22211554646492004\n",
      "E loss:  0.39511287212371826\n",
      "G loss: 0.2357853800058365\n",
      "Training Model  ...\n",
      "E loss:  0.41532832384109497\n",
      "G loss: 0.28019610047340393\n",
      "E loss:  0.41179358959198\n",
      "G loss: 0.27110031247138977\n",
      "E loss:  0.4176125228404999\n",
      "G loss: 0.2861892580986023\n",
      "E loss:  0.42742449045181274\n",
      "G loss: 0.26646217703819275\n",
      "E loss:  0.43324583768844604\n",
      "G loss: 0.25945770740509033\n",
      "Training Model  ...\n",
      "E loss:  0.3428291380405426\n",
      "G loss: 0.25723934173583984\n",
      "E loss:  0.3535681366920471\n",
      "G loss: 0.24736140668392181\n",
      "E loss:  0.34944188594818115\n",
      "G loss: 0.2273441106081009\n",
      "E loss:  0.3488794267177582\n",
      "G loss: 0.25654032826423645\n",
      "E loss:  0.35816437005996704\n",
      "G loss: 0.26773005723953247\n",
      "Training Model  ...\n",
      "E loss:  0.3706546425819397\n",
      "G loss: 0.2104860544204712\n",
      "E loss:  0.3712821304798126\n",
      "G loss: 0.23012007772922516\n",
      "E loss:  0.37578654289245605\n",
      "G loss: 0.24079212546348572\n",
      "E loss:  0.38189178705215454\n",
      "G loss: 0.27968716621398926\n",
      "E loss:  0.38506072759628296\n",
      "G loss: 0.25834938883781433\n",
      "Training Model  ...\n",
      "E loss:  0.45221683382987976\n",
      "G loss: 0.2631104588508606\n",
      "E loss:  0.43619969487190247\n",
      "G loss: 0.26584023237228394\n",
      "E loss:  0.4349977672100067\n",
      "G loss: 0.22657616436481476\n",
      "E loss:  0.43791988492012024\n",
      "G loss: 0.26187556982040405\n",
      "E loss:  0.43671655654907227\n",
      "G loss: 0.2374495416879654\n",
      "Training Model  ...\n",
      "E loss:  0.4019421637058258\n",
      "G loss: 0.25752872228622437\n",
      "E loss:  0.3945622742176056\n",
      "G loss: 0.2539214491844177\n",
      "E loss:  0.3999187648296356\n",
      "G loss: 0.2991325855255127\n",
      "E loss:  0.39944854378700256\n",
      "G loss: 0.26968079805374146\n",
      "E loss:  0.39173775911331177\n",
      "G loss: 0.22740299999713898\n",
      "Training Model  ...\n",
      "E loss:  0.42677122354507446\n",
      "G loss: 0.2405686378479004\n",
      "E loss:  0.4374518394470215\n",
      "G loss: 0.2970407009124756\n",
      "E loss:  0.4500455856323242\n",
      "G loss: 0.23670154809951782\n",
      "E loss:  0.4425477087497711\n",
      "G loss: 0.2575490474700928\n",
      "E loss:  0.44611576199531555\n",
      "G loss: 0.2741369903087616\n",
      "Training Model  ...\n",
      "E loss:  0.4172709286212921\n",
      "G loss: 0.2597850263118744\n",
      "E loss:  0.4131474494934082\n",
      "G loss: 0.20186716318130493\n",
      "E loss:  0.4043485224246979\n",
      "G loss: 0.20897617936134338\n",
      "E loss:  0.4031186103820801\n",
      "G loss: 0.23967675864696503\n",
      "E loss:  0.4030615985393524\n",
      "G loss: 0.23096314072608948\n",
      "Training Model  ...\n",
      "E loss:  0.454886257648468\n",
      "G loss: 0.25362905859947205\n",
      "E loss:  0.43751055002212524\n",
      "G loss: 0.2364668846130371\n",
      "E loss:  0.43283259868621826\n",
      "G loss: 0.19409389793872833\n",
      "E loss:  0.4301101863384247\n",
      "G loss: 0.26610249280929565\n",
      "E loss:  0.41976094245910645\n",
      "G loss: 0.2472183257341385\n",
      "Training Model  ...\n",
      "E loss:  0.37088117003440857\n",
      "G loss: 0.23688320815563202\n",
      "E loss:  0.37117689847946167\n",
      "G loss: 0.21513812243938446\n",
      "E loss:  0.37133803963661194\n",
      "G loss: 0.19539521634578705\n",
      "E loss:  0.3749333322048187\n",
      "G loss: 0.21407832205295563\n",
      "E loss:  0.3629218637943268\n",
      "G loss: 0.22135069966316223\n",
      "Training Model  ...\n",
      "E loss:  0.4816293716430664\n",
      "G loss: 0.2555408179759979\n",
      "E loss:  0.48918747901916504\n",
      "G loss: 0.2522629499435425\n",
      "E loss:  0.4933774173259735\n",
      "G loss: 0.2555796205997467\n",
      "E loss:  0.4871959984302521\n",
      "G loss: 0.26606595516204834\n",
      "E loss:  0.48550426959991455\n",
      "G loss: 0.2716842293739319\n",
      "Training Model  ...\n",
      "E loss:  0.45014652609825134\n",
      "G loss: 0.27414706349372864\n",
      "E loss:  0.44144970178604126\n",
      "G loss: 0.26387226581573486\n",
      "E loss:  0.443673700094223\n",
      "G loss: 0.29583311080932617\n",
      "E loss:  0.4360189139842987\n",
      "G loss: 0.2667520344257355\n",
      "E loss:  0.4318107068538666\n",
      "G loss: 0.23499782383441925\n",
      "Training Model  ...\n",
      "E loss:  0.3896114230155945\n",
      "G loss: 0.23220135271549225\n",
      "E loss:  0.3946796655654907\n",
      "G loss: 0.226018488407135\n",
      "E loss:  0.3935239017009735\n",
      "G loss: 0.22019200026988983\n",
      "E loss:  0.40270859003067017\n",
      "G loss: 0.22300046682357788\n",
      "E loss:  0.3970680236816406\n",
      "G loss: 0.2597076892852783\n",
      "Training Model  ...\n",
      "E loss:  0.43462732434272766\n",
      "G loss: 0.2472091168165207\n",
      "E loss:  0.43842506408691406\n",
      "G loss: 0.2034997195005417\n",
      "E loss:  0.4473598897457123\n",
      "G loss: 0.22521625459194183\n",
      "E loss:  0.45071616768836975\n",
      "G loss: 0.2225058376789093\n",
      "E loss:  0.4523600935935974\n",
      "G loss: 0.22854287922382355\n",
      "Training Model  ...\n",
      "E loss:  0.3937900960445404\n",
      "G loss: 0.22618955373764038\n",
      "E loss:  0.3973151445388794\n",
      "G loss: 0.24416567385196686\n",
      "E loss:  0.40496498346328735\n",
      "G loss: 0.19229865074157715\n",
      "E loss:  0.40783578157424927\n",
      "G loss: 0.25185564160346985\n",
      "E loss:  0.408713161945343\n",
      "G loss: 0.22433032095432281\n",
      "Training Model  ...\n",
      "E loss:  0.43586960434913635\n",
      "G loss: 0.2806410789489746\n",
      "E loss:  0.4332919120788574\n",
      "G loss: 0.3115885853767395\n",
      "E loss:  0.429790735244751\n",
      "G loss: 0.25780895352363586\n",
      "E loss:  0.4303359389305115\n",
      "G loss: 0.25683775544166565\n",
      "E loss:  0.42851927876472473\n",
      "G loss: 0.23853473365306854\n",
      "Training Model  ...\n",
      "E loss:  0.44127169251441956\n",
      "G loss: 0.2476927936077118\n",
      "E loss:  0.44296029210090637\n",
      "G loss: 0.2877306044101715\n",
      "E loss:  0.439289927482605\n",
      "G loss: 0.2791694402694702\n",
      "E loss:  0.4355630874633789\n",
      "G loss: 0.271575927734375\n",
      "E loss:  0.43205052614212036\n",
      "G loss: 0.21660825610160828\n",
      "Training Model  ...\n",
      "E loss:  0.3969351053237915\n",
      "G loss: 0.232864111661911\n",
      "E loss:  0.4022500813007355\n",
      "G loss: 0.22140617668628693\n",
      "E loss:  0.3899593949317932\n",
      "G loss: 0.22952134907245636\n",
      "E loss:  0.39975839853286743\n",
      "G loss: 0.21171414852142334\n",
      "E loss:  0.3858766555786133\n",
      "G loss: 0.2275254875421524\n",
      "Training Model  ...\n",
      "E loss:  0.41570350527763367\n",
      "G loss: 0.2290973663330078\n",
      "E loss:  0.4191640019416809\n",
      "G loss: 0.23607787489891052\n",
      "E loss:  0.41900205612182617\n",
      "G loss: 0.23367846012115479\n",
      "E loss:  0.424964040517807\n",
      "G loss: 0.2186715304851532\n",
      "E loss:  0.4202548861503601\n",
      "G loss: 0.26086127758026123\n",
      "Training Model  ...\n",
      "E loss:  0.3879700005054474\n",
      "G loss: 0.21993321180343628\n",
      "E loss:  0.3857327103614807\n",
      "G loss: 0.2265867292881012\n",
      "E loss:  0.3830350637435913\n",
      "G loss: 0.2376856803894043\n",
      "E loss:  0.376642107963562\n",
      "G loss: 0.26183706521987915\n",
      "E loss:  0.37988752126693726\n",
      "G loss: 0.23665836453437805\n",
      "Training Model  ...\n",
      "E loss:  0.40390074253082275\n",
      "G loss: 0.2052304744720459\n",
      "E loss:  0.40390682220458984\n",
      "G loss: 0.20352518558502197\n",
      "E loss:  0.3965719938278198\n",
      "G loss: 0.22504687309265137\n",
      "E loss:  0.3908339738845825\n",
      "G loss: 0.22454053163528442\n",
      "E loss:  0.3986104428768158\n",
      "G loss: 0.21870341897010803\n",
      "Training Model  ...\n",
      "E loss:  0.42311736941337585\n",
      "G loss: 0.25142037868499756\n",
      "E loss:  0.4315771758556366\n",
      "G loss: 0.2274666577577591\n",
      "E loss:  0.4305434226989746\n",
      "G loss: 0.24748243391513824\n",
      "E loss:  0.43326321244239807\n",
      "G loss: 0.23377428948879242\n",
      "E loss:  0.4303392469882965\n",
      "G loss: 0.20086759328842163\n",
      "Training Model  ...\n",
      "E loss:  0.37913084030151367\n",
      "G loss: 0.22710320353507996\n",
      "E loss:  0.3757552206516266\n",
      "G loss: 0.24214063584804535\n",
      "E loss:  0.37777817249298096\n",
      "G loss: 0.21629022061824799\n",
      "E loss:  0.37126216292381287\n",
      "G loss: 0.23132848739624023\n",
      "E loss:  0.37656253576278687\n",
      "G loss: 0.24727262556552887\n",
      "Training Model  ...\n",
      "E loss:  0.35512182116508484\n",
      "G loss: 0.24222491681575775\n",
      "E loss:  0.3574943542480469\n",
      "G loss: 0.21873648464679718\n",
      "E loss:  0.3492661118507385\n",
      "G loss: 0.21673306822776794\n",
      "E loss:  0.3543259799480438\n",
      "G loss: 0.22440974414348602\n",
      "E loss:  0.35860708355903625\n",
      "G loss: 0.2633064091205597\n",
      "Training Model  ...\n",
      "E loss:  0.3919229507446289\n",
      "G loss: 0.23016966879367828\n",
      "E loss:  0.391195148229599\n",
      "G loss: 0.21326740086078644\n",
      "E loss:  0.39312121272087097\n",
      "G loss: 0.23988115787506104\n",
      "E loss:  0.39913415908813477\n",
      "G loss: 0.21584823727607727\n",
      "E loss:  0.40152889490127563\n",
      "G loss: 0.19577786326408386\n",
      "Training Model  ...\n",
      "E loss:  0.4089294970035553\n",
      "G loss: 0.2341429740190506\n",
      "E loss:  0.41204455494880676\n",
      "G loss: 0.21244403719902039\n",
      "E loss:  0.41118112206459045\n",
      "G loss: 0.21053680777549744\n",
      "E loss:  0.4034730792045593\n",
      "G loss: 0.20798316597938538\n",
      "E loss:  0.4062867760658264\n",
      "G loss: 0.24132981896400452\n",
      "Training Model  ...\n",
      "E loss:  0.4326937794685364\n",
      "G loss: 0.2440802901983261\n",
      "E loss:  0.4208309054374695\n",
      "G loss: 0.26460862159729004\n",
      "E loss:  0.43253064155578613\n",
      "G loss: 0.25951746106147766\n",
      "E loss:  0.4206106960773468\n",
      "G loss: 0.23636263608932495\n",
      "E loss:  0.4173373281955719\n",
      "G loss: 0.20900975167751312\n",
      "Training Model  ...\n",
      "E loss:  0.41823258996009827\n",
      "G loss: 0.21430252492427826\n",
      "E loss:  0.4088883697986603\n",
      "G loss: 0.23453986644744873\n",
      "E loss:  0.4170871078968048\n",
      "G loss: 0.2557266652584076\n",
      "E loss:  0.41218656301498413\n",
      "G loss: 0.2450210303068161\n",
      "E loss:  0.4049426019191742\n",
      "G loss: 0.23077169060707092\n",
      "Training Model  ...\n",
      "E loss:  0.3795984983444214\n",
      "G loss: 0.2232196182012558\n",
      "E loss:  0.37734755873680115\n",
      "G loss: 0.2215302586555481\n",
      "E loss:  0.37593579292297363\n",
      "G loss: 0.19916656613349915\n",
      "E loss:  0.37165337800979614\n",
      "G loss: 0.21551547944545746\n",
      "E loss:  0.3670637011528015\n",
      "G loss: 0.2258615791797638\n",
      "Training Model  ...\n",
      "E loss:  0.41395318508148193\n",
      "G loss: 0.2248082160949707\n",
      "E loss:  0.414775550365448\n",
      "G loss: 0.24507620930671692\n",
      "E loss:  0.41447269916534424\n",
      "G loss: 0.20957371592521667\n",
      "E loss:  0.417318195104599\n",
      "G loss: 0.2097119688987732\n",
      "E loss:  0.4157869219779968\n",
      "G loss: 0.20627275109291077\n",
      "Training Model  ...\n",
      "E loss:  0.4110840857028961\n",
      "G loss: 0.2533425986766815\n",
      "E loss:  0.41140908002853394\n",
      "G loss: 0.2899963855743408\n",
      "E loss:  0.4184821844100952\n",
      "G loss: 0.2747827470302582\n",
      "E loss:  0.40977543592453003\n",
      "G loss: 0.2295655459165573\n",
      "E loss:  0.4071877896785736\n",
      "G loss: 0.22836154699325562\n",
      "Training Model  ...\n",
      "E loss:  0.365294873714447\n",
      "G loss: 0.21944358944892883\n",
      "E loss:  0.36932432651519775\n",
      "G loss: 0.24772173166275024\n",
      "E loss:  0.36772167682647705\n",
      "G loss: 0.2574752867221832\n",
      "E loss:  0.37539318203926086\n",
      "G loss: 0.2266581654548645\n",
      "E loss:  0.3719879984855652\n",
      "G loss: 0.2010173499584198\n",
      "Training Model  ...\n",
      "E loss:  0.41494011878967285\n",
      "G loss: 0.26051557064056396\n",
      "E loss:  0.4164884686470032\n",
      "G loss: 0.2203831523656845\n",
      "E loss:  0.42640015482902527\n",
      "G loss: 0.24011069536209106\n",
      "E loss:  0.4218880534172058\n",
      "G loss: 0.20942716300487518\n",
      "E loss:  0.4153805375099182\n",
      "G loss: 0.24783213436603546\n",
      "Training Model  ...\n",
      "E loss:  0.38700243830680847\n",
      "G loss: 0.17523550987243652\n",
      "E loss:  0.3896563649177551\n",
      "G loss: 0.230402410030365\n",
      "E loss:  0.3970380127429962\n",
      "G loss: 0.2058981955051422\n",
      "E loss:  0.39161643385887146\n",
      "G loss: 0.21733646094799042\n",
      "E loss:  0.3926609754562378\n",
      "G loss: 0.22849486768245697\n",
      "Training Model  ...\n",
      "E loss:  0.4087754786014557\n",
      "G loss: 0.24475520849227905\n",
      "E loss:  0.39879119396209717\n",
      "G loss: 0.2556661069393158\n",
      "E loss:  0.39339959621429443\n",
      "G loss: 0.24870312213897705\n",
      "E loss:  0.39346712827682495\n",
      "G loss: 0.24673868715763092\n",
      "E loss:  0.395844042301178\n",
      "G loss: 0.22826935350894928\n",
      "Training Model  ...\n",
      "E loss:  0.3896142840385437\n",
      "G loss: 0.2208903282880783\n",
      "E loss:  0.39561641216278076\n",
      "G loss: 0.23905928432941437\n",
      "E loss:  0.40608519315719604\n",
      "G loss: 0.2527925968170166\n",
      "E loss:  0.40958279371261597\n",
      "G loss: 0.23587104678153992\n",
      "E loss:  0.40334540605545044\n",
      "G loss: 0.2497653365135193\n",
      "Training Model  ...\n",
      "E loss:  0.3530855178833008\n",
      "G loss: 0.2091270536184311\n",
      "E loss:  0.34703829884529114\n",
      "G loss: 0.20512038469314575\n",
      "E loss:  0.3486536741256714\n",
      "G loss: 0.187901571393013\n",
      "E loss:  0.35027092695236206\n",
      "G loss: 0.19388559460639954\n",
      "E loss:  0.3485296964645386\n",
      "G loss: 0.22882232069969177\n",
      "Training Model  ...\n",
      "E loss:  0.3606404960155487\n",
      "G loss: 0.24621811509132385\n",
      "E loss:  0.36333125829696655\n",
      "G loss: 0.23612459003925323\n",
      "E loss:  0.36477336287498474\n",
      "G loss: 0.23454824090003967\n",
      "E loss:  0.3642338812351227\n",
      "G loss: 0.21029166877269745\n",
      "E loss:  0.3623749613761902\n",
      "G loss: 0.20921246707439423\n",
      "Training Model  ...\n",
      "E loss:  0.3884529173374176\n",
      "G loss: 0.2235262095928192\n",
      "E loss:  0.39905840158462524\n",
      "G loss: 0.25859469175338745\n",
      "E loss:  0.4074823558330536\n",
      "G loss: 0.23188060522079468\n",
      "E loss:  0.4074289798736572\n",
      "G loss: 0.2303789258003235\n",
      "E loss:  0.405384361743927\n",
      "G loss: 0.20077653229236603\n",
      "Training Model  ...\n",
      "E loss:  0.44885629415512085\n",
      "G loss: 0.2169029861688614\n",
      "E loss:  0.4448438882827759\n",
      "G loss: 0.2458006739616394\n",
      "E loss:  0.4345850646495819\n",
      "G loss: 0.31394854187965393\n",
      "E loss:  0.4350864589214325\n",
      "G loss: 0.25935831665992737\n",
      "E loss:  0.4371679127216339\n",
      "G loss: 0.27274832129478455\n",
      "Training Model  ...\n",
      "E loss:  0.43944597244262695\n",
      "G loss: 0.26345643401145935\n",
      "E loss:  0.43180590867996216\n",
      "G loss: 0.2323601245880127\n",
      "E loss:  0.4379388689994812\n",
      "G loss: 0.2124534696340561\n",
      "E loss:  0.43255457282066345\n",
      "G loss: 0.21400333940982819\n",
      "E loss:  0.42978349328041077\n",
      "G loss: 0.2107200026512146\n",
      "Training Model  ...\n",
      "E loss:  0.40432941913604736\n",
      "G loss: 0.22424514591693878\n",
      "E loss:  0.39370888471603394\n",
      "G loss: 0.2953489422798157\n",
      "E loss:  0.39488115906715393\n",
      "G loss: 0.23195284605026245\n",
      "E loss:  0.3960634768009186\n",
      "G loss: 0.2339630126953125\n",
      "E loss:  0.3983824849128723\n",
      "G loss: 0.23742157220840454\n",
      "Training Model  ...\n",
      "E loss:  0.386374831199646\n",
      "G loss: 0.24560388922691345\n",
      "E loss:  0.37539640069007874\n",
      "G loss: 0.25021183490753174\n",
      "E loss:  0.3737870156764984\n",
      "G loss: 0.2611311376094818\n",
      "E loss:  0.37180498242378235\n",
      "G loss: 0.23331160843372345\n",
      "E loss:  0.3753039240837097\n",
      "G loss: 0.2571285367012024\n",
      "Training Model  ...\n",
      "E loss:  0.41260436177253723\n",
      "G loss: 0.23219743371009827\n",
      "E loss:  0.4090590476989746\n",
      "G loss: 0.24677637219429016\n",
      "E loss:  0.4053637683391571\n",
      "G loss: 0.26642897725105286\n",
      "E loss:  0.4086467921733856\n",
      "G loss: 0.29162585735321045\n",
      "E loss:  0.4050899147987366\n",
      "G loss: 0.24145224690437317\n",
      "Training Model  ...\n",
      "E loss:  0.4144893288612366\n",
      "G loss: 0.21166685223579407\n",
      "E loss:  0.41513925790786743\n",
      "G loss: 0.2387053519487381\n",
      "E loss:  0.42618831992149353\n",
      "G loss: 0.2168065905570984\n",
      "E loss:  0.4239029586315155\n",
      "G loss: 0.21617351472377777\n",
      "E loss:  0.421176016330719\n",
      "G loss: 0.24984949827194214\n",
      "Training Model  ...\n",
      "E loss:  0.4346787631511688\n",
      "G loss: 0.26461711525917053\n",
      "E loss:  0.4352995753288269\n",
      "G loss: 0.18834920227527618\n",
      "E loss:  0.433010071516037\n",
      "G loss: 0.21951931715011597\n",
      "E loss:  0.42853760719299316\n",
      "G loss: 0.22352318465709686\n",
      "E loss:  0.42017704248428345\n",
      "G loss: 0.22483384609222412\n",
      "Training Model  ...\n",
      "E loss:  0.44411468505859375\n",
      "G loss: 0.2334328591823578\n",
      "E loss:  0.44023045897483826\n",
      "G loss: 0.2172570526599884\n",
      "E loss:  0.4351724684238434\n",
      "G loss: 0.23875956237316132\n",
      "E loss:  0.43586474657058716\n",
      "G loss: 0.24341964721679688\n",
      "E loss:  0.4288594424724579\n",
      "G loss: 0.27416232228279114\n",
      "Training Model  ...\n",
      "E loss:  0.4203949570655823\n",
      "G loss: 0.24481865763664246\n",
      "E loss:  0.430320143699646\n",
      "G loss: 0.24109849333763123\n",
      "E loss:  0.43098652362823486\n",
      "G loss: 0.24467459321022034\n",
      "E loss:  0.43720442056655884\n",
      "G loss: 0.2753373980522156\n",
      "E loss:  0.4290235638618469\n",
      "G loss: 0.25494158267974854\n",
      "Training Model  ...\n",
      "E loss:  0.4133598208427429\n",
      "G loss: 0.28208816051483154\n",
      "E loss:  0.40172040462493896\n",
      "G loss: 0.2681387662887573\n",
      "E loss:  0.4024216830730438\n",
      "G loss: 0.23602008819580078\n",
      "E loss:  0.39867809414863586\n",
      "G loss: 0.29416775703430176\n",
      "E loss:  0.3915887475013733\n",
      "G loss: 0.25758618116378784\n",
      "Training Model  ...\n",
      "E loss:  0.40322428941726685\n",
      "G loss: 0.24888217449188232\n",
      "E loss:  0.40563708543777466\n",
      "G loss: 0.22106413543224335\n",
      "E loss:  0.40039271116256714\n",
      "G loss: 0.23404721915721893\n",
      "E loss:  0.39901062846183777\n",
      "G loss: 0.2252313196659088\n",
      "E loss:  0.39418986439704895\n",
      "G loss: 0.24541321396827698\n",
      "Training Model  ...\n",
      "E loss:  0.3948878049850464\n",
      "G loss: 0.23358459770679474\n",
      "E loss:  0.39585012197494507\n",
      "G loss: 0.21831950545310974\n",
      "E loss:  0.3966185748577118\n",
      "G loss: 0.24593818187713623\n",
      "E loss:  0.40263456106185913\n",
      "G loss: 0.22661182284355164\n",
      "E loss:  0.4090486168861389\n",
      "G loss: 0.23991133272647858\n",
      "Training Model  ...\n",
      "E loss:  0.3735256791114807\n",
      "G loss: 0.20658904314041138\n",
      "E loss:  0.37664613127708435\n",
      "G loss: 0.22209703922271729\n",
      "E loss:  0.3722005784511566\n",
      "G loss: 0.21459949016571045\n",
      "E loss:  0.37347233295440674\n",
      "G loss: 0.21237143874168396\n",
      "E loss:  0.36171212792396545\n",
      "G loss: 0.21596021950244904\n",
      "Training Model  ...\n",
      "E loss:  0.41101375222206116\n",
      "G loss: 0.24210883677005768\n",
      "E loss:  0.41109001636505127\n",
      "G loss: 0.24955841898918152\n",
      "E loss:  0.42220720648765564\n",
      "G loss: 0.26872721314430237\n",
      "E loss:  0.4096904397010803\n",
      "G loss: 0.24992544949054718\n",
      "E loss:  0.41351318359375\n",
      "G loss: 0.22965256869792938\n",
      "Training Model  ...\n",
      "E loss:  0.427043616771698\n",
      "G loss: 0.22330959141254425\n",
      "E loss:  0.4314862787723541\n",
      "G loss: 0.23603639006614685\n",
      "E loss:  0.4243029057979584\n",
      "G loss: 0.22251901030540466\n",
      "E loss:  0.42532575130462646\n",
      "G loss: 0.2264772653579712\n",
      "E loss:  0.412850558757782\n",
      "G loss: 0.27839475870132446\n",
      "Training Model  ...\n",
      "E loss:  0.41015690565109253\n",
      "G loss: 0.2521437406539917\n",
      "E loss:  0.39835402369499207\n",
      "G loss: 0.20361879467964172\n",
      "E loss:  0.40357598662376404\n",
      "G loss: 0.21435445547103882\n",
      "E loss:  0.415475070476532\n",
      "G loss: 0.24138405919075012\n",
      "E loss:  0.4065689444541931\n",
      "G loss: 0.2421732246875763\n",
      "Training Model  ...\n",
      "E loss:  0.4186190366744995\n",
      "G loss: 0.24657300114631653\n",
      "E loss:  0.4111412763595581\n",
      "G loss: 0.25001731514930725\n",
      "E loss:  0.4077692925930023\n",
      "G loss: 0.23434928059577942\n",
      "E loss:  0.4057309925556183\n",
      "G loss: 0.22612810134887695\n",
      "E loss:  0.40020906925201416\n",
      "G loss: 0.2414211928844452\n",
      "Training Model  ...\n",
      "E loss:  0.3893261253833771\n",
      "G loss: 0.20378486812114716\n",
      "E loss:  0.3810410499572754\n",
      "G loss: 0.22110354900360107\n",
      "E loss:  0.3802725374698639\n",
      "G loss: 0.22035743296146393\n",
      "E loss:  0.3880532383918762\n",
      "G loss: 0.22654694318771362\n",
      "E loss:  0.3770214021205902\n",
      "G loss: 0.23889070749282837\n",
      "Training Model  ...\n",
      "E loss:  0.4191266596317291\n",
      "G loss: 0.2795655429363251\n",
      "E loss:  0.40713369846343994\n",
      "G loss: 0.24959558248519897\n",
      "E loss:  0.40401577949523926\n",
      "G loss: 0.29939717054367065\n",
      "E loss:  0.40769708156585693\n",
      "G loss: 0.3154469132423401\n",
      "E loss:  0.40849313139915466\n",
      "G loss: 0.3043774962425232\n",
      "Training Model  ...\n",
      "E loss:  0.4117239713668823\n",
      "G loss: 0.2555994689464569\n",
      "E loss:  0.40713635087013245\n",
      "G loss: 0.2078080028295517\n",
      "E loss:  0.3934459686279297\n",
      "G loss: 0.24800853431224823\n",
      "E loss:  0.3902576267719269\n",
      "G loss: 0.23653465509414673\n",
      "E loss:  0.3848347067832947\n",
      "G loss: 0.27179577946662903\n",
      "Training Model  ...\n",
      "E loss:  0.3888232111930847\n",
      "G loss: 0.23721125721931458\n",
      "E loss:  0.3904610276222229\n",
      "G loss: 0.25465479493141174\n",
      "E loss:  0.3858735263347626\n",
      "G loss: 0.28729942440986633\n",
      "E loss:  0.37931710481643677\n",
      "G loss: 0.25304850935935974\n",
      "E loss:  0.3805572986602783\n",
      "G loss: 0.24668437242507935\n",
      "\n"
     ]
    }
   ],
   "source": [
    "board_shape = (3,3)\n",
    "generator = NaiveGenerator(board_shape, 1, 256)\n",
    "evaluator = NaiveEvaluator(board_shape, 1, 256)\n",
    "batch_size = 512\n",
    "iterations = 2000\n",
    "\n",
    "for i in tqdm.notebook.tqdm(list(range(iterations))):\n",
    "    generator_training_samples = []\n",
    "    \n",
    "    for _ in range(batch_size):\n",
    "        noise = torch.randn(1, *board_shape)\n",
    "        #metrics = torch.tensor(get_random_metrics()).float()\n",
    "        metrics = torch.tensor(get_one_metrics()).float()\n",
    "        generator_training_samples.append((noise, metrics))\n",
    "        \n",
    "    generator_train_loader = torch.utils.data.DataLoader(\n",
    "        generator_training_samples,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "\n",
    "    optimizer_g = optim.SGD(generator.parameters(),lr=0.0001, momentum=0.9)\n",
    "    optimizer_e = optim.SGD(evaluator.parameters(), lr=0.001, momentum=0.9)\n",
    "    \n",
    "    # for some reason adam does not work as good as SGD\n",
    "    #optimizer_g = optim.Adam(generator.parameters(),lr=0.0001)\n",
    "    #optimizer_e = optim.Adam(evaluator.parameters(), lr=0.0001)\n",
    "    \n",
    "    train_eval = True\n",
    "    train_model(generator, evaluator, generator_train_loader, optimizer_g, optimizer_e, epochs=5, train_e=train_eval)\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T22:02:01.309021Z",
     "start_time": "2020-03-07T22:02:01.286332Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[-0.4865,  1.4574, -1.2855, -1.4279, -0.6453,  0.1917,  1.5447, -0.2996,\n",
      "          1.5044]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.6956, 0.1781, 0.1263],\n",
      "          [0.7969, 0.0967, 0.1064],\n",
      "          [0.4287, 0.2857, 0.2857]],\n",
      "\n",
      "         [[0.8821, 0.0589, 0.0590],\n",
      "          [0.3333, 0.3333, 0.3333],\n",
      "          [0.2573, 0.5395, 0.2032]],\n",
      "\n",
      "         [[0.2304, 0.5392, 0.2304],\n",
      "          [0.2144, 0.5341, 0.2515],\n",
      "          [0.3333, 0.3333, 0.3333]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 0],\n",
      "         [0, 2, 1],\n",
      "         [1, 1, 2]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[ 0.3884,  0.6235, -1.7281, -0.6157, -0.2992,  1.3378,  0.4642,  0.3869,\n",
      "          1.5130]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.5210, 0.2471, 0.2318],\n",
      "          [0.2645, 0.4923, 0.2431],\n",
      "          [0.3324, 0.3353, 0.3324]],\n",
      "\n",
      "         [[0.9581, 0.0202, 0.0217],\n",
      "          [0.4722, 0.2639, 0.2639],\n",
      "          [0.3686, 0.3041, 0.3273]],\n",
      "\n",
      "         [[0.1647, 0.6636, 0.1717],\n",
      "          [0.4453, 0.2773, 0.2773],\n",
      "          [0.3333, 0.3333, 0.3333]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 1, 1],\n",
      "         [0, 0, 0],\n",
      "         [1, 0, 2]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[ 0.4474, -1.7198, -1.5322,  0.3552, -0.1993, -0.7554, -1.8825, -0.1681,\n",
      "          0.0676]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.4812, 0.2594, 0.2594],\n",
      "          [0.3576, 0.3242, 0.3183],\n",
      "          [0.0791, 0.8418, 0.0791]],\n",
      "\n",
      "         [[0.2449, 0.5103, 0.2449],\n",
      "          [0.3289, 0.3421, 0.3289],\n",
      "          [0.1235, 0.7530, 0.1235]],\n",
      "\n",
      "         [[0.4831, 0.2585, 0.2585],\n",
      "          [0.3313, 0.3505, 0.3182],\n",
      "          [0.9271, 0.0365, 0.0365]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 1],\n",
      "         [1, 1, 1],\n",
      "         [0, 1, 0]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[ 1.3776e+00,  9.1436e-02, -2.9805e+00,  7.1179e-02, -5.1782e-01,\n",
      "         -1.3827e+00,  3.6784e-01, -1.4492e-03, -6.4611e-01]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.6327, 0.1899, 0.1775],\n",
      "          [0.7070, 0.1465, 0.1465],\n",
      "          [0.1791, 0.6418, 0.1791]],\n",
      "\n",
      "         [[0.3008, 0.3985, 0.3008],\n",
      "          [0.3514, 0.3243, 0.3243],\n",
      "          [0.2901, 0.4199, 0.2901]],\n",
      "\n",
      "         [[0.3333, 0.3333, 0.3333],\n",
      "          [0.1739, 0.6522, 0.1739],\n",
      "          [0.9122, 0.0439, 0.0439]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 1],\n",
      "         [1, 0, 1],\n",
      "         [2, 1, 0]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[-2.7018, -1.8796, -1.1194,  0.8872,  0.0928, -0.4821, -0.1113,  0.7095,\n",
      "         -1.5047]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.5659, 0.2269, 0.2073],\n",
      "          [0.8337, 0.0831, 0.0831],\n",
      "          [0.6348, 0.1826, 0.1826]],\n",
      "\n",
      "         [[0.7113, 0.1443, 0.1443],\n",
      "          [0.5151, 0.2425, 0.2425],\n",
      "          [0.3330, 0.3945, 0.2725]],\n",
      "\n",
      "         [[0.3480, 0.3260, 0.3260],\n",
      "          [0.5932, 0.2034, 0.2034],\n",
      "          [0.2629, 0.4742, 0.2629]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 0],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 1]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[-1.0646,  1.3757,  0.5021,  0.0949,  1.2031, -0.4527,  0.6535, -0.7406,\n",
      "          0.4065]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.7023, 0.1495, 0.1482],\n",
      "          [0.8404, 0.0674, 0.0921],\n",
      "          [0.4882, 0.2559, 0.2559]],\n",
      "\n",
      "         [[0.8893, 0.0525, 0.0582],\n",
      "          [0.3333, 0.3333, 0.3333],\n",
      "          [0.2323, 0.5496, 0.2181]],\n",
      "\n",
      "         [[0.2728, 0.4543, 0.2728],\n",
      "          [0.1498, 0.6583, 0.1919],\n",
      "          [0.3365, 0.3318, 0.3318]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 0],\n",
      "         [0, 2, 1],\n",
      "         [1, 1, 0]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[ 0.3822,  0.2777, -0.1643,  0.4993,  1.8611,  0.0612,  1.0170, -0.1204,\n",
      "          0.0880]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.6724, 0.1600, 0.1676],\n",
      "          [0.3176, 0.3592, 0.3231],\n",
      "          [0.4817, 0.2592, 0.2592]],\n",
      "\n",
      "         [[0.9880, 0.0043, 0.0076],\n",
      "          [0.3719, 0.3141, 0.3141],\n",
      "          [0.4365, 0.2440, 0.3195]],\n",
      "\n",
      "         [[0.2996, 0.4009, 0.2996],\n",
      "          [0.4273, 0.3148, 0.2579],\n",
      "          [0.3281, 0.3437, 0.3281]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 1, 0],\n",
      "         [0, 0, 0],\n",
      "         [1, 0, 1]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[-0.1990,  0.6297, -0.3395,  0.6537, -0.4778,  1.4227, -1.0436,  0.4284,\n",
      "          0.9477]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.4342, 0.2829, 0.2829],\n",
      "          [0.5985, 0.2130, 0.1885],\n",
      "          [0.2662, 0.4677, 0.2662]],\n",
      "\n",
      "         [[0.5969, 0.1994, 0.2037],\n",
      "          [0.6492, 0.1705, 0.1803],\n",
      "          [0.1735, 0.6627, 0.1637]],\n",
      "\n",
      "         [[0.2785, 0.4431, 0.2785],\n",
      "          [0.1743, 0.6383, 0.1874],\n",
      "          [0.8148, 0.0926, 0.0926]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 1],\n",
      "         [0, 0, 1],\n",
      "         [1, 1, 0]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[ 2.0776,  1.2195, -2.1899, -1.7306,  0.1037, -0.2033, -0.9652, -1.7160,\n",
      "         -0.5277]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.4505, 0.2747, 0.2747],\n",
      "          [0.4003, 0.2998, 0.2998],\n",
      "          [0.0795, 0.8410, 0.0795]],\n",
      "\n",
      "         [[0.3096, 0.3808, 0.3096],\n",
      "          [0.2682, 0.4637, 0.2682],\n",
      "          [0.2329, 0.5342, 0.2329]],\n",
      "\n",
      "         [[0.3333, 0.3333, 0.3333],\n",
      "          [0.1370, 0.7261, 0.1370],\n",
      "          [0.9668, 0.0152, 0.0180]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 1],\n",
      "         [1, 1, 1],\n",
      "         [2, 1, 0]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[-0.6538,  0.7359, -0.0537, -0.6582, -1.2081,  0.3049,  0.8745, -0.6666,\n",
      "          0.4326]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.6440, 0.1965, 0.1594],\n",
      "          [0.7273, 0.1367, 0.1360],\n",
      "          [0.4246, 0.2877, 0.2877]],\n",
      "\n",
      "         [[0.8926, 0.0459, 0.0616],\n",
      "          [0.3333, 0.3333, 0.3333],\n",
      "          [0.2625, 0.4657, 0.2718]],\n",
      "\n",
      "         [[0.2112, 0.5777, 0.2112],\n",
      "          [0.1682, 0.6236, 0.2081],\n",
      "          [0.4487, 0.2757, 0.2757]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 0],\n",
      "         [0, 2, 1],\n",
      "         [1, 1, 0]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[ 0.1297, -0.6493, -2.1177,  0.7848,  1.1676,  0.7711, -1.1676, -0.1361,\n",
      "         -1.2896]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.6645, 0.1676, 0.1679],\n",
      "          [0.4367, 0.2824, 0.2808],\n",
      "          [0.3333, 0.3333, 0.3333]],\n",
      "\n",
      "         [[0.8212, 0.0868, 0.0919],\n",
      "          [0.3582, 0.3209, 0.3209],\n",
      "          [0.3159, 0.3681, 0.3159]],\n",
      "\n",
      "         [[0.2452, 0.5097, 0.2452],\n",
      "          [0.2462, 0.5060, 0.2478],\n",
      "          [0.8046, 0.0977, 0.0977]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 2],\n",
      "         [0, 0, 1],\n",
      "         [1, 1, 0]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[ 0.2046, -1.0921, -0.9528,  0.3248, -0.9122,  0.2365, -0.2017,  0.6334,\n",
      "          0.1310]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.6461, 0.1753, 0.1786],\n",
      "          [0.2816, 0.4369, 0.2816],\n",
      "          [0.4139, 0.2930, 0.2930]],\n",
      "\n",
      "         [[0.9621, 0.0175, 0.0204],\n",
      "          [0.4540, 0.2730, 0.2730],\n",
      "          [0.3393, 0.3885, 0.2723]],\n",
      "\n",
      "         [[0.2723, 0.4554, 0.2723],\n",
      "          [0.4348, 0.2636, 0.3016],\n",
      "          [0.3180, 0.3639, 0.3180]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 1, 0],\n",
      "         [0, 0, 1],\n",
      "         [1, 0, 1]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[-1.0017,  1.2821,  0.1787, -0.5890, -0.5602,  0.9798, -1.3290, -1.3540,\n",
      "          0.2530]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.4922, 0.3008, 0.2070],\n",
      "          [0.7885, 0.0949, 0.1166],\n",
      "          [0.3333, 0.3333, 0.3333]],\n",
      "\n",
      "         [[0.3064, 0.3517, 0.3419],\n",
      "          [0.2847, 0.4306, 0.2847],\n",
      "          [0.1879, 0.6553, 0.1569]],\n",
      "\n",
      "         [[0.1909, 0.6182, 0.1909],\n",
      "          [0.0724, 0.8438, 0.0838],\n",
      "          [0.8602, 0.0699, 0.0699]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 2],\n",
      "         [1, 1, 1],\n",
      "         [1, 1, 0]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[-0.5777,  0.6515, -1.0140,  1.8471,  1.3240,  0.3994,  0.1787,  0.9118,\n",
      "          0.4643]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.5506, 0.2315, 0.2178],\n",
      "          [0.6243, 0.2079, 0.1678],\n",
      "          [0.4175, 0.2912, 0.2912]],\n",
      "\n",
      "         [[0.9405, 0.0283, 0.0312],\n",
      "          [0.6553, 0.1724, 0.1724],\n",
      "          [0.2839, 0.4817, 0.2344]],\n",
      "\n",
      "         [[0.3333, 0.3333, 0.3333],\n",
      "          [0.2847, 0.5102, 0.2051],\n",
      "          [0.3706, 0.3147, 0.3147]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 0],\n",
      "         [0, 0, 1],\n",
      "         [2, 1, 0]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[ 1.8567,  0.4840,  0.0907,  2.2315, -1.0472, -0.3197, -1.2984,  0.7777,\n",
      "         -0.8848]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.3333, 0.3333, 0.3333],\n",
      "          [0.3333, 0.3333, 0.3333],\n",
      "          [0.0310, 0.9378, 0.0312]],\n",
      "\n",
      "         [[0.2583, 0.4834, 0.2583],\n",
      "          [0.6699, 0.1983, 0.1318],\n",
      "          [0.1624, 0.6752, 0.1624]],\n",
      "\n",
      "         [[0.4112, 0.2944, 0.2944],\n",
      "          [0.1790, 0.6419, 0.1790],\n",
      "          [0.9756, 0.0119, 0.0125]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[2, 2, 1],\n",
      "         [1, 0, 1],\n",
      "         [0, 1, 0]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[ 1.2081, -0.9918,  0.9208,  0.3404,  0.1205, -0.8581, -1.0168, -0.0126,\n",
      "          0.9719]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.3333, 0.3333, 0.3333],\n",
      "          [0.2943, 0.4115, 0.2943],\n",
      "          [0.1159, 0.7484, 0.1358]],\n",
      "\n",
      "         [[0.9114, 0.0439, 0.0447],\n",
      "          [0.4135, 0.3277, 0.2587],\n",
      "          [0.2036, 0.6262, 0.1702]],\n",
      "\n",
      "         [[0.4793, 0.2604, 0.2604],\n",
      "          [0.5619, 0.2191, 0.2191],\n",
      "          [0.8593, 0.0703, 0.0703]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[2, 1, 1],\n",
      "         [0, 0, 1],\n",
      "         [0, 0, 0]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[ 0.0474,  0.6911, -0.7437, -0.1937,  1.9292,  0.9406,  0.4864, -0.6035,\n",
      "         -0.3198]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.5893, 0.1889, 0.2218],\n",
      "          [0.4060, 0.3567, 0.2374],\n",
      "          [0.4097, 0.2952, 0.2952]],\n",
      "\n",
      "         [[0.9835, 0.0061, 0.0103],\n",
      "          [0.3333, 0.3333, 0.3333],\n",
      "          [0.4034, 0.2711, 0.3254]],\n",
      "\n",
      "         [[0.2485, 0.5030, 0.2485],\n",
      "          [0.4317, 0.2981, 0.2702],\n",
      "          [0.3333, 0.3333, 0.3333]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 0],\n",
      "         [0, 2, 0],\n",
      "         [1, 0, 2]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[-1.5317,  0.3796,  1.4129, -0.6378,  0.0480, -0.6229, -0.1516, -0.2708,\n",
      "         -0.2910]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.5720, 0.2028, 0.2252],\n",
      "          [0.6037, 0.2148, 0.1815],\n",
      "          [0.5548, 0.2226, 0.2226]],\n",
      "\n",
      "         [[0.9301, 0.0305, 0.0394],\n",
      "          [0.3333, 0.3333, 0.3333],\n",
      "          [0.2004, 0.5895, 0.2101]],\n",
      "\n",
      "         [[0.3073, 0.3854, 0.3073],\n",
      "          [0.1778, 0.6082, 0.2140],\n",
      "          [0.3761, 0.3119, 0.3119]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 0],\n",
      "         [0, 2, 1],\n",
      "         [1, 1, 0]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[ 0.2971, -1.3280,  0.2155, -0.0220, -1.0014,  0.9843, -0.8088, -1.9070,\n",
      "         -0.1427]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.3333, 0.3333, 0.3333],\n",
      "          [0.3618, 0.3272, 0.3110],\n",
      "          [0.2096, 0.5808, 0.2096]],\n",
      "\n",
      "         [[0.3363, 0.3318, 0.3318],\n",
      "          [0.2950, 0.4101, 0.2950],\n",
      "          [0.2845, 0.4310, 0.2845]],\n",
      "\n",
      "         [[0.1177, 0.7645, 0.1177],\n",
      "          [0.3059, 0.3882, 0.3059],\n",
      "          [0.7822, 0.1143, 0.1035]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[2, 0, 1],\n",
      "         [0, 1, 1],\n",
      "         [1, 1, 0]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[-0.4537,  1.4563, -1.5815,  0.8153, -0.9280, -0.4250, -0.0576, -0.6829,\n",
      "          0.0348]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.5005, 0.3031, 0.1964],\n",
      "          [0.9262, 0.0301, 0.0438],\n",
      "          [0.3198, 0.3604, 0.3198]],\n",
      "\n",
      "         [[0.2543, 0.4814, 0.2643],\n",
      "          [0.4351, 0.2825, 0.2825],\n",
      "          [0.2247, 0.5507, 0.2247]],\n",
      "\n",
      "         [[0.2402, 0.5196, 0.2402],\n",
      "          [0.0407, 0.9168, 0.0425],\n",
      "          [0.9358, 0.0321, 0.0321]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 1],\n",
      "         [1, 0, 1],\n",
      "         [1, 1, 0]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[-1.6475,  1.7439, -0.9648, -0.2583, -2.3634, -0.2405,  1.2493, -0.7206,\n",
      "          1.0285]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.3305, 0.4892, 0.1803],\n",
      "          [0.9831, 0.0063, 0.0106],\n",
      "          [0.2719, 0.4561, 0.2719]],\n",
      "\n",
      "         [[0.2833, 0.4335, 0.2833],\n",
      "          [0.5454, 0.2273, 0.2273],\n",
      "          [0.2184, 0.5633, 0.2184]],\n",
      "\n",
      "         [[0.3045, 0.3911, 0.3045],\n",
      "          [0.0417, 0.9157, 0.0427],\n",
      "          [0.8032, 0.0984, 0.0984]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[1, 0, 1],\n",
      "         [1, 0, 1],\n",
      "         [1, 1, 0]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[-0.6259, -0.4043, -1.1661,  0.4782,  0.4329,  0.0336, -1.1672,  0.6789,\n",
      "          0.5389]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.6104, 0.1919, 0.1977],\n",
      "          [0.4530, 0.2995, 0.2475],\n",
      "          [0.3333, 0.3333, 0.3333]],\n",
      "\n",
      "         [[0.9349, 0.0296, 0.0355],\n",
      "          [0.4370, 0.2815, 0.2815],\n",
      "          [0.2103, 0.6158, 0.1739]],\n",
      "\n",
      "         [[0.3333, 0.3333, 0.3333],\n",
      "          [0.3860, 0.3661, 0.2479],\n",
      "          [0.5613, 0.2194, 0.2194]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 2],\n",
      "         [0, 0, 1],\n",
      "         [2, 0, 0]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[-0.7262, -0.0120,  0.3718, -0.6540,  0.0763, -1.7308, -2.7390, -1.0837,\n",
      "         -0.4748]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.3333, 0.3333, 0.3333],\n",
      "          [0.5464, 0.2268, 0.2268],\n",
      "          [0.1341, 0.7317, 0.1341]],\n",
      "\n",
      "         [[0.2227, 0.5545, 0.2227],\n",
      "          [0.2642, 0.4716, 0.2642],\n",
      "          [0.0297, 0.9384, 0.0319]],\n",
      "\n",
      "         [[0.3782, 0.3109, 0.3109],\n",
      "          [0.1002, 0.7996, 0.1002],\n",
      "          [0.9695, 0.0149, 0.0156]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[2, 0, 1],\n",
      "         [1, 1, 1],\n",
      "         [0, 1, 0]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[-1.1827,  0.7076,  1.7205, -1.0234, -1.4569, -0.3923, -1.1344, -1.3323,\n",
      "          1.1738]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.3333, 0.3333, 0.3333],\n",
      "          [0.7460, 0.1176, 0.1364],\n",
      "          [0.2293, 0.5414, 0.2293]],\n",
      "\n",
      "         [[0.2600, 0.4801, 0.2600],\n",
      "          [0.2696, 0.4609, 0.2696],\n",
      "          [0.0656, 0.8691, 0.0653]],\n",
      "\n",
      "         [[0.2332, 0.5337, 0.2332],\n",
      "          [0.1010, 0.7980, 0.1010],\n",
      "          [0.8615, 0.0692, 0.0692]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[2, 0, 1],\n",
      "         [1, 1, 1],\n",
      "         [1, 1, 0]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[-1.2669,  0.8618,  0.3462,  0.7305,  0.7123, -0.5528, -1.6201,  0.3871,\n",
      "         -0.2870]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.4718, 0.2641, 0.2641],\n",
      "          [0.6751, 0.1751, 0.1498],\n",
      "          [0.3333, 0.3333, 0.3333]],\n",
      "\n",
      "         [[0.5451, 0.2329, 0.2220],\n",
      "          [0.5098, 0.2352, 0.2551],\n",
      "          [0.1554, 0.7063, 0.1383]],\n",
      "\n",
      "         [[0.3333, 0.3333, 0.3333],\n",
      "          [0.1210, 0.7322, 0.1467],\n",
      "          [0.8392, 0.0804, 0.0804]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 2],\n",
      "         [0, 0, 1],\n",
      "         [2, 1, 0]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[-0.7441,  1.0668,  1.9879, -0.1511,  0.6949,  1.6273, -0.1513, -0.0841,\n",
      "          1.1612]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.3354, 0.3323, 0.3323],\n",
      "          [0.3167, 0.4689, 0.2144],\n",
      "          [0.4065, 0.2968, 0.2968]],\n",
      "\n",
      "         [[0.9267, 0.0310, 0.0423],\n",
      "          [0.4495, 0.2752, 0.2752],\n",
      "          [0.2857, 0.5082, 0.2061]],\n",
      "\n",
      "         [[0.2787, 0.4425, 0.2787],\n",
      "          [0.4233, 0.3447, 0.2321],\n",
      "          [0.3771, 0.3114, 0.3114]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 1, 0],\n",
      "         [0, 0, 1],\n",
      "         [1, 0, 0]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[-0.7184,  0.3767, -1.0727,  0.2190, -1.3533, -1.0705,  0.3886,  0.5670,\n",
      "         -0.1324]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.7306, 0.1385, 0.1308],\n",
      "          [0.8286, 0.0862, 0.0851],\n",
      "          [0.3333, 0.3333, 0.3333]],\n",
      "\n",
      "         [[0.6896, 0.1375, 0.1729],\n",
      "          [0.4692, 0.2654, 0.2654],\n",
      "          [0.2048, 0.5601, 0.2351]],\n",
      "\n",
      "         [[0.3333, 0.3333, 0.3333],\n",
      "          [0.1300, 0.7232, 0.1468],\n",
      "          [0.7057, 0.1471, 0.1471]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 2],\n",
      "         [0, 0, 1],\n",
      "         [2, 1, 0]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[ 0.3435,  0.4023, -0.8868, -0.2010,  0.3641, -0.7313, -0.8893,  0.9909,\n",
      "         -1.1364]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.5992, 0.1894, 0.2114],\n",
      "          [0.3098, 0.3978, 0.2924],\n",
      "          [0.3923, 0.3038, 0.3038]],\n",
      "\n",
      "         [[0.9043, 0.0422, 0.0535],\n",
      "          [0.5158, 0.2404, 0.2438],\n",
      "          [0.1541, 0.6766, 0.1692]],\n",
      "\n",
      "         [[0.3333, 0.3333, 0.3333],\n",
      "          [0.2887, 0.4981, 0.2132],\n",
      "          [0.7885, 0.1058, 0.1058]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 1, 0],\n",
      "         [0, 0, 1],\n",
      "         [2, 1, 0]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[ 0.0723, -0.1449,  0.3570,  0.5693, -0.1650, -0.8693,  0.6710, -1.0541,\n",
      "          0.0241]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.7181, 0.1512, 0.1307],\n",
      "          [0.5771, 0.2208, 0.2021],\n",
      "          [0.4217, 0.2892, 0.2892]],\n",
      "\n",
      "         [[0.9182, 0.0338, 0.0480],\n",
      "          [0.3333, 0.3333, 0.3333],\n",
      "          [0.2518, 0.4594, 0.2889]],\n",
      "\n",
      "         [[0.2318, 0.5364, 0.2318],\n",
      "          [0.1811, 0.6153, 0.2035],\n",
      "          [0.5489, 0.2255, 0.2255]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 0],\n",
      "         [0, 2, 1],\n",
      "         [1, 1, 0]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[-1.0099, -0.2119,  0.2712,  0.0883,  0.1909, -1.9167,  0.6532,  0.4021,\n",
      "         -0.7117]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.6857, 0.1421, 0.1723],\n",
      "          [0.5321, 0.2365, 0.2314],\n",
      "          [0.4951, 0.2525, 0.2525]],\n",
      "\n",
      "         [[0.9774, 0.0089, 0.0137],\n",
      "          [0.3761, 0.3119, 0.3119],\n",
      "          [0.3170, 0.4124, 0.2706]],\n",
      "\n",
      "         [[0.3333, 0.3333, 0.3333],\n",
      "          [0.2169, 0.5485, 0.2346],\n",
      "          [0.2934, 0.4133, 0.2934]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 0],\n",
      "         [0, 0, 1],\n",
      "         [2, 1, 1]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[-1.9738, -0.0780,  0.1150, -1.3868,  0.0284,  0.0454, -0.6853, -0.2069,\n",
      "         -0.8299]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.6387, 0.1624, 0.1989],\n",
      "          [0.6195, 0.2186, 0.1619],\n",
      "          [0.5206, 0.2397, 0.2397]],\n",
      "\n",
      "         [[0.9476, 0.0224, 0.0299],\n",
      "          [0.3333, 0.3333, 0.3333],\n",
      "          [0.1931, 0.5906, 0.2163]],\n",
      "\n",
      "         [[0.2792, 0.4416, 0.2792],\n",
      "          [0.2311, 0.5012, 0.2677],\n",
      "          [0.3333, 0.3333, 0.3333]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 0],\n",
      "         [0, 2, 1],\n",
      "         [1, 1, 2]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[ 0.5318,  1.5723,  0.9647, -1.2823,  1.0893, -0.9111,  0.8091, -1.0680,\n",
      "          1.1210]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.6668, 0.1666, 0.1666],\n",
      "          [0.3221, 0.3557, 0.3221],\n",
      "          [0.4720, 0.2640, 0.2640]],\n",
      "\n",
      "         [[0.9243, 0.0379, 0.0378],\n",
      "          [0.3333, 0.3333, 0.3333],\n",
      "          [0.1384, 0.7425, 0.1192]],\n",
      "\n",
      "         [[0.3333, 0.3333, 0.3333],\n",
      "          [0.2561, 0.4867, 0.2572],\n",
      "          [0.4697, 0.2652, 0.2652]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 1, 0],\n",
      "         [0, 2, 1],\n",
      "         [2, 1, 0]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[-0.0288, -2.3962, -0.3262,  0.0496, -0.3156, -0.0598,  0.3820, -0.7633,\n",
      "         -0.4003]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.6195, 0.1716, 0.2089],\n",
      "          [0.3042, 0.3916, 0.3042],\n",
      "          [0.4600, 0.2700, 0.2700]],\n",
      "\n",
      "         [[0.9823, 0.0086, 0.0091],\n",
      "          [0.3190, 0.3619, 0.3190],\n",
      "          [0.3333, 0.3333, 0.3333]],\n",
      "\n",
      "         [[0.1013, 0.7819, 0.1168],\n",
      "          [0.5982, 0.1933, 0.2085],\n",
      "          [0.2124, 0.5752, 0.2124]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 1, 0],\n",
      "         [0, 1, 2],\n",
      "         [1, 0, 1]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[ 1.2041,  1.0806, -0.5427,  0.6675, -0.1138, -0.0096, -0.3791, -0.1099,\n",
      "         -0.0594]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.6436, 0.1782, 0.1782],\n",
      "          [0.3366, 0.3734, 0.2900],\n",
      "          [0.3333, 0.3333, 0.3333]],\n",
      "\n",
      "         [[0.7974, 0.0982, 0.1044],\n",
      "          [0.5442, 0.2279, 0.2279],\n",
      "          [0.2179, 0.5363, 0.2458]],\n",
      "\n",
      "         [[0.2911, 0.4177, 0.2911],\n",
      "          [0.2157, 0.5195, 0.2647],\n",
      "          [0.8818, 0.0574, 0.0609]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 1, 2],\n",
      "         [0, 0, 1],\n",
      "         [1, 1, 0]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[ 0.6304,  0.1883,  0.2762,  1.2919, -0.1439,  1.5023, -0.4754, -0.4800,\n",
      "          0.6944]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.3605, 0.3198, 0.3198],\n",
      "          [0.3100, 0.4198, 0.2702],\n",
      "          [0.3164, 0.3541, 0.3295]],\n",
      "\n",
      "         [[0.8421, 0.0730, 0.0849],\n",
      "          [0.5715, 0.2248, 0.2037],\n",
      "          [0.2728, 0.4969, 0.2304]],\n",
      "\n",
      "         [[0.2378, 0.5244, 0.2378],\n",
      "          [0.2453, 0.5322, 0.2225],\n",
      "          [0.7825, 0.1087, 0.1087]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 1, 1],\n",
      "         [0, 0, 1],\n",
      "         [1, 1, 0]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[ 0.6168, -0.2851,  0.6815, -0.0383,  0.6563, -0.2283, -1.6312,  0.0387,\n",
      "         -0.0484]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.4127, 0.2936, 0.2936],\n",
      "          [0.2817, 0.4366, 0.2817],\n",
      "          [0.2879, 0.4241, 0.2879]],\n",
      "\n",
      "         [[0.9298, 0.0324, 0.0379],\n",
      "          [0.4399, 0.2784, 0.2817],\n",
      "          [0.1503, 0.7050, 0.1447]],\n",
      "\n",
      "         [[0.3333, 0.3333, 0.3333],\n",
      "          [0.5596, 0.2136, 0.2268],\n",
      "          [0.8469, 0.0766, 0.0766]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 1, 1],\n",
      "         [0, 0, 1],\n",
      "         [2, 0, 0]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[-1.4652, -0.1303, -0.9873, -1.9208,  0.1484,  0.0189,  1.2948, -1.2049,\n",
      "          1.4937]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.7463, 0.1383, 0.1154],\n",
      "          [0.7961, 0.1000, 0.1039],\n",
      "          [0.6979, 0.1510, 0.1510]],\n",
      "\n",
      "         [[0.8789, 0.0605, 0.0605],\n",
      "          [0.2718, 0.4563, 0.2718],\n",
      "          [0.3222, 0.3222, 0.3555]],\n",
      "\n",
      "         [[0.0801, 0.8275, 0.0925],\n",
      "          [0.3333, 0.3333, 0.3333],\n",
      "          [0.2266, 0.5467, 0.2266]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 0],\n",
      "         [0, 1, 2],\n",
      "         [1, 2, 1]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[ 0.0156,  0.1263, -0.9622,  0.5349, -0.3676, -1.2301, -0.3126, -0.1277,\n",
      "         -1.0945]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.6845, 0.1565, 0.1590],\n",
      "          [0.7106, 0.1447, 0.1447],\n",
      "          [0.3333, 0.3333, 0.3333]],\n",
      "\n",
      "         [[0.6893, 0.1374, 0.1733],\n",
      "          [0.3690, 0.3155, 0.3155],\n",
      "          [0.2034, 0.5660, 0.2306]],\n",
      "\n",
      "         [[0.3333, 0.3333, 0.3333],\n",
      "          [0.1263, 0.7235, 0.1502],\n",
      "          [0.8319, 0.0817, 0.0864]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 2],\n",
      "         [0, 0, 1],\n",
      "         [2, 1, 0]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[-1.0005, -0.6027,  0.6570, -0.1586, -2.0117,  0.0675, -0.6106,  0.2139,\n",
      "         -0.0537]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.5601, 0.2199, 0.2199],\n",
      "          [0.6614, 0.1578, 0.1807],\n",
      "          [0.3333, 0.3333, 0.3333]],\n",
      "\n",
      "         [[0.7797, 0.1108, 0.1095],\n",
      "          [0.4891, 0.2554, 0.2554],\n",
      "          [0.1551, 0.6923, 0.1526]],\n",
      "\n",
      "         [[0.3153, 0.3693, 0.3153],\n",
      "          [0.2360, 0.5029, 0.2611],\n",
      "          [0.7076, 0.1462, 0.1462]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 2],\n",
      "         [0, 0, 1],\n",
      "         [1, 1, 0]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[-1.7106, -0.8392, -1.5063, -0.7053, -0.1154,  0.2461, -0.0488,  0.2771,\n",
      "          0.1514]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.6034, 0.1924, 0.2042],\n",
      "          [0.6374, 0.1826, 0.1800],\n",
      "          [0.4996, 0.2502, 0.2502]],\n",
      "\n",
      "         [[0.9667, 0.0149, 0.0184],\n",
      "          [0.3583, 0.3208, 0.3208],\n",
      "          [0.3642, 0.3427, 0.2931]],\n",
      "\n",
      "         [[0.2212, 0.5575, 0.2212],\n",
      "          [0.3284, 0.3469, 0.3247],\n",
      "          [0.2904, 0.4192, 0.2904]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 0],\n",
      "         [0, 0, 0],\n",
      "         [1, 1, 1]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[ 2.1377, -1.3307,  1.2868,  0.4936, -0.0114, -0.0326, -1.1210, -1.3801,\n",
      "          1.2688]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.3333, 0.3333, 0.3333],\n",
      "          [0.2696, 0.4608, 0.2696],\n",
      "          [0.0058, 0.9861, 0.0081]],\n",
      "\n",
      "         [[0.2815, 0.4369, 0.2815],\n",
      "          [0.1541, 0.6919, 0.1541],\n",
      "          [0.2432, 0.5136, 0.2432]],\n",
      "\n",
      "         [[0.7225, 0.1462, 0.1313],\n",
      "          [0.5540, 0.2230, 0.2230],\n",
      "          [0.9705, 0.0147, 0.0147]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[2, 1, 1],\n",
      "         [1, 1, 1],\n",
      "         [0, 0, 0]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[-0.3592,  0.2822, -0.5341,  0.0505,  0.3975,  0.7214,  0.4201,  0.1367,\n",
      "         -0.7944]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.6385, 0.1700, 0.1915],\n",
      "          [0.5451, 0.2409, 0.2140],\n",
      "          [0.4403, 0.2799, 0.2799]],\n",
      "\n",
      "         [[0.9785, 0.0079, 0.0136],\n",
      "          [0.4181, 0.2910, 0.2910],\n",
      "          [0.3944, 0.2764, 0.3292]],\n",
      "\n",
      "         [[0.2464, 0.5072, 0.2464],\n",
      "          [0.2366, 0.4931, 0.2703],\n",
      "          [0.3333, 0.3333, 0.3333]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 0],\n",
      "         [0, 0, 0],\n",
      "         [1, 1, 2]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[-0.1879,  0.8124,  1.7072, -1.0159,  0.0597,  0.0561,  0.7058, -0.6174,\n",
      "         -0.9027]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.5727, 0.1992, 0.2281],\n",
      "          [0.4076, 0.3418, 0.2506],\n",
      "          [0.6181, 0.1909, 0.1909]],\n",
      "\n",
      "         [[0.9582, 0.0175, 0.0244],\n",
      "          [0.3333, 0.3333, 0.3333],\n",
      "          [0.2133, 0.5277, 0.2591]],\n",
      "\n",
      "         [[0.2877, 0.4246, 0.2877],\n",
      "          [0.2392, 0.5216, 0.2392],\n",
      "          [0.3333, 0.3333, 0.3333]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 0],\n",
      "         [0, 2, 1],\n",
      "         [1, 1, 2]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[-0.9875, -0.2338, -0.3788,  0.2773,  1.9248, -0.0820, -1.6618, -0.8202,\n",
      "          0.0076]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.5913, 0.1959, 0.2129],\n",
      "          [0.4980, 0.2712, 0.2308],\n",
      "          [0.3460, 0.3270, 0.3270]],\n",
      "\n",
      "         [[0.9248, 0.0344, 0.0408],\n",
      "          [0.3242, 0.3516, 0.3242],\n",
      "          [0.2213, 0.5850, 0.1937]],\n",
      "\n",
      "         [[0.2556, 0.4888, 0.2556],\n",
      "          [0.2959, 0.4359, 0.2683],\n",
      "          [0.4846, 0.2577, 0.2577]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 0],\n",
      "         [0, 1, 1],\n",
      "         [1, 1, 0]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[ 0.8220, -0.8784, -0.1735, -0.5612,  0.9200,  0.6782, -0.4063,  1.1333,\n",
      "         -0.2131]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.5074, 0.2463, 0.2463],\n",
      "          [0.2352, 0.5295, 0.2352],\n",
      "          [0.4612, 0.2694, 0.2694]],\n",
      "\n",
      "         [[0.9911, 0.0036, 0.0052],\n",
      "          [0.4311, 0.2845, 0.2845],\n",
      "          [0.3734, 0.2692, 0.3575]],\n",
      "\n",
      "         [[0.2303, 0.5394, 0.2303],\n",
      "          [0.6129, 0.1806, 0.2066],\n",
      "          [0.2925, 0.4150, 0.2925]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 1, 0],\n",
      "         [0, 0, 0],\n",
      "         [1, 0, 1]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[ 0.3799, -1.7958,  0.8889, -1.1338,  0.5998, -0.8973,  0.0837,  0.2375,\n",
      "         -1.5551]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.6302, 0.1849, 0.1849],\n",
      "          [0.2888, 0.4224, 0.2888],\n",
      "          [0.5847, 0.2076, 0.2076]],\n",
      "\n",
      "         [[0.9924, 0.0032, 0.0044],\n",
      "          [0.3333, 0.3333, 0.3333],\n",
      "          [0.3954, 0.2922, 0.3124]],\n",
      "\n",
      "         [[0.2821, 0.4359, 0.2821],\n",
      "          [0.6528, 0.1619, 0.1854],\n",
      "          [0.2701, 0.4598, 0.2701]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 1, 0],\n",
      "         [0, 2, 0],\n",
      "         [1, 0, 1]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[-0.9753, -0.1464,  0.5224,  1.3640,  0.6174,  0.3882,  1.0387,  0.9860,\n",
      "          1.0264]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.4869, 0.2413, 0.2718],\n",
      "          [0.4472, 0.3278, 0.2251],\n",
      "          [0.5789, 0.2105, 0.2105]],\n",
      "\n",
      "         [[0.9755, 0.0105, 0.0140],\n",
      "          [0.5942, 0.2029, 0.2029],\n",
      "          [0.4476, 0.2685, 0.2839]],\n",
      "\n",
      "         [[0.2633, 0.4734, 0.2633],\n",
      "          [0.4237, 0.3448, 0.2315],\n",
      "          [0.2638, 0.4723, 0.2638]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 0],\n",
      "         [0, 0, 0],\n",
      "         [1, 0, 1]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[-1.4459,  2.0011, -1.6180,  0.4157, -0.0854,  0.4452, -1.4722,  1.5551,\n",
      "         -0.3601]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.2986, 0.4028, 0.2986],\n",
      "          [0.8972, 0.0475, 0.0553],\n",
      "          [0.1889, 0.6223, 0.1889]],\n",
      "\n",
      "         [[0.3165, 0.3670, 0.3165],\n",
      "          [0.6768, 0.1616, 0.1616],\n",
      "          [0.1153, 0.7747, 0.1100]],\n",
      "\n",
      "         [[0.3333, 0.3333, 0.3333],\n",
      "          [0.0867, 0.8266, 0.0867],\n",
      "          [0.8954, 0.0523, 0.0523]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[1, 0, 1],\n",
      "         [1, 0, 1],\n",
      "         [2, 1, 0]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[-8.5678e-01,  1.8555e-03, -1.9176e+00,  1.3279e+00, -5.6143e-01,\n",
      "          1.4832e+00,  1.8001e+00,  1.6866e-01, -3.9669e-01]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.2648, 0.4936, 0.2416],\n",
      "          [0.9090, 0.0455, 0.0455],\n",
      "          [0.3772, 0.3114, 0.3114]],\n",
      "\n",
      "         [[0.5907, 0.2047, 0.2047],\n",
      "          [0.7711, 0.1145, 0.1145],\n",
      "          [0.3373, 0.3314, 0.3314]],\n",
      "\n",
      "         [[0.1223, 0.7554, 0.1223],\n",
      "          [0.2530, 0.5567, 0.1904],\n",
      "          [0.3011, 0.3977, 0.3011]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[1, 0, 0],\n",
      "         [0, 0, 0],\n",
      "         [1, 1, 1]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[-1.1046, -0.2400,  0.2550, -1.8654,  1.8174, -0.3174,  0.4726, -1.1546,\n",
      "          0.5189]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.6489, 0.1723, 0.1788],\n",
      "          [0.4371, 0.3215, 0.2414],\n",
      "          [0.6656, 0.1672, 0.1672]],\n",
      "\n",
      "         [[0.9589, 0.0201, 0.0211],\n",
      "          [0.2921, 0.4158, 0.2921],\n",
      "          [0.3297, 0.3297, 0.3406]],\n",
      "\n",
      "         [[0.1600, 0.6785, 0.1615],\n",
      "          [0.4687, 0.2656, 0.2656],\n",
      "          [0.2311, 0.5378, 0.2311]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 0],\n",
      "         [0, 1, 2],\n",
      "         [1, 0, 1]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[-1.1354, -0.8320,  0.8310, -0.8207,  0.1720, -1.0977,  0.3511, -0.2376,\n",
      "         -1.6633]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.6843, 0.1418, 0.1739],\n",
      "          [0.4097, 0.3297, 0.2606],\n",
      "          [0.6304, 0.1848, 0.1848]],\n",
      "\n",
      "         [[0.9763, 0.0093, 0.0144],\n",
      "          [0.3333, 0.3333, 0.3333],\n",
      "          [0.2624, 0.4499, 0.2878]],\n",
      "\n",
      "         [[0.3071, 0.3858, 0.3071],\n",
      "          [0.4160, 0.3691, 0.2149],\n",
      "          [0.3251, 0.3498, 0.3251]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 0],\n",
      "         [0, 2, 1],\n",
      "         [1, 0, 1]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[-0.1602,  0.8784, -0.8044,  0.4513,  0.8611, -0.7483, -0.7446, -0.1295,\n",
      "          1.6402]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.5943, 0.2045, 0.2013],\n",
      "          [0.5442, 0.2279, 0.2279],\n",
      "          [0.3092, 0.3817, 0.3092]],\n",
      "\n",
      "         [[0.7232, 0.1369, 0.1399],\n",
      "          [0.3333, 0.3333, 0.3333],\n",
      "          [0.1226, 0.7710, 0.1064]],\n",
      "\n",
      "         [[0.3607, 0.3196, 0.3196],\n",
      "          [0.2182, 0.5456, 0.2362],\n",
      "          [0.7896, 0.1052, 0.1052]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 1],\n",
      "         [0, 2, 1],\n",
      "         [0, 1, 0]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[-0.8886, -0.8218,  0.5895,  1.1214,  0.3341, -0.0921,  0.1101, -1.2122,\n",
      "         -0.0680]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.5975, 0.2476, 0.1549],\n",
      "          [0.6988, 0.1462, 0.1550],\n",
      "          [0.4614, 0.2693, 0.2693]],\n",
      "\n",
      "         [[0.9308, 0.0297, 0.0395],\n",
      "          [0.3378, 0.3311, 0.3311],\n",
      "          [0.3577, 0.3107, 0.3316]],\n",
      "\n",
      "         [[0.1773, 0.6455, 0.1773],\n",
      "          [0.2215, 0.5570, 0.2215],\n",
      "          [0.3333, 0.3333, 0.3333]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 0],\n",
      "         [0, 0, 0],\n",
      "         [1, 1, 2]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[-1.5726,  0.5089,  0.0186, -1.4844, -0.3777, -0.4544,  0.7135,  0.5084,\n",
      "          0.2037]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.6345, 0.1824, 0.1831],\n",
      "          [0.6248, 0.2028, 0.1724],\n",
      "          [0.5408, 0.2296, 0.2296]],\n",
      "\n",
      "         [[0.9562, 0.0191, 0.0247],\n",
      "          [0.3333, 0.3333, 0.3333],\n",
      "          [0.2463, 0.5165, 0.2373]],\n",
      "\n",
      "         [[0.2888, 0.4223, 0.2888],\n",
      "          [0.2327, 0.4736, 0.2937],\n",
      "          [0.3187, 0.3627, 0.3187]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 0],\n",
      "         [0, 2, 1],\n",
      "         [1, 1, 1]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[-0.6248, -1.1795, -1.5603, -0.4809, -0.3969,  0.1114, -0.7903,  0.5444,\n",
      "          0.1623]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.6150, 0.1775, 0.2076],\n",
      "          [0.3404, 0.4040, 0.2555],\n",
      "          [0.4085, 0.2958, 0.2958]],\n",
      "\n",
      "         [[0.9546, 0.0210, 0.0244],\n",
      "          [0.3333, 0.3333, 0.3333],\n",
      "          [0.2957, 0.4504, 0.2539]],\n",
      "\n",
      "         [[0.2796, 0.4408, 0.2796],\n",
      "          [0.4085, 0.3005, 0.2909],\n",
      "          [0.3209, 0.3582, 0.3209]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 1, 0],\n",
      "         [0, 2, 1],\n",
      "         [1, 0, 1]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[-0.6387,  1.1546,  0.9965, -0.0333,  0.5952,  0.9837, -0.1173,  1.0131,\n",
      "         -2.4433]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.3801, 0.3190, 0.3009],\n",
      "          [0.4277, 0.3991, 0.1732],\n",
      "          [0.4598, 0.2701, 0.2701]],\n",
      "\n",
      "         [[0.9449, 0.0229, 0.0322],\n",
      "          [0.6533, 0.1734, 0.1734],\n",
      "          [0.2623, 0.5157, 0.2220]],\n",
      "\n",
      "         [[0.3333, 0.3333, 0.3333],\n",
      "          [0.2838, 0.5024, 0.2138],\n",
      "          [0.4224, 0.2888, 0.2888]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 0],\n",
      "         [0, 0, 1],\n",
      "         [2, 1, 0]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[-1.3298,  0.1903, -0.3674,  0.0564,  2.0950, -0.6714, -0.8932, -1.1540,\n",
      "         -1.7986]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.6184, 0.1748, 0.2067],\n",
      "          [0.7410, 0.1406, 0.1185],\n",
      "          [0.5509, 0.2246, 0.2246]],\n",
      "\n",
      "         [[0.9069, 0.0445, 0.0486],\n",
      "          [0.3333, 0.3333, 0.3333],\n",
      "          [0.2321, 0.5593, 0.2085]],\n",
      "\n",
      "         [[0.3055, 0.3890, 0.3055],\n",
      "          [0.2031, 0.5541, 0.2429],\n",
      "          [0.3773, 0.3114, 0.3114]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 0],\n",
      "         [0, 2, 1],\n",
      "         [1, 1, 0]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[ 0.6045,  0.3451, -0.9382, -0.9833,  1.6882, -0.2761,  1.3854, -1.3863,\n",
      "         -0.5364]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.7532, 0.1185, 0.1283],\n",
      "          [0.3805, 0.3074, 0.3121],\n",
      "          [0.5968, 0.2016, 0.2016]],\n",
      "\n",
      "         [[0.9808, 0.0082, 0.0110],\n",
      "          [0.3333, 0.3333, 0.3333],\n",
      "          [0.3500, 0.3218, 0.3282]],\n",
      "\n",
      "         [[0.3033, 0.3933, 0.3033],\n",
      "          [0.4829, 0.2761, 0.2410],\n",
      "          [0.3333, 0.3333, 0.3333]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 0],\n",
      "         [0, 2, 0],\n",
      "         [1, 0, 2]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[ 1.0113, -0.0973, -1.2683,  0.8421,  0.9429,  0.2976, -1.6821,  1.2830,\n",
      "         -0.8638]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.5221, 0.2389, 0.2389],\n",
      "          [0.3032, 0.3936, 0.3032],\n",
      "          [0.2034, 0.5932, 0.2034]],\n",
      "\n",
      "         [[0.7866, 0.1036, 0.1097],\n",
      "          [0.5580, 0.2210, 0.2210],\n",
      "          [0.1522, 0.7006, 0.1472]],\n",
      "\n",
      "         [[0.3333, 0.3333, 0.3333],\n",
      "          [0.4090, 0.3801, 0.2110],\n",
      "          [0.8873, 0.0564, 0.0564]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 1, 1],\n",
      "         [0, 0, 1],\n",
      "         [2, 0, 0]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[ 0.1383,  0.4433, -0.0526,  0.1101,  1.4616, -0.5392,  1.6023,  0.0199,\n",
      "         -1.1145]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.7283, 0.1182, 0.1535],\n",
      "          [0.4309, 0.2715, 0.2976],\n",
      "          [0.5477, 0.2261, 0.2261]],\n",
      "\n",
      "         [[0.9849, 0.0057, 0.0094],\n",
      "          [0.3712, 0.3144, 0.3144],\n",
      "          [0.3626, 0.3064, 0.3310]],\n",
      "\n",
      "         [[0.3333, 0.3333, 0.3333],\n",
      "          [0.2775, 0.4657, 0.2568],\n",
      "          [0.3037, 0.3926, 0.3037]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 0],\n",
      "         [0, 0, 0],\n",
      "         [2, 1, 1]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[-0.1376, -2.5379,  0.2149,  0.4140,  0.1048, -0.6290,  0.1053,  1.3562,\n",
      "          0.1926]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.5712, 0.2144, 0.2144],\n",
      "          [0.2819, 0.4363, 0.2819],\n",
      "          [0.5522, 0.2239, 0.2239]],\n",
      "\n",
      "         [[0.9930, 0.0030, 0.0041],\n",
      "          [0.3333, 0.3333, 0.3333],\n",
      "          [0.3974, 0.3013, 0.3013]],\n",
      "\n",
      "         [[0.3175, 0.3508, 0.3317],\n",
      "          [0.6183, 0.1707, 0.2110],\n",
      "          [0.1670, 0.6659, 0.1670]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 1, 0],\n",
      "         [0, 2, 0],\n",
      "         [1, 0, 1]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[ 0.4227,  0.8703, -1.1281,  0.7649,  0.1230, -0.2394, -0.0478, -0.8167,\n",
      "         -0.2462]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.7456, 0.1349, 0.1195],\n",
      "          [0.7128, 0.1433, 0.1438],\n",
      "          [0.3333, 0.3333, 0.3333]],\n",
      "\n",
      "         [[0.5929, 0.2053, 0.2018],\n",
      "          [0.3333, 0.3333, 0.3333],\n",
      "          [0.2343, 0.4956, 0.2701]],\n",
      "\n",
      "         [[0.2102, 0.5796, 0.2102],\n",
      "          [0.1130, 0.7540, 0.1330],\n",
      "          [0.8342, 0.0807, 0.0851]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 2],\n",
      "         [0, 2, 1],\n",
      "         [1, 1, 0]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[-0.8194,  1.2837, -1.3179,  1.1990,  2.2372, -2.1863, -0.3068, -0.9778,\n",
      "          1.1572]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.6202, 0.1899, 0.1899],\n",
      "          [0.8977, 0.0511, 0.0511],\n",
      "          [0.2042, 0.5916, 0.2042]],\n",
      "\n",
      "         [[0.2468, 0.5065, 0.2468],\n",
      "          [0.2782, 0.4435, 0.2782],\n",
      "          [0.0240, 0.9509, 0.0252]],\n",
      "\n",
      "         [[0.5586, 0.2207, 0.2207],\n",
      "          [0.1386, 0.7229, 0.1386],\n",
      "          [0.9081, 0.0459, 0.0459]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 0, 1],\n",
      "         [1, 1, 1],\n",
      "         [0, 1, 0]]])\n",
      "\n",
      "Desired metrics:  [1]\n",
      "Input Noise: \n",
      " tensor([[ 1.6434, -0.6281, -0.1953,  1.0306, -0.9640,  0.1064,  0.9657,  0.1383,\n",
      "         -0.4842]])\n",
      "Board Tensor: \n",
      " tensor([[[[0.5232, 0.2309, 0.2459],\n",
      "          [0.2939, 0.4122, 0.2939],\n",
      "          [0.3454, 0.3273, 0.3273]],\n",
      "\n",
      "         [[0.9688, 0.0136, 0.0175],\n",
      "          [0.5204, 0.2398, 0.2398],\n",
      "          [0.3720, 0.3339, 0.2941]],\n",
      "\n",
      "         [[0.2439, 0.5122, 0.2439],\n",
      "          [0.5169, 0.2478, 0.2353],\n",
      "          [0.4488, 0.2756, 0.2756]]]], grad_fn=<SoftmaxBackward>)\n",
      "Board: \n",
      " tensor([[[0, 1, 0],\n",
      "         [0, 0, 0],\n",
      "         [1, 0, 0]]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test generator \n",
    "generator.eval()\n",
    "for _ in range(64):\n",
    "    metrics = get_one_metrics()\n",
    "    print(\"Desired metrics: \", metrics)\n",
    "    metrics = torch.tensor(metrics).float().reshape(1, -1)\n",
    "    noise = torch.randn(1, 9)\n",
    "    print('Input Noise: \\n', noise)\n",
    "    board = generator(noise, metrics)\n",
    "    print('Board Tensor: \\n',board)\n",
    "    print('Board: \\n',board.argmax(-1))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "go-banana",
   "language": "python",
   "name": "go-banana"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
